# Evaluation Function Specification

*Philosophy*

## Base Layer 


## File Structures
A standard evaluation function repository based on the provided [boilerplate](https://github.com/lambda-feedback/Evaluation-Function-Boilerplate) will have the following file structure:
```bash
app/
    __init__.py
    evaluation.py # Script containing the main evaluation_function
    docs.md # Documentation page for this function (required)
    evaluation_tests.py # Unittests for the main evaluation_function
    requirements.txt # list of packages needed for algorithm.py
    Dockerfile # for building whole image to deploy to AWS

.github/
    workflows/
        test-and-deploy.yml # Testing and deployment pipeline

config.json # Specify the name of the evaluation function in this file
.gitignore
```


## `evaluation.py`
The entire framework, validation and testing developed around evaluation functions is ultimately used to get to this file, or the `evaluation_function` function within it, to be more precise. 

### The `evaluation_function`
#### Inputs
- `response`: Data input by the user 
- `answer`: Data to compare user input to (could be from a DB of answers, or pre-generated by other functions)
- `params`: Parameters which affect the comparison process (replacements, tolerances, feedbacks, ...)

#### Outputs
The function should output a single JSON-encodable dictionary. Although a large amount of freedom is given to what this dict contains, when utilising the function alongside the [lambdafeedback](https://lambdafeedback.com/) web app, a few values are expected/able to be consumed:

**`is_correct: <bool>`**: Boolean parameter indicate whether the comparison between `response` and `answer` was deemed correct under the parameters. This field is then used by the web app to provide the most simple feedback to the user (green/red).

!!! info 
    *More standardised function outputs that the frontend can consume are to come*

### Error Handling
Error reporting should follow a specific approach for all evaluation functions. **If the `evaluation_function` you've written doesn't throw any errors, then it's output is returned under the `result` field - and assumed to have worked properly**. This means that if you catch an error in your code manually, and simply return it - the frontend will assume everything went fine. Instead, errors can be handled in two ways:

**Letting `evaluation_function` fail**: On the backend, the call to evaluation_function is wrapped in a try/except which catches any exception. This causes the evaluation to stop completely, returning a standard message, and a repr of the exception thrown in the `error.detail` field.

**Custom errors**: If you want to report more detailed errors from your function, use the `EvaluationException` class provided in the [evaluation-function-utils](module.md#errors) package. These are caught before all other standard exceptions, and are dealt with in a different way. These provide a way for your function to throw errors and stop executing safely, while supplying more accurate feedback to the front-end. 

!!! Example
    It is discouraged to do the following in the evaluation code:
    ```python
    if something.bad.happened():
        return {
            "error": {
                "message": "Some important message",
                "other": "details",
            }
        }
    ```

    As this causes the actual function output (by the AWS lambda function) to be:
    ```json 
    {
        "command": "eval",
        "result": {
            "error": {
                "message": "Some important message",
                "other": "details"
            }
        }
    }
    ```

    Instead, use custom exceptions from the [evaluation-function-utils](module.md#errors) package.
    ```python
    if something.bad.happened():
        raise EvaluationException(message="Some important message", other='details')
    ```

    As the actual function output will look like:
    ```json 
    {
        "command": "eval",
        "error": {
            "message": "Some important message",
            "other": "details"
        }
    }
    ```

    This immediately indicates to the frontend client that something has gone wrong, allowing for proper feedback to be displayed.

## `evaluation_tests.py`


## `docs.md` 