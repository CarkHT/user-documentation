{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Lambda Feedback! \u00b6 Students : Teachers : Accessible content - in the browser and on PDF. Feedback - express mathematical ideas naturally and get instant feedback. Analytics - track your progress, manage your time Social - connect with someone doing the same thing as you, now! Curate content - edit and publish your content in one place. Share - customise content from others; share your own content and get kudos. Analytics - curate automated feedback to meet student needs; arrange in-person lessons according to student needs. This project began at Imperial College in 2022. The project itself, and the documentation, is currently under construction. Lambda Feedback is a homework platform providing automated formative feedback . We are initially targeting STEM subjects in Higher Education , with a particular focus on mathematical subjects (such as mathematical methods or mechanics). The value proposition is given briefly here . In these docs you can find: Student guide Teacher guide Advanced teachers . Check out Lambda Feedback by clicking on the button below! Visit Lambda Feedback Articles relating to Lambda Feedback: Value proposition: Computers make us human Student experience of self-study: Getting stuck The role of challenge in self-study: Friction in the ideal learning process Configuring online learning: Worked solutions: when and how to use them? Articles on evaluation function algorithms, and on data analytics, are in preparation.","title":"Welcome to Lambda Feedback!"},{"location":"#welcome-to-lambda-feedback","text":"Students : Teachers : Accessible content - in the browser and on PDF. Feedback - express mathematical ideas naturally and get instant feedback. Analytics - track your progress, manage your time Social - connect with someone doing the same thing as you, now! Curate content - edit and publish your content in one place. Share - customise content from others; share your own content and get kudos. Analytics - curate automated feedback to meet student needs; arrange in-person lessons according to student needs. This project began at Imperial College in 2022. The project itself, and the documentation, is currently under construction. Lambda Feedback is a homework platform providing automated formative feedback . We are initially targeting STEM subjects in Higher Education , with a particular focus on mathematical subjects (such as mathematical methods or mechanics). The value proposition is given briefly here . In these docs you can find: Student guide Teacher guide Advanced teachers . Check out Lambda Feedback by clicking on the button below! Visit Lambda Feedback Articles relating to Lambda Feedback: Value proposition: Computers make us human Student experience of self-study: Getting stuck The role of challenge in self-study: Friction in the ideal learning process Configuring online learning: Worked solutions: when and how to use them? Articles on evaluation function algorithms, and on data analytics, are in preparation.","title":"Welcome to Lambda Feedback!"},{"location":"opportunities/","text":"Opportunities \u00b6 2023/06/05: there are no vacancees at the moment. The positions below are now filled. Principal Full Stack Developer \u00b6 We are recruiting a Principal Full Stack Developer to work on the Lambda Feedback project . Work is by contract for up to 40 days per year. The contractor will implement new features, review work by software engineers on the project, and advise on architecture decisions. The developer should be senior with significant experience taking responsibility for a full stack application, and ready to 'hit the ground running'. The tech stack is AWS / Postgres / Prisma / GraphQL / Nest.js / Next.js (React.js). The project is all built in Typescript. Infrastructure, testing, and deployment is all automated. We use test driven development, and CI/CD using CircleCI. We are a diverse team of educators, students, and software professionals developing an application in-situ with our users. This is a motivating application in education and uses exciting new technologies. We are a flexible team that values creativity and redefining problems before solving them. We have work that needs to be completed by July, and will have potential follow on work and commercialisation opportunities if the initial work is successful. Work is by contract (outside IR35) and we pay for deliverables. Prices for deliverables are agreed based on a day rate of \u00a3500-\u00a3800 depending on competence. We prefer to write and agree detailed completion criteria before agreeing to work. All work is quality assured, including testing with users, before approval. The Lambda Feedback project is owned and operated by Imperial College London. We work on campus in South Kensington, London. The position of contractor can optionally be fulfilled remotely if the holder prefers but we do expect occasional visits in-person. We are very flexible with timings and work regimes - our priority is to get the deliverables; we know that people work differently, and that people work best when they work in their own way. If you are interested, please enquire by email with peter [dot] johnson [at] ic.ac.uk. Summer placements \u00b6 Bursary: 8 positions, 8 weeks each (flexible), full-time. Bursary: \u00a3365 per week. Who should apply: Students from Imperial College London from any cohort who study in the following subjects: Aeronautics, Chemical Engineering, Computing, Design Engineering, Materials Science, Mathematics, Mechanical Engineering, Physics. Campus/Location: South Kensington, with opportunities to be remote later in the project. How to apply: apply here . Applications should be 300-500 words. Deadline: 31st March Contact details: Peter [dot] Johnson [at] ic.ac.uk More details on the project: Lambda Feedback software, a place to do homework \u00b6 Lambda Feedback is a web platform that hosts homework, with a focus on mathematical subjects. The platform hosts\u202fquestion content both in the browser and in traditional PDF format. Online step-by-step solutions are also provided and are particularly popular with students. In addition to content delivery, the platform provides automated feedback on student responses. The long-term vision is rich, timely, personalised, feedback to students at the time of doing their homework. The software is being developed within Imperial. This year is our first academic year in \u2018alpha\u2019 version which hosted 9 modules across 8 departments and 2 faculties, with over 1,000 student users. We are now working to widen the availability of the software and to improve the functionality. More information about the software can be found here:\u202f Article: https://teachingengineers.wordpress.com/2022/07/18/computers-make-us-human/\u202f Presentation: click here We have 8 StudentShapers positions in summer 2023 each with the following purpose:\u202f \u2022 In partnership with an academic staff member, curate their content on Lambda Feedback. Key aspects include content transfer and editing; setting up automated feedback; improving the content.\u202f \u2022 As part of the wider team of summer students, develop the software more broadly. Key aspects include documenting good practice, testing new features, designing new features, and designing a broader vision for the future software \u2013 for example curating positive learning communities on the platform, identifying key analytics to serve students and teachers, or developing study aids. Essential skills and experience that we are looking for:\u202f \u2022 A passion for and knowledge of your own subject\u202f \u2022 A deep appreciation for the student experience in your subject, and the key needs of students\u202f \u2022 A keen interest in content management, including typesetting (markdown, LaTeX, images; learn as you go!)\u202f \u2022 A vision for digital education where software serves the needs of today\u2019s students Additional ways you can add value to the project if you have the skills:\u202f \u2022 Mathematical computing skills, e.g. in Python, to help develop our evaluation functions (more info here: https://lambda-feedback.github.io/user-documentation/advanced/)\u202f \u2022 Data science skills to analyse our growing data set, for example skills in SQL queries, data analysis e.g. in Pandas, data visualisation, machine learning.\u202f \u2022 Graphic design, UI/UX design, vision development for online products\u202f \u2022 Web development skills in any part of the following stack: AWS / Postgres / Prisma / GraphQL / Nest.js / Next.js (React.js) / Typescript / CircleCI","title":"Opportunities"},{"location":"opportunities/#opportunities","text":"2023/06/05: there are no vacancees at the moment. The positions below are now filled.","title":"Opportunities"},{"location":"opportunities/#principal-full-stack-developer","text":"We are recruiting a Principal Full Stack Developer to work on the Lambda Feedback project . Work is by contract for up to 40 days per year. The contractor will implement new features, review work by software engineers on the project, and advise on architecture decisions. The developer should be senior with significant experience taking responsibility for a full stack application, and ready to 'hit the ground running'. The tech stack is AWS / Postgres / Prisma / GraphQL / Nest.js / Next.js (React.js). The project is all built in Typescript. Infrastructure, testing, and deployment is all automated. We use test driven development, and CI/CD using CircleCI. We are a diverse team of educators, students, and software professionals developing an application in-situ with our users. This is a motivating application in education and uses exciting new technologies. We are a flexible team that values creativity and redefining problems before solving them. We have work that needs to be completed by July, and will have potential follow on work and commercialisation opportunities if the initial work is successful. Work is by contract (outside IR35) and we pay for deliverables. Prices for deliverables are agreed based on a day rate of \u00a3500-\u00a3800 depending on competence. We prefer to write and agree detailed completion criteria before agreeing to work. All work is quality assured, including testing with users, before approval. The Lambda Feedback project is owned and operated by Imperial College London. We work on campus in South Kensington, London. The position of contractor can optionally be fulfilled remotely if the holder prefers but we do expect occasional visits in-person. We are very flexible with timings and work regimes - our priority is to get the deliverables; we know that people work differently, and that people work best when they work in their own way. If you are interested, please enquire by email with peter [dot] johnson [at] ic.ac.uk.","title":"Principal Full Stack Developer"},{"location":"opportunities/#summer-placements","text":"Bursary: 8 positions, 8 weeks each (flexible), full-time. Bursary: \u00a3365 per week. Who should apply: Students from Imperial College London from any cohort who study in the following subjects: Aeronautics, Chemical Engineering, Computing, Design Engineering, Materials Science, Mathematics, Mechanical Engineering, Physics. Campus/Location: South Kensington, with opportunities to be remote later in the project. How to apply: apply here . Applications should be 300-500 words. Deadline: 31st March Contact details: Peter [dot] Johnson [at] ic.ac.uk More details on the project:","title":"Summer placements"},{"location":"opportunities/#lambda-feedback-software-a-place-to-do-homework","text":"Lambda Feedback is a web platform that hosts homework, with a focus on mathematical subjects. The platform hosts\u202fquestion content both in the browser and in traditional PDF format. Online step-by-step solutions are also provided and are particularly popular with students. In addition to content delivery, the platform provides automated feedback on student responses. The long-term vision is rich, timely, personalised, feedback to students at the time of doing their homework. The software is being developed within Imperial. This year is our first academic year in \u2018alpha\u2019 version which hosted 9 modules across 8 departments and 2 faculties, with over 1,000 student users. We are now working to widen the availability of the software and to improve the functionality. More information about the software can be found here:\u202f Article: https://teachingengineers.wordpress.com/2022/07/18/computers-make-us-human/\u202f Presentation: click here We have 8 StudentShapers positions in summer 2023 each with the following purpose:\u202f \u2022 In partnership with an academic staff member, curate their content on Lambda Feedback. Key aspects include content transfer and editing; setting up automated feedback; improving the content.\u202f \u2022 As part of the wider team of summer students, develop the software more broadly. Key aspects include documenting good practice, testing new features, designing new features, and designing a broader vision for the future software \u2013 for example curating positive learning communities on the platform, identifying key analytics to serve students and teachers, or developing study aids. Essential skills and experience that we are looking for:\u202f \u2022 A passion for and knowledge of your own subject\u202f \u2022 A deep appreciation for the student experience in your subject, and the key needs of students\u202f \u2022 A keen interest in content management, including typesetting (markdown, LaTeX, images; learn as you go!)\u202f \u2022 A vision for digital education where software serves the needs of today\u2019s students Additional ways you can add value to the project if you have the skills:\u202f \u2022 Mathematical computing skills, e.g. in Python, to help develop our evaluation functions (more info here: https://lambda-feedback.github.io/user-documentation/advanced/)\u202f \u2022 Data science skills to analyse our growing data set, for example skills in SQL queries, data analysis e.g. in Pandas, data visualisation, machine learning.\u202f \u2022 Graphic design, UI/UX design, vision development for online products\u202f \u2022 Web development skills in any part of the following stack: AWS / Postgres / Prisma / GraphQL / Nest.js / Next.js (React.js) / Typescript / CircleCI","title":"Lambda Feedback software, a place to do homework"},{"location":"terminology/","text":"General Information \u00b6 LambdaFeedback is a place to study online. Teachers curate content that students can access. Content is available in the browser and in PDF. Automated feedback on final answers and detailed worked solutions are provided. Terminology Used and Definitions \u00b6 Here, the fundamental structure and terminology will be laid out. Evaluation Functions \u00b6 An evaluation function is an algorithm that is applied to a response area, whereby the student's response is evaluated and checked. The types of evaluation functions correspond to the types of response areas. Final Answer \u00b6 This section lies within the \"Help\" panel which appears upon clicking on the \"Help\" button. It serves as a simple container for the final answer, so that the student may compare results. This is an optional section, and so does not have to be included in any question. Modules \u00b6 A module is a set programme of taught material. In a university setting, this would correspond to a course module. Questions \u00b6 A question is a problem within a set, and it may contain any number of parts. Response Areas \u00b6 A response area is an interactive element. Student enters a response and receives feedback. There are different types of response area (text, numerical, array etc.), and these correspond to the different types of information the student is required to input. Sets \u00b6 We use the word 'Set' to refer to a group of questionscontent. In a university setting, a Set typically corresponds to an individual homework/tutorial sheet. Structured Tutorial \u00b6 Content providing a structure with which to approach a problem - but not to give the full details away. It is encouraged to be the first piece of guidance for the student before they look at the \"Worked Solution\", or \"Final Answer\". This section lies within the \"Help\" panel which appears upon clicking on the \"Help\" button. This is an optional section, and so does not have to be included in any question. Student (learner) \u00b6 From the perspective of Lambda Feedback, a student is someone who accesses and responds to problem sets. A student only has permissions to view and respond to problem sets (not to edit them). Teacher \u00b6 From the perspective of Lambda Feedback, a teacher is someone who creates and manages content. A teacher's account has permissions to create, edit, and delete content within a Module. Worked Solution \u00b6 The stages of working that lead to a final answer. It may be split into multiple steps which the student can reveal sequentially. This section lies within the \"Help\" panel which appears upon clicking on the \"Help\" button. This is an optional section, and so does not have to be included in any question.","title":"Terminology"},{"location":"terminology/#general-information","text":"LambdaFeedback is a place to study online. Teachers curate content that students can access. Content is available in the browser and in PDF. Automated feedback on final answers and detailed worked solutions are provided.","title":"General Information"},{"location":"terminology/#terminology-used-and-definitions","text":"Here, the fundamental structure and terminology will be laid out.","title":"Terminology Used and Definitions"},{"location":"terminology/#evaluation-functions","text":"An evaluation function is an algorithm that is applied to a response area, whereby the student's response is evaluated and checked. The types of evaluation functions correspond to the types of response areas.","title":"Evaluation Functions"},{"location":"terminology/#final-answer","text":"This section lies within the \"Help\" panel which appears upon clicking on the \"Help\" button. It serves as a simple container for the final answer, so that the student may compare results. This is an optional section, and so does not have to be included in any question.","title":"Final Answer"},{"location":"terminology/#modules","text":"A module is a set programme of taught material. In a university setting, this would correspond to a course module.","title":"Modules"},{"location":"terminology/#questions","text":"A question is a problem within a set, and it may contain any number of parts.","title":"Questions"},{"location":"terminology/#response-areas","text":"A response area is an interactive element. Student enters a response and receives feedback. There are different types of response area (text, numerical, array etc.), and these correspond to the different types of information the student is required to input.","title":"Response Areas"},{"location":"terminology/#sets","text":"We use the word 'Set' to refer to a group of questionscontent. In a university setting, a Set typically corresponds to an individual homework/tutorial sheet.","title":"Sets"},{"location":"terminology/#structured-tutorial","text":"Content providing a structure with which to approach a problem - but not to give the full details away. It is encouraged to be the first piece of guidance for the student before they look at the \"Worked Solution\", or \"Final Answer\". This section lies within the \"Help\" panel which appears upon clicking on the \"Help\" button. This is an optional section, and so does not have to be included in any question.","title":"Structured Tutorial"},{"location":"terminology/#student-learner","text":"From the perspective of Lambda Feedback, a student is someone who accesses and responds to problem sets. A student only has permissions to view and respond to problem sets (not to edit them).","title":"Student (learner)"},{"location":"terminology/#teacher","text":"From the perspective of Lambda Feedback, a teacher is someone who creates and manages content. A teacher's account has permissions to create, edit, and delete content within a Module.","title":"Teacher"},{"location":"terminology/#worked-solution","text":"The stages of working that lead to a final answer. It may be split into multiple steps which the student can reveal sequentially. This section lies within the \"Help\" panel which appears upon clicking on the \"Help\" button. This is an optional section, and so does not have to be included in any question.","title":"Worked Solution"},{"location":"advanced/","text":"Advanced users \u00b6 Advanced users can develop their own evaluation functions. Evaluation functions \u00b6 Evaluation functions are responsible for taking in a user's response, comparing it with a correct answer, and providing feedback to the frontend application. Living as containserized Lambda functions on the cloud, they are infinitely customisable and language-agnostic. Content authors should be able to create their own at will. However, we are aware that in a lot of cases, this grading logic will be similar, which is why a few functions have already been created. Evaluation functions - Quickstart Guide Response areas \u00b6 Response areas are components in the frontend where student users can enter a response. The response is sent to the evaluation function, which returns feedback to the response area. In the alpha version response areas are built into the software (rather than being modular) so are not straightforward to redevelop. This website catalogues the basic behaviour of response areas, to inform developers of evaluation functions. Response areas - overview System Architecture \u00b6 Technologies Deployment pipelines Hierarchy Future Features \u00b6","title":"Advanced users"},{"location":"advanced/#advanced-users","text":"Advanced users can develop their own evaluation functions.","title":"Advanced users"},{"location":"advanced/#evaluation-functions","text":"Evaluation functions are responsible for taking in a user's response, comparing it with a correct answer, and providing feedback to the frontend application. Living as containserized Lambda functions on the cloud, they are infinitely customisable and language-agnostic. Content authors should be able to create their own at will. However, we are aware that in a lot of cases, this grading logic will be similar, which is why a few functions have already been created. Evaluation functions - Quickstart Guide","title":"Evaluation functions"},{"location":"advanced/#response-areas","text":"Response areas are components in the frontend where student users can enter a response. The response is sent to the evaluation function, which returns feedback to the response area. In the alpha version response areas are built into the software (rather than being modular) so are not straightforward to redevelop. This website catalogues the basic behaviour of response areas, to inform developers of evaluation functions. Response areas - overview","title":"Response areas"},{"location":"advanced/#system-architecture","text":"Technologies Deployment pipelines Hierarchy","title":"System Architecture"},{"location":"advanced/#future-features","text":"","title":"Future Features"},{"location":"advanced/placeholder/","text":"coming soon \u00b6","title":"*coming soon*"},{"location":"advanced/placeholder/#coming-soon","text":"","title":"coming soon"},{"location":"advanced/evaluation_functions/","text":"Deployed Evaluation Functions \u00b6 Documentation for each of the functions registered to the LambdaFeedback platform are pulled in this section automatically. This is done using a custom MkDocs plugin EvalDocsLoader . If you can't see any documentation files under this section, please contact an admin.","title":"Deployed Evaluation Functions"},{"location":"advanced/evaluation_functions/#deployed-evaluation-functions","text":"Documentation for each of the functions registered to the LambdaFeedback platform are pulled in this section automatically. This is done using a custom MkDocs plugin EvalDocsLoader . If you can't see any documentation files under this section, please contact an admin.","title":"Deployed Evaluation Functions"},{"location":"advanced/evaluation_functions/alternate_languages/","text":"Alternate Evaluation Function Languages \u00b6 Lambda-Compatible Images \u00b6 Extending a pre-built Lambda image \u00b6 Available for: Node.js, Python, Java, .NET, Go, Ruby Docs Repo These base images are regularly updated, and the most widely used (more docs) They also come with pre-packaged runtime interface clients - a HTTP interface for runtimes to receive invocation events and respond Good for local development Creating custom base images \u00b6 Using the lambda/provided image This \"contains all the required components to run functions packaged as container images on Lambda\" Building a custom runtime from scratch Custom AWS Lambda runtimes Runtimes walkthrough tutorial Emulate execution locally? Lambda provides a runtime interface emulator (RIE) for you to test your function locally. The AWS base images for Lambda and base images for custom runtimes include the RIE. For other base images, you can download the Runtime interface emulator from the AWS GitHub repository. Misc Notes/Sources \u00b6 The Lambda Execution Environment Create Images from Alternative base images Development Philosophy \u00b6 Ultimately we want to call a function made by a user in any language. Two ways to do this: We write and provide runtime in all the different languages. This means that all the logic happens in that language. We write the code that actually receives the requests from lambda function events. In this case, the user function can be imported from those handlers. Writing handlers in each of those languages requires time and extensive knowledge (in order to write robust code) Handler code needs to: Have clean and reliable error catching We write a global runtime, which makes a call to their function via a sub-process. We call their script, which must recieve the payload as a commandline argument. User has to write more code For allowing cmdline arguments, and parsing of inputs Might be slower than in other languages. Since another script has to be executed.","title":"Alternate Function Languages"},{"location":"advanced/evaluation_functions/alternate_languages/#alternate-evaluation-function-languages","text":"","title":"Alternate Evaluation Function Languages"},{"location":"advanced/evaluation_functions/alternate_languages/#lambda-compatible-images","text":"","title":"Lambda-Compatible Images"},{"location":"advanced/evaluation_functions/alternate_languages/#extending-a-pre-built-lambda-image","text":"Available for: Node.js, Python, Java, .NET, Go, Ruby Docs Repo These base images are regularly updated, and the most widely used (more docs) They also come with pre-packaged runtime interface clients - a HTTP interface for runtimes to receive invocation events and respond Good for local development","title":"Extending a pre-built Lambda image"},{"location":"advanced/evaluation_functions/alternate_languages/#creating-custom-base-images","text":"Using the lambda/provided image This \"contains all the required components to run functions packaged as container images on Lambda\" Building a custom runtime from scratch Custom AWS Lambda runtimes Runtimes walkthrough tutorial Emulate execution locally? Lambda provides a runtime interface emulator (RIE) for you to test your function locally. The AWS base images for Lambda and base images for custom runtimes include the RIE. For other base images, you can download the Runtime interface emulator from the AWS GitHub repository.","title":"Creating custom base images"},{"location":"advanced/evaluation_functions/alternate_languages/#misc-notessources","text":"The Lambda Execution Environment Create Images from Alternative base images","title":"Misc Notes/Sources"},{"location":"advanced/evaluation_functions/alternate_languages/#development-philosophy","text":"Ultimately we want to call a function made by a user in any language. Two ways to do this: We write and provide runtime in all the different languages. This means that all the logic happens in that language. We write the code that actually receives the requests from lambda function events. In this case, the user function can be imported from those handlers. Writing handlers in each of those languages requires time and extensive knowledge (in order to write robust code) Handler code needs to: Have clean and reliable error catching We write a global runtime, which makes a call to their function via a sub-process. We call their script, which must recieve the payload as a commandline argument. User has to write more code For allowing cmdline arguments, and parsing of inputs Might be slower than in other languages. Since another script has to be executed.","title":"Development Philosophy"},{"location":"advanced/evaluation_functions/feedback/","text":"Base Layer Feedback Implementation \u00b6 Input structure: { \"response\" : \"user input\" , \"answer\" : \"original answer\" , \"params\" : { \"cases\" : [ { \"answer\" : \"same shape as original answer\" , \"feedback\" : \"feedback string\" , \"params\" : { ... } # A n y parame ters t o se t or override }, ... ] } } Execution Logic for the eval command \u00b6 First evaluation_function is called using the response, answer and params If evaluation threw an error, then return the error message If evaluation was successful, check for matching cases If \"params\" contains a non-empty list of \"cases\", determine the correct feedback, add it to the result and return the block (Logic for this is described in the next section) If \"params\" doesn't contain a list of cases, simply return the result Determining the correct feedback case \u00b6 Iterate through each case in the list of cases : Validate the case has an 'answer' and 'feedback' If the case contains 'params', then merge them with the original 'params', overwriting values if they already exist Call evaluation_function with the student \"response\", case \"answer\" and merged \"params\" If the function returns \"is_correct: true\", we have a match, store case and feedback returned from the evaluation function If the function returns an error, catch it and add it to a list of warnings If no matches were found, don't return any feedback If exactly one match was found, check if override_eval_feedback is in parameters If override_eval_feedback is set to true, return the case feedback If override_eval_feedback is not set or set to false, append the evaluation function feedback to the case feedback, separated by a linebreak and the return the result If more than one matches were found, return the first one (using the same procedure as if only one match was found) and add a warning explaining which cases matched, and why only the first was selected.","title":"Feedback"},{"location":"advanced/evaluation_functions/feedback/#base-layer-feedback-implementation","text":"Input structure: { \"response\" : \"user input\" , \"answer\" : \"original answer\" , \"params\" : { \"cases\" : [ { \"answer\" : \"same shape as original answer\" , \"feedback\" : \"feedback string\" , \"params\" : { ... } # A n y parame ters t o se t or override }, ... ] } }","title":"Base Layer Feedback Implementation"},{"location":"advanced/evaluation_functions/feedback/#execution-logic-for-the-eval-command","text":"First evaluation_function is called using the response, answer and params If evaluation threw an error, then return the error message If evaluation was successful, check for matching cases If \"params\" contains a non-empty list of \"cases\", determine the correct feedback, add it to the result and return the block (Logic for this is described in the next section) If \"params\" doesn't contain a list of cases, simply return the result","title":"Execution Logic for the eval command"},{"location":"advanced/evaluation_functions/feedback/#determining-the-correct-feedback-case","text":"Iterate through each case in the list of cases : Validate the case has an 'answer' and 'feedback' If the case contains 'params', then merge them with the original 'params', overwriting values if they already exist Call evaluation_function with the student \"response\", case \"answer\" and merged \"params\" If the function returns \"is_correct: true\", we have a match, store case and feedback returned from the evaluation function If the function returns an error, catch it and add it to a list of warnings If no matches were found, don't return any feedback If exactly one match was found, check if override_eval_feedback is in parameters If override_eval_feedback is set to true, return the case feedback If override_eval_feedback is not set or set to false, append the evaluation function feedback to the case feedback, separated by a linebreak and the return the result If more than one matches were found, return the first one (using the same procedure as if only one match was found) and add a warning explaining which cases matched, and why only the first was selected.","title":"Determining the correct feedback case"},{"location":"advanced/evaluation_functions/local/","text":"Running and Testing Functions Locally \u00b6 Simple \u00b6 Using Docker \u00b6 This method builds and runs evaluation functions in the same way they are deployed on AWS as Lambda functions. Extending a pre-built and AWS-maintained base python image , the container contains a HTTP client which can be used to locally simulate Lambda execution events. Note that this is different from the simple method proposed, in that it gives access to all the functionality provided by the base layer. This means that commands such as docs and healthcheck can be tested. Install Docker on your machine Navigate to the root directory of your function Build the image. This will pull our base image from Dockerhub, extend it with files specific to your evaluation function and name it eval-tmp . docker image build -t eval-tmp app Spin up a container using the image built in the previous step. docker run --rm -d --name eval-function -p 9000 :8080 eval-tmp You can now simulate requests to the function using any request client (like Insomnia or Postman ). By default, the url you can hit is: http://localhost:9000/2015-03-31/functions/function/invocations Warning When deployed, our Lambda functions are triggered by calls made through an AWS API Gateway . This means that when testing locally, events sent should follow the structure of events triggered by that resource. That is, if you want to simulate what it would be like to make web requests to the deployed function. Specifically, this means structuring requests in the following way: { \"headers\" : { \"command\" : \"eval\" }, \"body\" : { \"response\" : \"a\" , \"answer\" : \"a\" , \"params\" : { \"garlic\" : \"moreish\" } } } The main difference is that headers and body are sent as keys in the main body of the local request. When hitting the deployed function through the API Gateway, the command field would instead be passed in the actual HTTP headers of the request - and the actual request body would only contain the response , answer and params fields. (Optional) The run command specifies the -d flag, which spins up the container in detached mode. If you want to inspect the logs of the function, you can run: docker container logs -f eval-function Tip You will very rarely need this, but you can peek into the running container by opening a shell within it using: docker exec -it eval-function bash Useful Links \u00b6","title":"Testing Functions Locally"},{"location":"advanced/evaluation_functions/local/#running-and-testing-functions-locally","text":"","title":"Running and Testing Functions Locally"},{"location":"advanced/evaluation_functions/local/#simple","text":"","title":"Simple"},{"location":"advanced/evaluation_functions/local/#using-docker","text":"This method builds and runs evaluation functions in the same way they are deployed on AWS as Lambda functions. Extending a pre-built and AWS-maintained base python image , the container contains a HTTP client which can be used to locally simulate Lambda execution events. Note that this is different from the simple method proposed, in that it gives access to all the functionality provided by the base layer. This means that commands such as docs and healthcheck can be tested. Install Docker on your machine Navigate to the root directory of your function Build the image. This will pull our base image from Dockerhub, extend it with files specific to your evaluation function and name it eval-tmp . docker image build -t eval-tmp app Spin up a container using the image built in the previous step. docker run --rm -d --name eval-function -p 9000 :8080 eval-tmp You can now simulate requests to the function using any request client (like Insomnia or Postman ). By default, the url you can hit is: http://localhost:9000/2015-03-31/functions/function/invocations Warning When deployed, our Lambda functions are triggered by calls made through an AWS API Gateway . This means that when testing locally, events sent should follow the structure of events triggered by that resource. That is, if you want to simulate what it would be like to make web requests to the deployed function. Specifically, this means structuring requests in the following way: { \"headers\" : { \"command\" : \"eval\" }, \"body\" : { \"response\" : \"a\" , \"answer\" : \"a\" , \"params\" : { \"garlic\" : \"moreish\" } } } The main difference is that headers and body are sent as keys in the main body of the local request. When hitting the deployed function through the API Gateway, the command field would instead be passed in the actual HTTP headers of the request - and the actual request body would only contain the response , answer and params fields. (Optional) The run command specifies the -d flag, which spins up the container in detached mode. If you want to inspect the logs of the function, you can run: docker container logs -f eval-function Tip You will very rarely need this, but you can peek into the running container by opening a shell within it using: docker exec -it eval-function bash","title":"Using Docker "},{"location":"advanced/evaluation_functions/local/#useful-links","text":"","title":"Useful Links"},{"location":"advanced/evaluation_functions/module/","text":"evaluation-function-utils Package \u00b6 Error Reporting Schema validation Local testing Errors \u00b6 Submodule containing custom error and exception classes, which can be properly caught by the base evaluation layer, and return more detailed and appropriate errors. class EvaluationException \u00b6 This class extends the usual python Exception , with additional functionality. It can be used to package additional fields and values to errors thrown and returned by evaluation functions. Example If at some point in the execution of the evaluation_function , an error is thrown: from evaluation_function_utils.errors import EvaluationException if isinstance ( input , str ): raise EvaluationException ( \"The input must not be a string\" , valid_types = [ \"int\" , \"float\" , \"array\" ], ) Then the output generated by the lambda function will look like: { \"command\" : \"eval\" , \"error\" : { \"message\" : \"The input must not be a string\" , \"valid_types\" : [ \"int\" , \"float\" , \"array\" ] } } This class contains an error_dict property, which packages the additional arguments given to the Exception instance into a JSON-serializable object. It does so in an error-safe way, also reporting serialization errors if they occur. Client \u00b6 This submodule contains a custom EvaluationFunctionClient , which can be used to call other deployed evaluation functions. class EvaluationFunctionClient \u00b6 Client wrapped around the botocore.client.Lambda, for invoking deployed evaluation functions. On initialisation, it fetches credentials from environment variables \"INVOKER_KEY\", \"INVOKER_ID\" and \"INVOKER_REGION\", or from an optional environment file prescrived by env_path . Example from evaluation_function_utils.client import EvaluationFunctionClient client = EvaluationFunctionClient () def evaluation_function ( response , answer , params ): return client . invoke ( 'isExactEqual' , response , answer , params ) In this example, the evaluation_function completely offloads grading to the deployed 'isExactEqual' function. Note: The EvaluationFunctionClient.invoke method was designed to behave exactly as if the evaluation_function function defined in the targeted deployed function was called directly. This means that if errors are encountered an EvaluationException is raised.","title":"Evaluation Function Utils"},{"location":"advanced/evaluation_functions/module/#evaluation-function-utils-package","text":"Error Reporting Schema validation Local testing","title":"evaluation-function-utils Package"},{"location":"advanced/evaluation_functions/module/#errors","text":"Submodule containing custom error and exception classes, which can be properly caught by the base evaluation layer, and return more detailed and appropriate errors.","title":"Errors"},{"location":"advanced/evaluation_functions/module/#class-evaluationexception","text":"This class extends the usual python Exception , with additional functionality. It can be used to package additional fields and values to errors thrown and returned by evaluation functions. Example If at some point in the execution of the evaluation_function , an error is thrown: from evaluation_function_utils.errors import EvaluationException if isinstance ( input , str ): raise EvaluationException ( \"The input must not be a string\" , valid_types = [ \"int\" , \"float\" , \"array\" ], ) Then the output generated by the lambda function will look like: { \"command\" : \"eval\" , \"error\" : { \"message\" : \"The input must not be a string\" , \"valid_types\" : [ \"int\" , \"float\" , \"array\" ] } } This class contains an error_dict property, which packages the additional arguments given to the Exception instance into a JSON-serializable object. It does so in an error-safe way, also reporting serialization errors if they occur.","title":"class EvaluationException"},{"location":"advanced/evaluation_functions/module/#client","text":"This submodule contains a custom EvaluationFunctionClient , which can be used to call other deployed evaluation functions.","title":"Client"},{"location":"advanced/evaluation_functions/module/#class-evaluationfunctionclient","text":"Client wrapped around the botocore.client.Lambda, for invoking deployed evaluation functions. On initialisation, it fetches credentials from environment variables \"INVOKER_KEY\", \"INVOKER_ID\" and \"INVOKER_REGION\", or from an optional environment file prescrived by env_path . Example from evaluation_function_utils.client import EvaluationFunctionClient client = EvaluationFunctionClient () def evaluation_function ( response , answer , params ): return client . invoke ( 'isExactEqual' , response , answer , params ) In this example, the evaluation_function completely offloads grading to the deployed 'isExactEqual' function. Note: The EvaluationFunctionClient.invoke method was designed to behave exactly as if the evaluation_function function defined in the targeted deployed function was called directly. This means that if errors are encountered an EvaluationException is raised.","title":"class EvaluationFunctionClient"},{"location":"advanced/evaluation_functions/quickstart/","text":"Developing Evaluation Functions: Getting Started \u00b6 What is an Evaluation Function? \u00b6 It's a cloud function which performs some computation given some user input (the response ), a problem-specific source of truth (the answer ), and some optional parameters ( params ). Evaluation functions capture and automate the role of a teacher who has to keep marking the same question countless times. The simplest example for this would be one which checks for exact equivalence - where the function signals a response is correct only if it is identical to the answer . However, more complex and exotic ones such as symbolic expression equivalence and parsing of physical units can be imagined. Getting Setup for Development \u00b6 Get the code on your local machine (Using github desktop or the git cli) For new functions: create and clone a new repository using the boilerplate template . Make sure the new repository is set to public (it needs access to organisation secrets) . For existing functions: please make your changes on a new separate branch If you are creating a new function , you'll need to set it's name (as it will be deployed) in the config.json file, available in the root directory. The name must be unique. To view existing grading functions, go to: Staging API Gateway Integrations Production API Gateway Integrations You are now ready to start making changes and implementing features by editing each of the three main function-logic files: app/evaluation.py : This file contains the main evaluation_function function, which ultimately gets called to compare a response to an answer . evaluation.py Specification app/evaluation_tests.py : This is where you can test the logic in evaluation.py , following the standard unittest format. evaluation_tests.py Specification Documentation files: app/docs/dev.md : This file should be edited to reflect any changes/features implemented, following a developer perspective. It is baked into the function's image to be pulled by this documentation website under the deployed functions section. app/docs/user.md : This file documents how the function can be used by a teacher user, from the perspective of editing content on the LambdaFeedback platform. This time, files are collated and displayed in the Teacher section. Changes can be tested locally by running the tests you've written using: python -m unittest app/evaluation_tests.py Running and Testing Functions Locally Merge commits into the default branch will trigger the test-and-deploy.yml workflow, which will build the docker image, push it to a shared ECR repository, then call the backend grading-function/ensure route to build the necessary infrastructure to make the function available from the client app. You can now test the deployed evaluation function using your prefered request client (such as Insomnia or Postman or simply curl from a terminal). Functions are made available at: https://c1o0u8se7b.execute-api.eu-west-2.amazonaws.com/default/<function name as defined in config.json> Example Request to SymbolicEqual curl --request GET \\ --url https://c1o0u8se7b.execute-api.eu-west-2.amazonaws.com/default/symbolicEqual \\ --header 'Content-Type: application/json' \\ --header 'command: eval' \\ --data '{\"response\": \"x + x\", \"answer\": \"2*x\"}' In order to make your new function available on the LambdaFeedback platform, you have to register it via the Admin Panel . This is done by supplying its name, url (the same as the one above) and supported response types. More Info \u00b6 General Function Specification and Behaviour Function philosophy including deployment strategy Request/Response schemas and communication spec Base layer logic, properties and behaviour EvaluationFunctionUtils (python package) Error Reporting Schema validation Local testing","title":"Quickstart Guide"},{"location":"advanced/evaluation_functions/quickstart/#developing-evaluation-functions-getting-started","text":"","title":"Developing Evaluation Functions: Getting Started"},{"location":"advanced/evaluation_functions/quickstart/#what-is-an-evaluation-function","text":"It's a cloud function which performs some computation given some user input (the response ), a problem-specific source of truth (the answer ), and some optional parameters ( params ). Evaluation functions capture and automate the role of a teacher who has to keep marking the same question countless times. The simplest example for this would be one which checks for exact equivalence - where the function signals a response is correct only if it is identical to the answer . However, more complex and exotic ones such as symbolic expression equivalence and parsing of physical units can be imagined.","title":"What is an Evaluation Function?"},{"location":"advanced/evaluation_functions/quickstart/#getting-setup-for-development","text":"Get the code on your local machine (Using github desktop or the git cli) For new functions: create and clone a new repository using the boilerplate template . Make sure the new repository is set to public (it needs access to organisation secrets) . For existing functions: please make your changes on a new separate branch If you are creating a new function , you'll need to set it's name (as it will be deployed) in the config.json file, available in the root directory. The name must be unique. To view existing grading functions, go to: Staging API Gateway Integrations Production API Gateway Integrations You are now ready to start making changes and implementing features by editing each of the three main function-logic files: app/evaluation.py : This file contains the main evaluation_function function, which ultimately gets called to compare a response to an answer . evaluation.py Specification app/evaluation_tests.py : This is where you can test the logic in evaluation.py , following the standard unittest format. evaluation_tests.py Specification Documentation files: app/docs/dev.md : This file should be edited to reflect any changes/features implemented, following a developer perspective. It is baked into the function's image to be pulled by this documentation website under the deployed functions section. app/docs/user.md : This file documents how the function can be used by a teacher user, from the perspective of editing content on the LambdaFeedback platform. This time, files are collated and displayed in the Teacher section. Changes can be tested locally by running the tests you've written using: python -m unittest app/evaluation_tests.py Running and Testing Functions Locally Merge commits into the default branch will trigger the test-and-deploy.yml workflow, which will build the docker image, push it to a shared ECR repository, then call the backend grading-function/ensure route to build the necessary infrastructure to make the function available from the client app. You can now test the deployed evaluation function using your prefered request client (such as Insomnia or Postman or simply curl from a terminal). Functions are made available at: https://c1o0u8se7b.execute-api.eu-west-2.amazonaws.com/default/<function name as defined in config.json> Example Request to SymbolicEqual curl --request GET \\ --url https://c1o0u8se7b.execute-api.eu-west-2.amazonaws.com/default/symbolicEqual \\ --header 'Content-Type: application/json' \\ --header 'command: eval' \\ --data '{\"response\": \"x + x\", \"answer\": \"2*x\"}' In order to make your new function available on the LambdaFeedback platform, you have to register it via the Admin Panel . This is done by supplying its name, url (the same as the one above) and supported response types.","title":"Getting Setup for Development"},{"location":"advanced/evaluation_functions/quickstart/#more-info","text":"General Function Specification and Behaviour Function philosophy including deployment strategy Request/Response schemas and communication spec Base layer logic, properties and behaviour EvaluationFunctionUtils (python package) Error Reporting Schema validation Local testing","title":"More Info"},{"location":"advanced/evaluation_functions/specification/","text":"Evaluation Function Specification \u00b6 Introduction and Philosophy \u00b6 Functionality for each evaluation function is split up as follows: Universal function behaviour applicable to every function, such as the ability to run tests, return documentation and execute the evaluation is handled by the Base Layer . This is the docker image which is extended by every developed evaluation function. Functionality that may be required in more than one function (but not necessarily all), such as the ability to call already deployed functions and error reporting is handled by the evaluation_function_utils python package. This package comes pre-installed in the base layer, and can optionally be imported and called from the evaluation_function . Finally, specific comparison logic and handling of bespoke evaluation parameters is done in the custom evaluation_function , unique to each deployed instance. This is the logic that differenciates each function (comparing numbers, matrices, images, equations, graphs, text, tables, etc ...). Commands \u00b6 Commands are handled by the base layer . They define a unified interface for interacting with all deployed evaluation functions on the web. Practically, these are specified in the \"command\" request header. Example To execute the docs-user command for a function, the following header would be specified alonside the http request made to the endpoint on which the function is made available: ```bash curl --request GET \\ --url https://c1o0u8se7b.execute-api.eu-west-2.amazonaws.com/default/isExactEqual \\ --header 'command: docs-user' ``` eval \u00b6 This is the default command, used to compare a student's response and correct answer , given certain params . Outputs for this command depend on the success of the execution of the user-defined evaluation_function . If an error was thrown during execution, it is caught by the main handler and an error block is returned - otherwise, successful execution outputs are supplied under a result field. Output Structure: Successful evaluation { \"command\" : \"eval\" , \"result\" : { \"is_correct\" : \"<bool>\" , # Optional fields added by feedback generation (1) \"feedback\" : \"<string>\" , \"warnings\" : \"<array>\" # This output can also contain any number of fields given by `evaluation_function` } } See the Feedback Page for more information Output Structure: Error thrown during Execution { \"command\" : \"eval\" , \"error\" : { \"message\" : \"<string>\" , # Always present # This object can contain other number of additional fields # passed through by the EvaluationException (1) for debugging e.g.: \"serialization_errors\" : [], \"culprit\" : \"user\" , \"detail\" : \"...\" } } This is a custom error class from the evaluation-function-utils package, which developers are encouraged to use in order to output richer errors. See the Error handling section for more information. preview \u00b6 This command is similar to eval , except it doesn't return whether an answer is correct or provide feedback. Instead, preview provides a way for students view their response after some pre-processing, e.g. as rendered LaTeX when using Sympy for symbolic algebra. This should be faster to compute than eval , allowing students to get live preview of their response. healthcheck \u00b6 This command runs and returns a summary three testing suites: requests, responses and evaluation. Request and response tests check that inputs and outputs to the function work correctly, and follow the correct syntax. Evaluation tests are unique to each evaluation function and test the actual comparison logic. docs-user \u00b6 Command returns the docs/user.md file (base64 encoded) docs-dev \u00b6 Command returns the docs/dev.md file (base64 encoded) Base Layer \u00b6 File Structure \u00b6 A standard evaluation function repository based on the provided boilerplate will have the following file structure: app/ __init__.py evaluation.py # Script containing the main evaluation_function evaluation_tests.py # Unittests for the main evaluation_function requirements.txt # list of packages needed for algorithm.py Dockerfile # for building whole image to deploy to AWS docs/ # Documentation pages for this function (required) dev.md # Developer-oriented documentation user.md # LambdaFeedback content author documentation .github/ workflows/ test-and-deploy.yml # Testing and deployment pipeline config.json # Specify the name of the evaluation function in this file README.md .gitignore Warning If you want to split up function logic into different files, these must be added to the Dockerfile . This is so they are packaged with the built image when deployed. For example, if evaluation.py imports functionality from an app/utils.py file, then the following line must be added: 9 10 11 12 13 14 15 16 17 18 19 RUN pip3 install -r requirements.txt # Copy the evaluation and testing scripts COPY evaluation.py ./app/ COPY evaluation_tests.py ./app/ # Copy additional files COPY utils.py ./app/ # Copy Documentation COPY docs/dev.md ./app/docs/dev.md evaluation.py \u00b6 The entire framework, validation and testing developed around evaluation functions is ultimately used to get to this file, or the evaluation_function function within it, to be more precise. The evaluation_function \u00b6 Inputs \u00b6 All evaluation functions are passed three arguments: response : Data input by the user answer : Data to compare user input to (could be from a DB of answers, or pre-generated by other functions) params : Parameters which affect the comparison process (replacements, tolerances, feedbacks, ...) For evaluation functions that use Sympy or LaTeX for mathematical expressions, it's not always possible for a student to type the correct symbols. Instead we need to use simpler symbols. For example, \\(\\overline{U_{ij}}\\) cannot be written using standard sympy syntax, and therefore has to be substituted for something else, such as \"u\" or \"U\" . Therefore, evaluation functions using mathematical expressions should be able to handle multiple symbols to represent the same variable. To achieve this, every evaluation function is passed a symbols entry in params , to allow functions to convert a student's response: { \"response\" : \"user input\" , \"answer\" : \"model response to compare against\" , \"params\" : { \"symbols\" : { ... }, ... # params se t by t he tea cher } } symbols is a dictionary, where each key represents the main Sympy symbol (known as the code ), and has two entries: latex : the string used for rendering the symbol in LaTeX aliases : a list of alternative Sympy symbols that can be used by the student to represent the code . For the example above with \\(\\overline{U_{ij}}\\) , symbols would have the form: { ... \"params\" : { \"symbols\" : { \"u\" : { \"latex\" : \"\\\\overline{U_{ij}}\" , \"aliases\" : [ \"U\" ] } } } } Note that in JSON, special characters need to be escaped, so the latex symbol above will have a double-backslash instead. Currently, the backend only supports one LaTeX symbol for multiple Sympy symbols. In future, this will be a many-to-many relationship. Outputs \u00b6 The function should output a single JSON-encodable dictionary. Although a large amount of freedom is given to what this dict contains, when utilising the function alongside the lambdafeedback web app, a few values are expected/able to be consumed: is_correct: <bool> : Boolean parameter indicate whether the comparison between response and answer was deemed correct under the parameters. This field is then used by the web app to provide the most simple feedback to the user (green/red). Info More standardised function outputs that the frontend can consume are to come Error Handling \u00b6 Error reporting should follow a specific approach for all evaluation functions. If the evaluation_function you've written doesn't throw any errors, then it's output is returned under the result field - and assumed to have worked properly . This means that if you catch an error in your code manually, and simply return it - the frontend will assume everything went fine. Instead, errors can be handled in two ways: Letting evaluation_function fail : On the request handler in the Base Layer , the call to evaluation_function is wrapped in a try/except which catches any exception. This causes the evaluation to stop completely, returning a standard message, and a repr of the exception thrown in the error.detail field. Custom errors : If you want to report more detailed errors from your function, use the EvaluationException class provided in the evaluation-function-utils package. These are caught before all other standard exceptions, and are dealt with in a different way. These provide a way for your function to throw errors and stop executing safely, while supplying more accurate feedback to the front-end. Example It is discouraged to do the following in the evaluation code: python if something.bad.happened(): return { \"error\": { \"message\": \"Some important message\", \"other\": \"details\", } } As this causes the actual function output (by the AWS lambda function) to be: ```json { \"command\": \"eval\", \"result\": { \"error\": { \"message\": \"Some important message\", \"other\": \"details\" } } } ``` Instead, use custom exceptions from the [evaluation-function-utils](module.md#errors) package. ```python if something.bad.happened(): raise EvaluationException(message=\"Some important message\", other='details') ``` As the actual function output will look like: ```json { \"command\": \"eval\", \"error\": { \"message\": \"Some important message\", \"other\": \"details\" } } ``` This immediately indicates to the frontend client that something has gone wrong, allowing for proper feedback to be displayed. evaluation_tests.py \u00b6 Documentation \u00b6 Two essential and required documentation files are copied over during the creation of the evaluation function docker image. These are subsequently served by the function under the docs-dev and docs-user commands, to be accessed by this documentation website, as well as for embedding on LambdaFeedback. For more information about the markdown syntax, please refer to the following sources: MkDocs Documentation MkDocs-Material Documentation docs/dev.md \u00b6 docs/user.md \u00b6","title":"General Specification"},{"location":"advanced/evaluation_functions/specification/#evaluation-function-specification","text":"","title":"Evaluation Function Specification"},{"location":"advanced/evaluation_functions/specification/#introduction-and-philosophy","text":"Functionality for each evaluation function is split up as follows: Universal function behaviour applicable to every function, such as the ability to run tests, return documentation and execute the evaluation is handled by the Base Layer . This is the docker image which is extended by every developed evaluation function. Functionality that may be required in more than one function (but not necessarily all), such as the ability to call already deployed functions and error reporting is handled by the evaluation_function_utils python package. This package comes pre-installed in the base layer, and can optionally be imported and called from the evaluation_function . Finally, specific comparison logic and handling of bespoke evaluation parameters is done in the custom evaluation_function , unique to each deployed instance. This is the logic that differenciates each function (comparing numbers, matrices, images, equations, graphs, text, tables, etc ...).","title":"Introduction and Philosophy"},{"location":"advanced/evaluation_functions/specification/#commands","text":"Commands are handled by the base layer . They define a unified interface for interacting with all deployed evaluation functions on the web. Practically, these are specified in the \"command\" request header. Example To execute the docs-user command for a function, the following header would be specified alonside the http request made to the endpoint on which the function is made available: ```bash curl --request GET \\ --url https://c1o0u8se7b.execute-api.eu-west-2.amazonaws.com/default/isExactEqual \\ --header 'command: docs-user' ```","title":"Commands"},{"location":"advanced/evaluation_functions/specification/#eval","text":"This is the default command, used to compare a student's response and correct answer , given certain params . Outputs for this command depend on the success of the execution of the user-defined evaluation_function . If an error was thrown during execution, it is caught by the main handler and an error block is returned - otherwise, successful execution outputs are supplied under a result field. Output Structure: Successful evaluation { \"command\" : \"eval\" , \"result\" : { \"is_correct\" : \"<bool>\" , # Optional fields added by feedback generation (1) \"feedback\" : \"<string>\" , \"warnings\" : \"<array>\" # This output can also contain any number of fields given by `evaluation_function` } } See the Feedback Page for more information Output Structure: Error thrown during Execution { \"command\" : \"eval\" , \"error\" : { \"message\" : \"<string>\" , # Always present # This object can contain other number of additional fields # passed through by the EvaluationException (1) for debugging e.g.: \"serialization_errors\" : [], \"culprit\" : \"user\" , \"detail\" : \"...\" } } This is a custom error class from the evaluation-function-utils package, which developers are encouraged to use in order to output richer errors. See the Error handling section for more information.","title":"eval"},{"location":"advanced/evaluation_functions/specification/#preview","text":"This command is similar to eval , except it doesn't return whether an answer is correct or provide feedback. Instead, preview provides a way for students view their response after some pre-processing, e.g. as rendered LaTeX when using Sympy for symbolic algebra. This should be faster to compute than eval , allowing students to get live preview of their response.","title":"preview"},{"location":"advanced/evaluation_functions/specification/#healthcheck","text":"This command runs and returns a summary three testing suites: requests, responses and evaluation. Request and response tests check that inputs and outputs to the function work correctly, and follow the correct syntax. Evaluation tests are unique to each evaluation function and test the actual comparison logic.","title":"healthcheck"},{"location":"advanced/evaluation_functions/specification/#docs-user","text":"Command returns the docs/user.md file (base64 encoded)","title":"docs-user"},{"location":"advanced/evaluation_functions/specification/#docs-dev","text":"Command returns the docs/dev.md file (base64 encoded)","title":"docs-dev"},{"location":"advanced/evaluation_functions/specification/#base-layer","text":"","title":"Base Layer"},{"location":"advanced/evaluation_functions/specification/#file-structure","text":"A standard evaluation function repository based on the provided boilerplate will have the following file structure: app/ __init__.py evaluation.py # Script containing the main evaluation_function evaluation_tests.py # Unittests for the main evaluation_function requirements.txt # list of packages needed for algorithm.py Dockerfile # for building whole image to deploy to AWS docs/ # Documentation pages for this function (required) dev.md # Developer-oriented documentation user.md # LambdaFeedback content author documentation .github/ workflows/ test-and-deploy.yml # Testing and deployment pipeline config.json # Specify the name of the evaluation function in this file README.md .gitignore Warning If you want to split up function logic into different files, these must be added to the Dockerfile . This is so they are packaged with the built image when deployed. For example, if evaluation.py imports functionality from an app/utils.py file, then the following line must be added: 9 10 11 12 13 14 15 16 17 18 19 RUN pip3 install -r requirements.txt # Copy the evaluation and testing scripts COPY evaluation.py ./app/ COPY evaluation_tests.py ./app/ # Copy additional files COPY utils.py ./app/ # Copy Documentation COPY docs/dev.md ./app/docs/dev.md","title":"File Structure"},{"location":"advanced/evaluation_functions/specification/#evaluationpy","text":"The entire framework, validation and testing developed around evaluation functions is ultimately used to get to this file, or the evaluation_function function within it, to be more precise.","title":"evaluation.py"},{"location":"advanced/evaluation_functions/specification/#the-evaluation_function","text":"","title":"The evaluation_function"},{"location":"advanced/evaluation_functions/specification/#inputs","text":"All evaluation functions are passed three arguments: response : Data input by the user answer : Data to compare user input to (could be from a DB of answers, or pre-generated by other functions) params : Parameters which affect the comparison process (replacements, tolerances, feedbacks, ...) For evaluation functions that use Sympy or LaTeX for mathematical expressions, it's not always possible for a student to type the correct symbols. Instead we need to use simpler symbols. For example, \\(\\overline{U_{ij}}\\) cannot be written using standard sympy syntax, and therefore has to be substituted for something else, such as \"u\" or \"U\" . Therefore, evaluation functions using mathematical expressions should be able to handle multiple symbols to represent the same variable. To achieve this, every evaluation function is passed a symbols entry in params , to allow functions to convert a student's response: { \"response\" : \"user input\" , \"answer\" : \"model response to compare against\" , \"params\" : { \"symbols\" : { ... }, ... # params se t by t he tea cher } } symbols is a dictionary, where each key represents the main Sympy symbol (known as the code ), and has two entries: latex : the string used for rendering the symbol in LaTeX aliases : a list of alternative Sympy symbols that can be used by the student to represent the code . For the example above with \\(\\overline{U_{ij}}\\) , symbols would have the form: { ... \"params\" : { \"symbols\" : { \"u\" : { \"latex\" : \"\\\\overline{U_{ij}}\" , \"aliases\" : [ \"U\" ] } } } } Note that in JSON, special characters need to be escaped, so the latex symbol above will have a double-backslash instead. Currently, the backend only supports one LaTeX symbol for multiple Sympy symbols. In future, this will be a many-to-many relationship.","title":"Inputs"},{"location":"advanced/evaluation_functions/specification/#outputs","text":"The function should output a single JSON-encodable dictionary. Although a large amount of freedom is given to what this dict contains, when utilising the function alongside the lambdafeedback web app, a few values are expected/able to be consumed: is_correct: <bool> : Boolean parameter indicate whether the comparison between response and answer was deemed correct under the parameters. This field is then used by the web app to provide the most simple feedback to the user (green/red). Info More standardised function outputs that the frontend can consume are to come","title":"Outputs"},{"location":"advanced/evaluation_functions/specification/#error-handling","text":"Error reporting should follow a specific approach for all evaluation functions. If the evaluation_function you've written doesn't throw any errors, then it's output is returned under the result field - and assumed to have worked properly . This means that if you catch an error in your code manually, and simply return it - the frontend will assume everything went fine. Instead, errors can be handled in two ways: Letting evaluation_function fail : On the request handler in the Base Layer , the call to evaluation_function is wrapped in a try/except which catches any exception. This causes the evaluation to stop completely, returning a standard message, and a repr of the exception thrown in the error.detail field. Custom errors : If you want to report more detailed errors from your function, use the EvaluationException class provided in the evaluation-function-utils package. These are caught before all other standard exceptions, and are dealt with in a different way. These provide a way for your function to throw errors and stop executing safely, while supplying more accurate feedback to the front-end. Example It is discouraged to do the following in the evaluation code: python if something.bad.happened(): return { \"error\": { \"message\": \"Some important message\", \"other\": \"details\", } } As this causes the actual function output (by the AWS lambda function) to be: ```json { \"command\": \"eval\", \"result\": { \"error\": { \"message\": \"Some important message\", \"other\": \"details\" } } } ``` Instead, use custom exceptions from the [evaluation-function-utils](module.md#errors) package. ```python if something.bad.happened(): raise EvaluationException(message=\"Some important message\", other='details') ``` As the actual function output will look like: ```json { \"command\": \"eval\", \"error\": { \"message\": \"Some important message\", \"other\": \"details\" } } ``` This immediately indicates to the frontend client that something has gone wrong, allowing for proper feedback to be displayed.","title":"Error Handling"},{"location":"advanced/evaluation_functions/specification/#evaluation_testspy","text":"","title":"evaluation_tests.py"},{"location":"advanced/evaluation_functions/specification/#documentation","text":"Two essential and required documentation files are copied over during the creation of the evaluation function docker image. These are subsequently served by the function under the docs-dev and docs-user commands, to be accessed by this documentation website, as well as for embedding on LambdaFeedback. For more information about the markdown syntax, please refer to the following sources: MkDocs Documentation MkDocs-Material Documentation","title":"Documentation"},{"location":"advanced/evaluation_functions/specification/#docsdevmd","text":"","title":"docs/dev.md"},{"location":"advanced/evaluation_functions/specification/#docsusermd","text":"","title":"docs/user.md"},{"location":"advanced/response_areas/overview/","text":"Overview of response areas \u00b6 List of response areas \u00b6 The list of response areas is maintained in the Teacher section here . In the Developer area (here), the behaviour of the response areas is documented. Data types when submitting empty responses (default behaviour) \u00b6 If a user submits a response without inputing a value, the response areas convert the responses as follows before passing them to the evaluation functions: Input Value Type Default Value Comment number undefined [Behaviour needs updating] string undefined [Behaviour needs updating] MATRIX \" \" empty string in all cells TABLE \" \" empty string in all cells MULTIPLE CHOICE False all choices set to false","title":"Overview"},{"location":"advanced/response_areas/overview/#overview-of-response-areas","text":"","title":"Overview of response areas"},{"location":"advanced/response_areas/overview/#list-of-response-areas","text":"The list of response areas is maintained in the Teacher section here . In the Developer area (here), the behaviour of the response areas is documented.","title":"List of response areas"},{"location":"advanced/response_areas/overview/#data-types-when-submitting-empty-responses-default-behaviour","text":"If a user submits a response without inputing a value, the response areas convert the responses as follows before passing them to the evaluation functions: Input Value Type Default Value Comment number undefined [Behaviour needs updating] string undefined [Behaviour needs updating] MATRIX \" \" empty string in all cells TABLE \" \" empty string in all cells MULTIPLE CHOICE False all choices set to false","title":"Data types when submitting empty responses (default behaviour)"},{"location":"releases/","text":"Release 2024/07/16 \u00b6 b395-vertical-text-align - remove excess margin from top part of question Release 2024/06/28 \u00b6 b258-performance-analyse-db - faster response from DB queries b343-canvas - student canvas in beta mode (hidden by default) b351-teacher-module-page-ui-upgrades - Teacher module home page UI upgrades (tabs added) b357-upgrade-aws-sdk-v2-to-v3 - software library updates b359-generate-tex-file - upload/download whole Sets as LaTeX files Release 2024/06/14 \u00b6 b305-export-whole-set - import/export whole sets b334-upgrade-node-next-nest - library updates Release 2024/05/29 \u00b6 b352-support-to-eval-function-20-get-all-routes-by-getroutes - evaluation function deployment (check existing routes) b353-question-import-filter-out-unicode-characters - remove character (U-2006) on json import b354-double-confirm-on-confirmation-pop-ups - fix confirm button on modals in teacher mode Release 2024/05/24 \u00b6 b97-remove-all-references-to-mongodb - remove code linking to legacy databases b243-add-question-id-to-the-url - add question identifier to the url b285-move-module-instance-drop-down-to-left-to-replace-the-instance-label - more user-friendly module instance selection b304-milkdown-element-in-admin-that-will-display-on-home-page - add an administrator page to configure a home page banner b330-modal-update - question version switch: allow teacher either to save or discard existing draft b349-support-to-eval-function-20-ensure-deployments - evaluation functions: production and non-production versions Release 2024/05/15 \u00b6 b250-expression-ra-scan-mode-copy-and-paste - allow copy and paste and other improvements in the scan mode functionality b314-remove-experimental-from-ra-panel - \"experimental\" from the photo upload and handwritting labels removed b338-create-a-new-set-when-creating-a-module - a default set automatically creating when a new module is created b342-do-not-generate-pdf-when-creating-new-question - prevent PDF generation when a new question is added b345-after-pdf-generation-extraction-cleanup - a technical improvements into the PDF generation Release 2024/04/26 \u00b6 b230-pdf-generation-in-a-separate-lambda-function - PDF generation is faster, more secure, and frees up bandwidth on the main server. b328-improve-expression-ui-in-tests - fix expression response area preview in tests tab b331-work-solutions-empty-content-not-handled-as-no-work-solutions - work solutions and structured tutorials buttons not to be displayed if the content is empty b339-accessible-response-area-feedback - displaying error returned by evaluation functions in a user-friendly format b340-remove-input-type-changed-warning-on-new-ra - do not display warning about response area type change for new response areas Release 2024/04/19 \u00b6 337-individual-tests-always-fail - fix individual reponse area test runs 327-consolidate-response-area-components - improvements for better response area consistency Release 2024/03/21 \u00b6 b332-table-smart-resizing - resize table width based on the screen size Release 2024/03/19 \u00b6 b286-ra-analytics-when-config-is-changed - Fix aggregates in stats b311-expression-area-layout-issues - Improve expression RA layout b325-populate-new-tests-with-the-answer - Populate new tests with the answer b322-enable-live-preview-in-teacher-mode - Attempt to enable live preview in teacher mode b35/b329-simplify-response-components - Simplificaton of Response Type components code Release 2024/03/13 \u00b6 b84-legacy-content-db-tables - DB updates. No change to UX b315-include-answer-when-importing-case - ensure the answer value is included when importing a case from stats b320-response-type-allowlist - improves modular response areas b325-populate-new-tests-with-the-answer - prepopulate the answer by the correct answer when creating a new test b326-question-alignments - imroves alignments on the edit question page Release 2024/03/08 \u00b6 b283-table-with-1-column-layout - improves table layout b324-show-required-error-on-number-input-wizard - improves number input validations Release 2024/03/05 \u00b6 b301-redesign-part-response-areas-and-text-between-them - teachers can drag response areas while surrounding text stays in place, and merges where necessary. b35/b310-modular-response-areas-phase-6-cleanup - completes modular response areas. Code improvements and removing legacy tables. b323-delete-empty-answer-in-ra-panel - ensure delete works in answer box in response area panel Release 2024/03/04 \u00b6 b319-survey-promotion-banner-on-home-page - add a banner onto the landing page advertising a survey with a link Release 2024/03/01 \u00b6 b287-limit-access-to-sets-published-outside-of-current-date - ensure access to Sets follows release rules, including via URL b303-redirect-help-to-userdocs - redirect lambdafeedback.com/help to user documentation and lambdafeedback.com/[module slug] to the module page b318-url-for-survey - redirect lambdafeedback.com/survey Release 2024/02/29 \u00b6 b35/b308-modular-response-areas-phase-4-custom-response-types - allow admin to dynamically create and manage new response types b35/b309-modular-response-areas-phase-5-migration - migrate all existing response types to the new modular type b313-always-display-post-ra-text-in-pdf-but-not-in-stats-mode - include all text in PDF (including after first response area) Release 2024/02/26 \u00b6 b35-number-input-nan - fix handling of non-number input in the number answer wizard b317-no-header-refetch-on-mount - avoid unwanted refetch when resizing browser window on a set page b35/b307-modular-response-areas-phase-3-all-writes - start writting new and edited Response Area's Response to the new modular table b274-when-deleting-a-question-display-loading-message - display \"loading\" message when deleting a question Release 2024/02/20 \u00b6 b290-the-final-answer-button-is-displayed-even-if-there-is-no-final-answer - fix: only display 'final answer' button when there is content to show b297-give-error-if-creating-module-with-same-name-as-deleted-module - improved formatting of error messages b299-legacy-content-db-tables-ra-contents - DB updates. No change to UX. b302-modal-warning-before-disabling-branching - warning modal when disabling branching in worked solutions and structured tutorials b35/b306-modular-response-areas-phase-2-new-modular-type - backend updates for modular response areas. Release 2024/02/15 \u00b6 b35/b295-modular-response-areas-phase-1-switchless-frontend - a technical improvement to the response area building blocks in the code, so that it is easier, more intuitive and more straight forward to add new response areas Release 2024/02/13 \u00b6 b271-unify-modals - unified modals to use same style b294-check-imports-from-material-ui - prevent importing whole library when importing an icon b300-delete-ra-add-warning-into-the-modal-that-the-text-below-the-ra-will-be-deleted-as-well - when deleting a response area (RA), warning modal that text below RA will also be deleted Release 2024/01/25 \u00b6 b240-structured-tutorial-component-upgrade - converted structured tutorial to use the same structure and logic as worked solutions Release 2024/01/24 \u00b6 b273-limit-access-to-unpublished-sets - ensure no student access to hidden sets via a url b277-milkdown-first-non-markdown-update-is-ignored - milkdown fix to for edge cases that were not saved (single character; deleting selection). b279-table-with-1-column - wider columns for table response areas with one column b280-change-response-colour-to-white - specifically for 'riskAssessment' evaluation function: display feedback for incorrect answer in white colour b281-tweaks-to-ra-analytics - tweaks to response area analytics Release 2024/01/16 \u00b6 b272-legacy-db-tables-tutorial-sections - refactoring the database. No change to UX. Release 2024/01/10 \u00b6 b264-untangle-changes - a technical improvement to make the milkdown wrapper code clearer. b247-re-generate-pdf-after-deleting-a-question - an improvement so that the PDF is automatically re-generated when a published question is deleted b158-change-prod-bucket-to-prod-not-staging - a technical change so that imported images and generated PDF files are saved in the correct AWS bucket dependently on the environment (production, staging or development) b232-ra-analytics-visual-alignment - a change to display response area analytics correctly aligned with labels b77-published-question-change-of-input-type - an improvement to allow changing of the input type on the response area that was already published. b262-legacy-content-db-tables-part-contents - refactoring the database. No change to UX. b245-question-numbering-is-sometimes-wrong-on-the-student-module-home-page - a correction so that question numbers are reconciled after a question is deleted b141-update-link-in-modal - a correction of the link from the modal (which appears when deleting a response area) to the user documentation b211-response-area-preview-remove-border - a change in the question preview in the teacher mode so that it is displayed in the same way as in the student mode b103-milkdown-slow-rendering - a technical change to speed up testing in local development environments Release 2023/12/15 \u00b6 b103-milkdown-slow-rendering - developers can set a flag in local environment to speed up rendering pages with milkdown b235-content-with-hash-copied-across - prevent milkdown copying content with hash from one question to another b244-fix-notes-saving-in-the-student-mode - ensure student notes are visible including when switching from teacher to student mode b248-remove-unwanted-content-from-pdf - removed legacy response area pre-text and post-text from PDFs b251-post-a-reply-in-one-click - post a reply to a comment with one click b256-include-frequency-data-when-downloading-csv - correction to csv file generation for question stats, to include question numbers and frequency b260-number-and-unit-ra-do-not-align-with-pre-text-in-student-mode - align pre-text in the response area with number and units in student mode b261-master-content-sometimes-not-saved - ensure master content entered by the user is saved after publishing a question (not copied from the published version) Release 2023/12/08 \u00b6 b246-rendering-of-list-of-sets-in-teacher-mode-takes-long-time - an improvement to render list of sets in teacher mode quicker b255-recover-lost-marked-parts - further corrections to DB. Some question parts were not marked correctly as DONE for questions imported from JSON between 13/10/23 and 5/12/24. Release 2023/12/05 \u00b6 b242-mark-as-done-copied-across-questions - correction to DB submissions for questions imported from JSON between 13/10/23 and 5/12/24, which were linked together incorrectly. Release 2023/12/04 \u00b6 b224-add-guidance-to-help - guidance on a question, already visible to users in a widget on top-right, is now also visible with the support material below the question b228-legacy-content-db-tables-master-content - refactoring the database. No change to UX. b109-expression-input-tweaks - tweaks to the few improvements in the expression response area (555 in 2023/05/26 ): icons, placeholder, upload size limit. b249-selected-question-index-lost - editor UX, improve the robustness of: when a question is added or published, ensure that question remains in focus to the user. b241-link-from-feed-needs-updating - corrected a URL linking from the teacher feed to a question. Release 2023/11/13 \u00b6 b227-correct-set-estimates - time format improvement for displaying time estimate for each set in the list of set b233-publish-set-pdf-generation - an adjustment to the Publish whole set functionality to generate PDF after the confirmation button is clicked Release 2023/11/09 \u00b6 b186-add-time-estimates-for-each-set-in-teacher-mode - added set estimates which is calculated as summary of estimates of all questions b204-input-symbols-empty-row-should-not-be-validated - an improvement to prevent validation of input symbols when a new row to enter input symbols is added b206-input-symbols-with-spaces - an improvement to remove potential spaces entered into the input symbol alternatives (the values must be seaparated by comma without spaces to make sure they work correctly) b226-update-question-split-prisma-transaction - extended Prisma timeout when a question is being saved or publish b225-bug-in-timed-release-for-pm-times - a change to display hours in 24 hour format when displaying time Release 2023/11/03 \u00b6 b214-admin-dashboard-carry-on - admin dashboard improvements: A drop down list to select the time period for the user access events graph The last part of the graph lines are dotted to make clear that last values are subject to change Release 2023/11/01 \u00b6 b207-pressing-enter-in-the-flag-textbox - an improvement so that when a user is using an expression response area and he attempts to submit a comment (or flag a problem) at the same time by clicking the enter, then only the comment (or the problem message) is submitted (and not the answer in the response area) b213-question-export-import-to-handle-mp3 - an improvement to allow to export and import questions containing an audio (or more audios) b217-remove-header-text-on-module-page-for-students - removed the header on the student module page as it is not needed b215-do-not-update-or-delete-notes-in-teacher-preview - an improvement to prevent submitting student solutions in the teacher preview mode b208-unposted-comments - an imrovement to handle the scenario when a user enters a comment and then, withouth submitting it, selects different question (the comment was copied to the newly selected question which is not a desired feature) b209-zero-comments-invite-comments - an improvement to open comments when there are no comments to invite users to comment Release 2023/10/20 \u00b6 b202-ensure-eval-function-defaults-for-new-response-areas - an improvement so that evaluation function parameters are set to default values when creating a new response area b71-analytics-tweaks-teacher-view - the students list, view and contact pages were merged into a single page: - Filters by email and/or by access are available to filter the single list of students - A click on a student email opens a view which displays the same analytics the student can see b162-analytics-tweaks-stats-modal - improvements in the analytics view: - Colour is indicating the answer's case colour, if any, or the correct/incorrect default colour - Checkmark is indicating that the answer was correct - More options added to allow the user to agregate student answers b67-simplify-stats-interaction - few changes to response area statistics in the teacher mode: The case is imported straight into the relevant response area The response area menu has a new button EXPLORE so that the teacher can see the statistics per response area b192-reaction-count-one-hour-challenge - users can see the individual count of each type of reaction b183-activity-feed-make-clear-there-are-more-flags-than-5 - make clear to the user how many flags and comments there are in total as there might be more than 5 displayed on the teacher dashboard b110-import-multiple-jsons-from-a-single-zip - allows to import more questions from one zip file. This includes questions with attached pictures. Import of questions with attached audio files is yet to come. b205-admin-analytics-initial-work - first version of the admin dashboard is now provided. It includes information about number of current users, questions and user access events Release 2023/10/12 \u00b6 b180-prod-freezing-and-restarting - increasing allocated memory to accommodate multiple users triggering heavy processes (PDF compilation) b193-implement-auto-scaling-on-infrastructure - Infrastructure upgrades for larger scale usage. b124-question-export-with-pictures-fails-sometimes-on-cors-error - forcing Chrome to refresh media retriaval from S3 bucket to make sure correct headers are attached to the response b199-migration-script-for-physics-expression-ra - DB migration for legacy content. b194-create-set-and-first-question-improvement - new set automatically has a blank question ready. b197-not-possible-to-delete-a-question - increase timeout when deleting a question b143-more-info-in-modal-when-publish-whole-set - displaying list of questions that will be published in a modal before publishing whole set. b144-modal-to-check-before-removing-branches - a warning message is displayed before a branch from worked solutions is deleted Release 2023/10/05 \u00b6 b136-change-to-breadcrumbs - an improvement to remove module instances from teachers and students breadcrumbs as they do not link to any pages b188-add-information-when-rendering-a-new-question - adding information that a question is being created when adding a new question b189-failed-fetching-your-problem-set-message-appearing-when-it-should-not - a warning message 'Failed fetching your problem set' is to be displayed only if there is an error Release 2023/10/03 \u00b6 b191-expression-response-area-defaults - an improvement so that when creating a new response area of type EXPRESSION, the default values are set to: TRUE for Live preview FALSE for Display input symbols FALSE for Include in PDF TRUE for Enable handwriting input TRUE for Enable photo upload b187-support-materials-access-enhancements - enhancements to the support materals student access configuration: A new button event was added to record whether students proceeded or cancelled after a warning message appeared when a student tried to open a support material Labels were renamed to make their meaning clearer (e.g. 'Open' was changed to 'Available' and 'Hidden' to 'Unavailable') When a question part is marked as done, then no warning is displayed to a student when opening a support material (even if marked as Open with warnings) Release 2023/09/29 \u00b6 b148-problem-adding-new-question-after-changing-name-of-current-question - an improvement so that a user cannot start changing newly added question (e.g. changing name) until all processes are finished and therefore preventing these changes to be wiped out. b161-renaming-question-straight-after-making-it - this is the same problem as b148 b151-quote-marks-can-break-flags - an improvement so that double-quote marks, if used in a text, are displayed correctly in the generated csv file b164-grade-param-type-changed-reverts-to-string-when-value-is-empty - an improvment to identify a number as a number in the grade parameters, so that the type is displayed number and not as string b166-no-template-questions-in-the-list - an improvement to display all existing template questions in the list (when adding a new question from a template) b190-draw-area-width-keeps-changing - an improvement to stop the drawing area changing its width when a warning message is displayed that the writting cannot be interpreted Release 2023/09/27 \u00b6 b157-new-eval-function-reset-parameters - improvement in the response area panel, when the evaluation function is changed, then the default evaluation function parameters are re-set. b167-teachers-are-sent-to-the-most-recent-instance-on-the-module-homepage-even-when-they-dont-have-access-they-should-be-sent-to-the-most-recent-one-that-they-have-access-to b163-failed-fetching-your-problem-set-displayed-on-every-page-load - an improvement so that the warning message only appears when the fetch returns an error. b149-restrict-access-to-worked-solutions - restrict student access to support materials on set level and on question level. b165-preview-not-the-same-as-student-view - an improvement to displaye pre-text, value and post-text aligned horizontally in the response area student view Release 2023/09/08 \u00b6 128-feedback-area-does-not-support-latex-rendering - Feedbacks returned by the evaluation function are displayed using latex editor. Release 2023/09/07 \u00b6 b155-aws-ending-support-for-nodejs-14-in-aws-lambda - A clear-up of an outdated library. b153-pressing-enter-in-a-number-response-adds-new-line-to-the-response - Handle Enter in the response area as a submission of the answer. b139-archive-feature-enhancements - Enancements of module as module instance archiving. b68-cleaning-up-the-editor - many ui enhancements in the question editing page b115-case-color-under-feedback-tab-for-response-area-is-not-functional - The custom colour for feedbacks is now displayed correctly. Release 2023/08/30 \u00b6 b33-audio-clips - in the content editor, drag-and-drop an audio file, and it will add a sound (e.g. narration) to the content. b145-xetex-pdf - PDFs are now compiled with xelatex, not PDFlatex. b150-extracting-code-from-listener-into-callback-fn - stats for typed expressions now record full submissions only (not keystrokes) Release 2023/08/22 \u00b6 b147-time-guidance-is-currently-very-small - An adjustment after upgrading one of the libraries which caused the time guidance to shrink. b142-module-clone-enhancements - An enhancemnt to include links to already generated PDF files for all sets in the cloned module instance. b138-503-error - An enhancement to navigate to the teacher module / module instance after clicking Cancel button in the Set Metadata page. Release 2023/08/18 \u00b6 b114-matrix-input-centering-in-teacher-mode-but-not-in-student-mode - The Check button for matrix questions in the response area panel is now vertically centred in the student view. b140-response-area-pre-text-doubled - The legacy response area pre-text was removed from the student view. Release 2023/08/16 \u00b6 b127-cloned-instances-are-missing-tutorials-and-worked-solutions - An enhancement of the module cloning functionality to include worked solutions and tutorials. b125-when-publishing-question-update-the-student-view - An enhacement so that when a teacher publishes a question then, this question is visible in the student view without having to refresh the browser or log out and back in again. b83-revisit-set-archiving - This is a technical improvement of the existing functionality to archive sets so that it is done in the same way as archiving of other entities. It has no visible any impacts to a user. b111-archive-module-instance-option - A new feature to allow to archive a module instance. This feature is only available to an administrator. b126-archive-module-option - A new feature to allow to archive a module. This feature is only available to an administrator. b108-error-when-clicking-add-question-button-while-inside-part-content-box - Technical improvement. Upgrade of some libraries (Material UI) to prevent errors caused by issues in the older library version. Release 2023/07/21 \u00b6 b101-tests-run-from-the-configure-panel-have-the-islatex-parameter-set-to-true - A correction to the settings on the new Expression input (see 555 in 2023/05/26 ). When calling an evaluation function, the is_latex parameter dependends on the type of input (type/draw/scan). b120-PDF-skill-time-info - PDFs now include information on skill level, time estimates, and guidance below the question title and above the question content. b122-multi-year-carry-on - extended UI features referring to module instances (see b82 below). Release 2023/07/19 \u00b6 b82-multi-year-duplicate-module-instance-and-link-entities - new feature to clone module instances b118-multi-year-tidy-up - multi module feature enhancements such as sorting and filtering module instances on the admin Module page Release 2023/07/14 \u00b6 b72-multi-year-module-instances-introduction - All Modules now exist as an 'Instance' of a Module, in preparation for allowing multiple Instances. The UI navigation is updated to handle Module Instances. b81-show-preview-of-ra-in-input-type-select - Selecting an Input Type for a Response Area: a searchable preview of Input Types improves the UX: users see the preview while selecting. b91-prevent-multiple-blank-questions - When a question is added, the 'add quesiton' button is temporarily disabled while the application updates. b112-bug-the-tab-navigation-bar-at-the-top-disappears - Editor tabs are pesistent including during keyboard navigation b116-pdf-display-between-ras - PDF generation: for multiple Response Areas in a Part, the order is now always correct Release 2023/06/22 \u00b6 b39-new-editor-menus - question editor area menus have been converted into tabs. Other improvements have also been made to the editor layout inlcuding switching between teacher and student mode and staying on the same question. b62-add-tabs-to-reponse-area-panel - the Response Area panel is grouped into tabs that aid navigation and encourage a workflow that matches the way teachers think. Other layout improvements were also made within the tabs. b79-input-type-on-published-ra-should-not-be-editable - input type cannot be changed after publishing (see 598 here 2023/06/05 ). b85-incorrect-required-error-message - enhanced validation for number 0 in numeric response area. 588-question-import-export-handle-images - import export includes images; a zip file is used to combine the JSON and the images. 601-parameter-defaults-for-an-eval-function-cpq - improved the appearance of boolean evaluation function parameters. 603-user-docs-updates - user documentation repo renamed from \"documentation\" to \"user_documentation\". 608-link-word-sign-in-to-sign-in-on-homepage - on the home page 'sign in' text is now a link to sign in. 619-mcq-check-button-should-be-vertically-central - the Check button for multi-choice questions in the response area panel is now vertically centred. Release 2023/06/05 \u00b6 598-published-questions-change-of-approach - questions are now fully editable after publishing. All data from student responses persists through these changes. One exception is that the input type of a response area cannot be changed after publication, because this would change the format of the data that is recorded (you can, however, delete the response area and create a new one instead). Other new features: duplicate a Response Area; reorder Response Areas using drag and drop (in a similar way as reordering Parts). 613-enable-publish-whole-set - see 606 below ( 2023/05/26 ). The 'Publish Whole Set' button is now enabled. 614-error-with-stats-on-dev - ensures statistics still work with the new handwriting input (see 555 below). Release 2023/05/26 \u00b6 555-handwriting-response-area-upgrades - A new version of the Expression input type is in use. Input by handwriting onscreen or with scanned images is an option for teachers to make available to students (default: off). Also, regardless of the input mode (type/draw/scan) the live preview now gives 'pre-submission' feedback on whether the response can be interpreted, and the Check button is only available if interpretation is successful. 606-publish-whole-set-causing-stats-to-disappear - The 'Publish Whole Set' button in Teacher Edit mode has been disabled because it was causing data to become unlikned in the DB, giving the effect of data like number of completed parts 'disappearing'. Existing data has now been relinked and is all visible to users. The feature that caused the problem has been disabled while we prepare a replacement to be pushed shortly. 612-whole-part-marked-as-done-with-more-response-areas - Student functionality. If a question part has multiple Response Areas, the logic is now that only if all Response Areas are correctly answered will the 'Mark as done' feature be automatically checked. Previously only one correct answer was required to trigger this effect. 585-question-simple-import-and-export - Teacher functionality. The import/export functionality has been enhanced so that it Response Area parameters, cases, and tests are now all included. Release 2023/03/21 \u00b6 571-simple-teacher-comment-feed - Teacher functionality. New 'Activity feed' (formerly 'Flagged Questions') contains flagged questions and comments. The teacher can filter the table to see e.g. only flags or only comments. The teacher can also sort the table e.g. to see the new activities first. 569-numeric-input-strips-out-strings-that-may-have-meaning - Technical dept. For the Response Area input type 'Number', additional validation added; if the input contains a non-numeric value then a relevant error message is displayed to the user (this is linked to the 573-response-area-validation-specific-errors below). 573-response-area-validation-specific-errors - Teacher and student functionality. More specific error messages are displayed when the user inserts a value in an incorrect format (e.g. a non-numeric value into the input that expects a number). 582-empty-structured-tutorial-shouldnt-display - Technical debt. When a tutorial is deleted, it is not displayed at all to students (as opposed to being blank). 585-question-simple-import-and-export - Teacher functionality. Export a question to a file in JSON format. Import a question from a file in JSON format. Images are not imported/exported - these need to be handled manually until a new feature is ready. This feature opens the door to file imports if content can be converted into the correct format. 586-question-import-add-schema-validation - Teacher functionality. When importing a question from a file, the data structure and format is validated. If the validation fails then relevant error messages displayed to the user. Release 2023/03/06 \u00b6 566-pdf-error-identification - Teacher functionality. When a PDF fails to compile, the location of the error source is given in more detail, e.g. 'Q2(c)'. 576-orderedsetids-throws-error-in-main - Technical. When loading sets in a module on the teacher side, an error no longer appears in the console. 522-adding-teacher-when-creating-module-inadequate-error-message - Admin functionality. When adding a new module, a teacher can be added simultaneously. If the proposed teacher is not already registered as a teacher, then they are now automatically created as a teacher and a confirmation message is displayed. 524-remove-teacher-from-list - Admin functionality. Remove a teacher from the list of teachers. If the teacher is still a teacher on a module, then display a modal confirming which modules the teacher will be removed from. If the user confirms, the teacher is removed from all the modules and then they are deleted from the list of teachers. 572-comment-upvote-tweeks - Teacher and Student functionality. Right margin on the comments tweaked so that the sorting feature and 'post' button are not too far away from each other. 483-show-all-button - Teacher functionality. The Show All feature is now enabled in the question preview mode. 520-default-to-an-eval-function-after-selecting-the-response-area - Teacher functionality. In the Response area edit panel, automatically select a default eval function as follows (it can be edited by the teacher if necessary). The default selections are: Response area Default evaluation function MCQ arrayEqual NUMERIC isSimilar Expression and Text symbolicEqual Table and Matrix arraySymbolicEqual NUMERIC_UNITS comparePhysicalQuantities Release 2023/02/10 \u00b6 560-comment-feature-tweaks - UI improvements to the comments feature. 550-comment-upvotes - users can upvote comments by clicking on the heart. Sorting by upvotes is default 546-always-test-a-response-area-when-saving - [For teachers only] this is an invisible feature that automatically tests a response area, when closing the editing panel, to check that the correct answer is accepted as correct. A failure to pass this test will show an error and not save the response area until fixed. The reason for this feature is to catch things like empty cells in the teacher answer which, e.g. for a matrix, would make marking student answers impossible. If such errors were allowed to pass, then students would experience errors when using the response area - hence it cannot be allowed. The auto-test feature is not enabled for all response areas as some are not compatible - the option can be enabled/disabled for each response area by admins. 568-numeric-input-expects-string - upgrade to the numeric input type when dealing with string inputs.","title":"Releases"},{"location":"releases/#release-20240716","text":"b395-vertical-text-align - remove excess margin from top part of question","title":"Release 2024/07/16"},{"location":"releases/#release-20240628","text":"b258-performance-analyse-db - faster response from DB queries b343-canvas - student canvas in beta mode (hidden by default) b351-teacher-module-page-ui-upgrades - Teacher module home page UI upgrades (tabs added) b357-upgrade-aws-sdk-v2-to-v3 - software library updates b359-generate-tex-file - upload/download whole Sets as LaTeX files","title":"Release 2024/06/28"},{"location":"releases/#release-20240614","text":"b305-export-whole-set - import/export whole sets b334-upgrade-node-next-nest - library updates","title":"Release 2024/06/14"},{"location":"releases/#release-20240529","text":"b352-support-to-eval-function-20-get-all-routes-by-getroutes - evaluation function deployment (check existing routes) b353-question-import-filter-out-unicode-characters - remove character (U-2006) on json import b354-double-confirm-on-confirmation-pop-ups - fix confirm button on modals in teacher mode","title":"Release 2024/05/29"},{"location":"releases/#release-20240524","text":"b97-remove-all-references-to-mongodb - remove code linking to legacy databases b243-add-question-id-to-the-url - add question identifier to the url b285-move-module-instance-drop-down-to-left-to-replace-the-instance-label - more user-friendly module instance selection b304-milkdown-element-in-admin-that-will-display-on-home-page - add an administrator page to configure a home page banner b330-modal-update - question version switch: allow teacher either to save or discard existing draft b349-support-to-eval-function-20-ensure-deployments - evaluation functions: production and non-production versions","title":"Release 2024/05/24"},{"location":"releases/#release-20240515","text":"b250-expression-ra-scan-mode-copy-and-paste - allow copy and paste and other improvements in the scan mode functionality b314-remove-experimental-from-ra-panel - \"experimental\" from the photo upload and handwritting labels removed b338-create-a-new-set-when-creating-a-module - a default set automatically creating when a new module is created b342-do-not-generate-pdf-when-creating-new-question - prevent PDF generation when a new question is added b345-after-pdf-generation-extraction-cleanup - a technical improvements into the PDF generation","title":"Release 2024/05/15"},{"location":"releases/#release-20240426","text":"b230-pdf-generation-in-a-separate-lambda-function - PDF generation is faster, more secure, and frees up bandwidth on the main server. b328-improve-expression-ui-in-tests - fix expression response area preview in tests tab b331-work-solutions-empty-content-not-handled-as-no-work-solutions - work solutions and structured tutorials buttons not to be displayed if the content is empty b339-accessible-response-area-feedback - displaying error returned by evaluation functions in a user-friendly format b340-remove-input-type-changed-warning-on-new-ra - do not display warning about response area type change for new response areas","title":"Release 2024/04/26"},{"location":"releases/#release-20240419","text":"337-individual-tests-always-fail - fix individual reponse area test runs 327-consolidate-response-area-components - improvements for better response area consistency","title":"Release 2024/04/19"},{"location":"releases/#release-20240321","text":"b332-table-smart-resizing - resize table width based on the screen size","title":"Release 2024/03/21"},{"location":"releases/#release-20240319","text":"b286-ra-analytics-when-config-is-changed - Fix aggregates in stats b311-expression-area-layout-issues - Improve expression RA layout b325-populate-new-tests-with-the-answer - Populate new tests with the answer b322-enable-live-preview-in-teacher-mode - Attempt to enable live preview in teacher mode b35/b329-simplify-response-components - Simplificaton of Response Type components code","title":"Release 2024/03/19"},{"location":"releases/#release-20240313","text":"b84-legacy-content-db-tables - DB updates. No change to UX b315-include-answer-when-importing-case - ensure the answer value is included when importing a case from stats b320-response-type-allowlist - improves modular response areas b325-populate-new-tests-with-the-answer - prepopulate the answer by the correct answer when creating a new test b326-question-alignments - imroves alignments on the edit question page","title":"Release 2024/03/13"},{"location":"releases/#release-20240308","text":"b283-table-with-1-column-layout - improves table layout b324-show-required-error-on-number-input-wizard - improves number input validations","title":"Release 2024/03/08"},{"location":"releases/#release-20240305","text":"b301-redesign-part-response-areas-and-text-between-them - teachers can drag response areas while surrounding text stays in place, and merges where necessary. b35/b310-modular-response-areas-phase-6-cleanup - completes modular response areas. Code improvements and removing legacy tables. b323-delete-empty-answer-in-ra-panel - ensure delete works in answer box in response area panel","title":"Release 2024/03/05"},{"location":"releases/#release-20240304","text":"b319-survey-promotion-banner-on-home-page - add a banner onto the landing page advertising a survey with a link","title":"Release 2024/03/04"},{"location":"releases/#release-20240301","text":"b287-limit-access-to-sets-published-outside-of-current-date - ensure access to Sets follows release rules, including via URL b303-redirect-help-to-userdocs - redirect lambdafeedback.com/help to user documentation and lambdafeedback.com/[module slug] to the module page b318-url-for-survey - redirect lambdafeedback.com/survey","title":"Release 2024/03/01"},{"location":"releases/#release-20240229","text":"b35/b308-modular-response-areas-phase-4-custom-response-types - allow admin to dynamically create and manage new response types b35/b309-modular-response-areas-phase-5-migration - migrate all existing response types to the new modular type b313-always-display-post-ra-text-in-pdf-but-not-in-stats-mode - include all text in PDF (including after first response area)","title":"Release 2024/02/29"},{"location":"releases/#release-20240226","text":"b35-number-input-nan - fix handling of non-number input in the number answer wizard b317-no-header-refetch-on-mount - avoid unwanted refetch when resizing browser window on a set page b35/b307-modular-response-areas-phase-3-all-writes - start writting new and edited Response Area's Response to the new modular table b274-when-deleting-a-question-display-loading-message - display \"loading\" message when deleting a question","title":"Release 2024/02/26"},{"location":"releases/#release-20240220","text":"b290-the-final-answer-button-is-displayed-even-if-there-is-no-final-answer - fix: only display 'final answer' button when there is content to show b297-give-error-if-creating-module-with-same-name-as-deleted-module - improved formatting of error messages b299-legacy-content-db-tables-ra-contents - DB updates. No change to UX. b302-modal-warning-before-disabling-branching - warning modal when disabling branching in worked solutions and structured tutorials b35/b306-modular-response-areas-phase-2-new-modular-type - backend updates for modular response areas.","title":"Release 2024/02/20"},{"location":"releases/#release-20240215","text":"b35/b295-modular-response-areas-phase-1-switchless-frontend - a technical improvement to the response area building blocks in the code, so that it is easier, more intuitive and more straight forward to add new response areas","title":"Release 2024/02/15"},{"location":"releases/#release-20240213","text":"b271-unify-modals - unified modals to use same style b294-check-imports-from-material-ui - prevent importing whole library when importing an icon b300-delete-ra-add-warning-into-the-modal-that-the-text-below-the-ra-will-be-deleted-as-well - when deleting a response area (RA), warning modal that text below RA will also be deleted","title":"Release 2024/02/13"},{"location":"releases/#release-20240125","text":"b240-structured-tutorial-component-upgrade - converted structured tutorial to use the same structure and logic as worked solutions","title":"Release 2024/01/25"},{"location":"releases/#release-20240124","text":"b273-limit-access-to-unpublished-sets - ensure no student access to hidden sets via a url b277-milkdown-first-non-markdown-update-is-ignored - milkdown fix to for edge cases that were not saved (single character; deleting selection). b279-table-with-1-column - wider columns for table response areas with one column b280-change-response-colour-to-white - specifically for 'riskAssessment' evaluation function: display feedback for incorrect answer in white colour b281-tweaks-to-ra-analytics - tweaks to response area analytics","title":"Release 2024/01/24"},{"location":"releases/#release-20240116","text":"b272-legacy-db-tables-tutorial-sections - refactoring the database. No change to UX.","title":"Release 2024/01/16"},{"location":"releases/#release-20240110","text":"b264-untangle-changes - a technical improvement to make the milkdown wrapper code clearer. b247-re-generate-pdf-after-deleting-a-question - an improvement so that the PDF is automatically re-generated when a published question is deleted b158-change-prod-bucket-to-prod-not-staging - a technical change so that imported images and generated PDF files are saved in the correct AWS bucket dependently on the environment (production, staging or development) b232-ra-analytics-visual-alignment - a change to display response area analytics correctly aligned with labels b77-published-question-change-of-input-type - an improvement to allow changing of the input type on the response area that was already published. b262-legacy-content-db-tables-part-contents - refactoring the database. No change to UX. b245-question-numbering-is-sometimes-wrong-on-the-student-module-home-page - a correction so that question numbers are reconciled after a question is deleted b141-update-link-in-modal - a correction of the link from the modal (which appears when deleting a response area) to the user documentation b211-response-area-preview-remove-border - a change in the question preview in the teacher mode so that it is displayed in the same way as in the student mode b103-milkdown-slow-rendering - a technical change to speed up testing in local development environments","title":"Release 2024/01/10"},{"location":"releases/#release-20231215","text":"b103-milkdown-slow-rendering - developers can set a flag in local environment to speed up rendering pages with milkdown b235-content-with-hash-copied-across - prevent milkdown copying content with hash from one question to another b244-fix-notes-saving-in-the-student-mode - ensure student notes are visible including when switching from teacher to student mode b248-remove-unwanted-content-from-pdf - removed legacy response area pre-text and post-text from PDFs b251-post-a-reply-in-one-click - post a reply to a comment with one click b256-include-frequency-data-when-downloading-csv - correction to csv file generation for question stats, to include question numbers and frequency b260-number-and-unit-ra-do-not-align-with-pre-text-in-student-mode - align pre-text in the response area with number and units in student mode b261-master-content-sometimes-not-saved - ensure master content entered by the user is saved after publishing a question (not copied from the published version)","title":"Release 2023/12/15"},{"location":"releases/#release-20231208","text":"b246-rendering-of-list-of-sets-in-teacher-mode-takes-long-time - an improvement to render list of sets in teacher mode quicker b255-recover-lost-marked-parts - further corrections to DB. Some question parts were not marked correctly as DONE for questions imported from JSON between 13/10/23 and 5/12/24.","title":"Release 2023/12/08"},{"location":"releases/#release-20231205","text":"b242-mark-as-done-copied-across-questions - correction to DB submissions for questions imported from JSON between 13/10/23 and 5/12/24, which were linked together incorrectly.","title":"Release 2023/12/05"},{"location":"releases/#release-20231204","text":"b224-add-guidance-to-help - guidance on a question, already visible to users in a widget on top-right, is now also visible with the support material below the question b228-legacy-content-db-tables-master-content - refactoring the database. No change to UX. b109-expression-input-tweaks - tweaks to the few improvements in the expression response area (555 in 2023/05/26 ): icons, placeholder, upload size limit. b249-selected-question-index-lost - editor UX, improve the robustness of: when a question is added or published, ensure that question remains in focus to the user. b241-link-from-feed-needs-updating - corrected a URL linking from the teacher feed to a question.","title":"Release 2023/12/04"},{"location":"releases/#release-20231113","text":"b227-correct-set-estimates - time format improvement for displaying time estimate for each set in the list of set b233-publish-set-pdf-generation - an adjustment to the Publish whole set functionality to generate PDF after the confirmation button is clicked","title":"Release 2023/11/13"},{"location":"releases/#release-20231109","text":"b186-add-time-estimates-for-each-set-in-teacher-mode - added set estimates which is calculated as summary of estimates of all questions b204-input-symbols-empty-row-should-not-be-validated - an improvement to prevent validation of input symbols when a new row to enter input symbols is added b206-input-symbols-with-spaces - an improvement to remove potential spaces entered into the input symbol alternatives (the values must be seaparated by comma without spaces to make sure they work correctly) b226-update-question-split-prisma-transaction - extended Prisma timeout when a question is being saved or publish b225-bug-in-timed-release-for-pm-times - a change to display hours in 24 hour format when displaying time","title":"Release 2023/11/09"},{"location":"releases/#release-20231103","text":"b214-admin-dashboard-carry-on - admin dashboard improvements: A drop down list to select the time period for the user access events graph The last part of the graph lines are dotted to make clear that last values are subject to change","title":"Release 2023/11/03"},{"location":"releases/#release-20231101","text":"b207-pressing-enter-in-the-flag-textbox - an improvement so that when a user is using an expression response area and he attempts to submit a comment (or flag a problem) at the same time by clicking the enter, then only the comment (or the problem message) is submitted (and not the answer in the response area) b213-question-export-import-to-handle-mp3 - an improvement to allow to export and import questions containing an audio (or more audios) b217-remove-header-text-on-module-page-for-students - removed the header on the student module page as it is not needed b215-do-not-update-or-delete-notes-in-teacher-preview - an improvement to prevent submitting student solutions in the teacher preview mode b208-unposted-comments - an imrovement to handle the scenario when a user enters a comment and then, withouth submitting it, selects different question (the comment was copied to the newly selected question which is not a desired feature) b209-zero-comments-invite-comments - an improvement to open comments when there are no comments to invite users to comment","title":"Release 2023/11/01"},{"location":"releases/#release-20231020","text":"b202-ensure-eval-function-defaults-for-new-response-areas - an improvement so that evaluation function parameters are set to default values when creating a new response area b71-analytics-tweaks-teacher-view - the students list, view and contact pages were merged into a single page: - Filters by email and/or by access are available to filter the single list of students - A click on a student email opens a view which displays the same analytics the student can see b162-analytics-tweaks-stats-modal - improvements in the analytics view: - Colour is indicating the answer's case colour, if any, or the correct/incorrect default colour - Checkmark is indicating that the answer was correct - More options added to allow the user to agregate student answers b67-simplify-stats-interaction - few changes to response area statistics in the teacher mode: The case is imported straight into the relevant response area The response area menu has a new button EXPLORE so that the teacher can see the statistics per response area b192-reaction-count-one-hour-challenge - users can see the individual count of each type of reaction b183-activity-feed-make-clear-there-are-more-flags-than-5 - make clear to the user how many flags and comments there are in total as there might be more than 5 displayed on the teacher dashboard b110-import-multiple-jsons-from-a-single-zip - allows to import more questions from one zip file. This includes questions with attached pictures. Import of questions with attached audio files is yet to come. b205-admin-analytics-initial-work - first version of the admin dashboard is now provided. It includes information about number of current users, questions and user access events","title":"Release 2023/10/20"},{"location":"releases/#release-20231012","text":"b180-prod-freezing-and-restarting - increasing allocated memory to accommodate multiple users triggering heavy processes (PDF compilation) b193-implement-auto-scaling-on-infrastructure - Infrastructure upgrades for larger scale usage. b124-question-export-with-pictures-fails-sometimes-on-cors-error - forcing Chrome to refresh media retriaval from S3 bucket to make sure correct headers are attached to the response b199-migration-script-for-physics-expression-ra - DB migration for legacy content. b194-create-set-and-first-question-improvement - new set automatically has a blank question ready. b197-not-possible-to-delete-a-question - increase timeout when deleting a question b143-more-info-in-modal-when-publish-whole-set - displaying list of questions that will be published in a modal before publishing whole set. b144-modal-to-check-before-removing-branches - a warning message is displayed before a branch from worked solutions is deleted","title":"Release 2023/10/12"},{"location":"releases/#release-20231005","text":"b136-change-to-breadcrumbs - an improvement to remove module instances from teachers and students breadcrumbs as they do not link to any pages b188-add-information-when-rendering-a-new-question - adding information that a question is being created when adding a new question b189-failed-fetching-your-problem-set-message-appearing-when-it-should-not - a warning message 'Failed fetching your problem set' is to be displayed only if there is an error","title":"Release 2023/10/05"},{"location":"releases/#release-20231003","text":"b191-expression-response-area-defaults - an improvement so that when creating a new response area of type EXPRESSION, the default values are set to: TRUE for Live preview FALSE for Display input symbols FALSE for Include in PDF TRUE for Enable handwriting input TRUE for Enable photo upload b187-support-materials-access-enhancements - enhancements to the support materals student access configuration: A new button event was added to record whether students proceeded or cancelled after a warning message appeared when a student tried to open a support material Labels were renamed to make their meaning clearer (e.g. 'Open' was changed to 'Available' and 'Hidden' to 'Unavailable') When a question part is marked as done, then no warning is displayed to a student when opening a support material (even if marked as Open with warnings)","title":"Release 2023/10/03"},{"location":"releases/#release-20230929","text":"b148-problem-adding-new-question-after-changing-name-of-current-question - an improvement so that a user cannot start changing newly added question (e.g. changing name) until all processes are finished and therefore preventing these changes to be wiped out. b161-renaming-question-straight-after-making-it - this is the same problem as b148 b151-quote-marks-can-break-flags - an improvement so that double-quote marks, if used in a text, are displayed correctly in the generated csv file b164-grade-param-type-changed-reverts-to-string-when-value-is-empty - an improvment to identify a number as a number in the grade parameters, so that the type is displayed number and not as string b166-no-template-questions-in-the-list - an improvement to display all existing template questions in the list (when adding a new question from a template) b190-draw-area-width-keeps-changing - an improvement to stop the drawing area changing its width when a warning message is displayed that the writting cannot be interpreted","title":"Release 2023/09/29"},{"location":"releases/#release-20230927","text":"b157-new-eval-function-reset-parameters - improvement in the response area panel, when the evaluation function is changed, then the default evaluation function parameters are re-set. b167-teachers-are-sent-to-the-most-recent-instance-on-the-module-homepage-even-when-they-dont-have-access-they-should-be-sent-to-the-most-recent-one-that-they-have-access-to b163-failed-fetching-your-problem-set-displayed-on-every-page-load - an improvement so that the warning message only appears when the fetch returns an error. b149-restrict-access-to-worked-solutions - restrict student access to support materials on set level and on question level. b165-preview-not-the-same-as-student-view - an improvement to displaye pre-text, value and post-text aligned horizontally in the response area student view","title":"Release 2023/09/27"},{"location":"releases/#release-20230908","text":"128-feedback-area-does-not-support-latex-rendering - Feedbacks returned by the evaluation function are displayed using latex editor.","title":"Release 2023/09/08"},{"location":"releases/#release-20230907","text":"b155-aws-ending-support-for-nodejs-14-in-aws-lambda - A clear-up of an outdated library. b153-pressing-enter-in-a-number-response-adds-new-line-to-the-response - Handle Enter in the response area as a submission of the answer. b139-archive-feature-enhancements - Enancements of module as module instance archiving. b68-cleaning-up-the-editor - many ui enhancements in the question editing page b115-case-color-under-feedback-tab-for-response-area-is-not-functional - The custom colour for feedbacks is now displayed correctly.","title":"Release 2023/09/07"},{"location":"releases/#release-20230830","text":"b33-audio-clips - in the content editor, drag-and-drop an audio file, and it will add a sound (e.g. narration) to the content. b145-xetex-pdf - PDFs are now compiled with xelatex, not PDFlatex. b150-extracting-code-from-listener-into-callback-fn - stats for typed expressions now record full submissions only (not keystrokes)","title":"Release 2023/08/30"},{"location":"releases/#release-20230822","text":"b147-time-guidance-is-currently-very-small - An adjustment after upgrading one of the libraries which caused the time guidance to shrink. b142-module-clone-enhancements - An enhancemnt to include links to already generated PDF files for all sets in the cloned module instance. b138-503-error - An enhancement to navigate to the teacher module / module instance after clicking Cancel button in the Set Metadata page.","title":"Release 2023/08/22"},{"location":"releases/#release-20230818","text":"b114-matrix-input-centering-in-teacher-mode-but-not-in-student-mode - The Check button for matrix questions in the response area panel is now vertically centred in the student view. b140-response-area-pre-text-doubled - The legacy response area pre-text was removed from the student view.","title":"Release 2023/08/18"},{"location":"releases/#release-20230816","text":"b127-cloned-instances-are-missing-tutorials-and-worked-solutions - An enhancement of the module cloning functionality to include worked solutions and tutorials. b125-when-publishing-question-update-the-student-view - An enhacement so that when a teacher publishes a question then, this question is visible in the student view without having to refresh the browser or log out and back in again. b83-revisit-set-archiving - This is a technical improvement of the existing functionality to archive sets so that it is done in the same way as archiving of other entities. It has no visible any impacts to a user. b111-archive-module-instance-option - A new feature to allow to archive a module instance. This feature is only available to an administrator. b126-archive-module-option - A new feature to allow to archive a module. This feature is only available to an administrator. b108-error-when-clicking-add-question-button-while-inside-part-content-box - Technical improvement. Upgrade of some libraries (Material UI) to prevent errors caused by issues in the older library version.","title":"Release 2023/08/16"},{"location":"releases/#release-20230721","text":"b101-tests-run-from-the-configure-panel-have-the-islatex-parameter-set-to-true - A correction to the settings on the new Expression input (see 555 in 2023/05/26 ). When calling an evaluation function, the is_latex parameter dependends on the type of input (type/draw/scan). b120-PDF-skill-time-info - PDFs now include information on skill level, time estimates, and guidance below the question title and above the question content. b122-multi-year-carry-on - extended UI features referring to module instances (see b82 below).","title":"Release 2023/07/21"},{"location":"releases/#release-20230719","text":"b82-multi-year-duplicate-module-instance-and-link-entities - new feature to clone module instances b118-multi-year-tidy-up - multi module feature enhancements such as sorting and filtering module instances on the admin Module page","title":"Release 2023/07/19"},{"location":"releases/#release-20230714","text":"b72-multi-year-module-instances-introduction - All Modules now exist as an 'Instance' of a Module, in preparation for allowing multiple Instances. The UI navigation is updated to handle Module Instances. b81-show-preview-of-ra-in-input-type-select - Selecting an Input Type for a Response Area: a searchable preview of Input Types improves the UX: users see the preview while selecting. b91-prevent-multiple-blank-questions - When a question is added, the 'add quesiton' button is temporarily disabled while the application updates. b112-bug-the-tab-navigation-bar-at-the-top-disappears - Editor tabs are pesistent including during keyboard navigation b116-pdf-display-between-ras - PDF generation: for multiple Response Areas in a Part, the order is now always correct","title":"Release 2023/07/14"},{"location":"releases/#release-20230622","text":"b39-new-editor-menus - question editor area menus have been converted into tabs. Other improvements have also been made to the editor layout inlcuding switching between teacher and student mode and staying on the same question. b62-add-tabs-to-reponse-area-panel - the Response Area panel is grouped into tabs that aid navigation and encourage a workflow that matches the way teachers think. Other layout improvements were also made within the tabs. b79-input-type-on-published-ra-should-not-be-editable - input type cannot be changed after publishing (see 598 here 2023/06/05 ). b85-incorrect-required-error-message - enhanced validation for number 0 in numeric response area. 588-question-import-export-handle-images - import export includes images; a zip file is used to combine the JSON and the images. 601-parameter-defaults-for-an-eval-function-cpq - improved the appearance of boolean evaluation function parameters. 603-user-docs-updates - user documentation repo renamed from \"documentation\" to \"user_documentation\". 608-link-word-sign-in-to-sign-in-on-homepage - on the home page 'sign in' text is now a link to sign in. 619-mcq-check-button-should-be-vertically-central - the Check button for multi-choice questions in the response area panel is now vertically centred.","title":"Release 2023/06/22"},{"location":"releases/#release-20230605","text":"598-published-questions-change-of-approach - questions are now fully editable after publishing. All data from student responses persists through these changes. One exception is that the input type of a response area cannot be changed after publication, because this would change the format of the data that is recorded (you can, however, delete the response area and create a new one instead). Other new features: duplicate a Response Area; reorder Response Areas using drag and drop (in a similar way as reordering Parts). 613-enable-publish-whole-set - see 606 below ( 2023/05/26 ). The 'Publish Whole Set' button is now enabled. 614-error-with-stats-on-dev - ensures statistics still work with the new handwriting input (see 555 below).","title":"Release 2023/06/05"},{"location":"releases/#release-20230526","text":"555-handwriting-response-area-upgrades - A new version of the Expression input type is in use. Input by handwriting onscreen or with scanned images is an option for teachers to make available to students (default: off). Also, regardless of the input mode (type/draw/scan) the live preview now gives 'pre-submission' feedback on whether the response can be interpreted, and the Check button is only available if interpretation is successful. 606-publish-whole-set-causing-stats-to-disappear - The 'Publish Whole Set' button in Teacher Edit mode has been disabled because it was causing data to become unlikned in the DB, giving the effect of data like number of completed parts 'disappearing'. Existing data has now been relinked and is all visible to users. The feature that caused the problem has been disabled while we prepare a replacement to be pushed shortly. 612-whole-part-marked-as-done-with-more-response-areas - Student functionality. If a question part has multiple Response Areas, the logic is now that only if all Response Areas are correctly answered will the 'Mark as done' feature be automatically checked. Previously only one correct answer was required to trigger this effect. 585-question-simple-import-and-export - Teacher functionality. The import/export functionality has been enhanced so that it Response Area parameters, cases, and tests are now all included.","title":"Release 2023/05/26"},{"location":"releases/#release-20230321","text":"571-simple-teacher-comment-feed - Teacher functionality. New 'Activity feed' (formerly 'Flagged Questions') contains flagged questions and comments. The teacher can filter the table to see e.g. only flags or only comments. The teacher can also sort the table e.g. to see the new activities first. 569-numeric-input-strips-out-strings-that-may-have-meaning - Technical dept. For the Response Area input type 'Number', additional validation added; if the input contains a non-numeric value then a relevant error message is displayed to the user (this is linked to the 573-response-area-validation-specific-errors below). 573-response-area-validation-specific-errors - Teacher and student functionality. More specific error messages are displayed when the user inserts a value in an incorrect format (e.g. a non-numeric value into the input that expects a number). 582-empty-structured-tutorial-shouldnt-display - Technical debt. When a tutorial is deleted, it is not displayed at all to students (as opposed to being blank). 585-question-simple-import-and-export - Teacher functionality. Export a question to a file in JSON format. Import a question from a file in JSON format. Images are not imported/exported - these need to be handled manually until a new feature is ready. This feature opens the door to file imports if content can be converted into the correct format. 586-question-import-add-schema-validation - Teacher functionality. When importing a question from a file, the data structure and format is validated. If the validation fails then relevant error messages displayed to the user.","title":"Release 2023/03/21"},{"location":"releases/#release-20230306","text":"566-pdf-error-identification - Teacher functionality. When a PDF fails to compile, the location of the error source is given in more detail, e.g. 'Q2(c)'. 576-orderedsetids-throws-error-in-main - Technical. When loading sets in a module on the teacher side, an error no longer appears in the console. 522-adding-teacher-when-creating-module-inadequate-error-message - Admin functionality. When adding a new module, a teacher can be added simultaneously. If the proposed teacher is not already registered as a teacher, then they are now automatically created as a teacher and a confirmation message is displayed. 524-remove-teacher-from-list - Admin functionality. Remove a teacher from the list of teachers. If the teacher is still a teacher on a module, then display a modal confirming which modules the teacher will be removed from. If the user confirms, the teacher is removed from all the modules and then they are deleted from the list of teachers. 572-comment-upvote-tweeks - Teacher and Student functionality. Right margin on the comments tweaked so that the sorting feature and 'post' button are not too far away from each other. 483-show-all-button - Teacher functionality. The Show All feature is now enabled in the question preview mode. 520-default-to-an-eval-function-after-selecting-the-response-area - Teacher functionality. In the Response area edit panel, automatically select a default eval function as follows (it can be edited by the teacher if necessary). The default selections are: Response area Default evaluation function MCQ arrayEqual NUMERIC isSimilar Expression and Text symbolicEqual Table and Matrix arraySymbolicEqual NUMERIC_UNITS comparePhysicalQuantities","title":"Release 2023/03/06"},{"location":"releases/#release-20230210","text":"560-comment-feature-tweaks - UI improvements to the comments feature. 550-comment-upvotes - users can upvote comments by clicking on the heart. Sorting by upvotes is default 546-always-test-a-response-area-when-saving - [For teachers only] this is an invisible feature that automatically tests a response area, when closing the editing panel, to check that the correct answer is accepted as correct. A failure to pass this test will show an error and not save the response area until fixed. The reason for this feature is to catch things like empty cells in the teacher answer which, e.g. for a matrix, would make marking student answers impossible. If such errors were allowed to pass, then students would experience errors when using the response area - hence it cannot be allowed. The auto-test feature is not enabled for all response areas as some are not compatible - the option can be enabled/disabled for each response area by admins. 568-numeric-input-expects-string - upgrade to the numeric input type when dealing with string inputs.","title":"Release 2023/02/10"},{"location":"student/","text":"Student/User Documentation \u00b6 Question structure \u00b6 The image above shows an example question, with numbers to indicate: Breadcrumbs showing location Name of the Problem Set PDF version (link) Names of the questions in the Set, indicating which question is open Question number and name Guidance (expands on hover) Master content (always visible to student) Part selection (tabs) Part content (only visible when relevant part is open - (a),(b), etc.) Response area , where student responses are entered and feedback is given Feedback to the teacher (currently in flux regarding the design - 31/8/22) Access to content 'below the line' providing extra support. Below the line \u00b6 My solutions - create your own content. Drag and drop images or type content. Use standard markdown. Structured tutorial - teachers use this in different ways. It is generally a way to provide scaffolding if you're struggling. Final Answer - warning, don't ever look at the answer before you make your own genuine attempt at answering the question. Worked solutions - warning, don't ever look at the solutions before you make your own attempt. If necessary, look at the first line and reveal a step at a time.","title":"Student/User Documentation"},{"location":"student/#studentuser-documentation","text":"","title":"Student/User Documentation"},{"location":"student/#question-structure","text":"The image above shows an example question, with numbers to indicate: Breadcrumbs showing location Name of the Problem Set PDF version (link) Names of the questions in the Set, indicating which question is open Question number and name Guidance (expands on hover) Master content (always visible to student) Part selection (tabs) Part content (only visible when relevant part is open - (a),(b), etc.) Response area , where student responses are entered and feedback is given Feedback to the teacher (currently in flux regarding the design - 31/8/22) Access to content 'below the line' providing extra support.","title":"Question structure"},{"location":"student/#below-the-line","text":"My solutions - create your own content. Drag and drop images or type content. Use standard markdown. Structured tutorial - teachers use this in different ways. It is generally a way to provide scaffolding if you're struggling. Final Answer - warning, don't ever look at the answer before you make your own genuine attempt at answering the question. Worked solutions - warning, don't ever look at the solutions before you make your own attempt. If necessary, look at the first line and reveal a step at a time.","title":"Below the line"},{"location":"student/answering_questions/","text":"Answering Questions \u00b6 Overview \u00b6 The main view of a question is divided into two parts. The top half contains content that is relevant to the whole question, and the bottom half contains content for each individual part. Additionally, the part content may include one or more response areas . When you think you have answered the question, enter your answer into the response area, if it exists, and press the \"Check\" button to check your work. If you are correct, the question will be marked as \"done\". If there is no response area (e.g. for a \"show that...\" question), you can manually mark the question as done using the box at the bottom right. Answers and Worked Solutions \u00b6 If you are stuck, you can view worked solutions using the \"Worked Solutions\" option on the bottom ribbon. The steps in the solution are revealed step-by-step, so you should avoid the temptation to look at the whole solution at once, and try to complete as much as possible independantly. You can also view the answer to each question using the \"Final Answer\" option on the bottom ribbon. This contains the answer only, with no intermediate results or working. It is important that you always make your own genuine attempt to solve each problem before resorting to the final answers or the worked solutions. To help encourage this, a warning will appear if you try to access help before a question-specific time limit has elapsed.","title":"Answering Questions"},{"location":"student/answering_questions/#answering-questions","text":"","title":"Answering Questions"},{"location":"student/answering_questions/#overview","text":"The main view of a question is divided into two parts. The top half contains content that is relevant to the whole question, and the bottom half contains content for each individual part. Additionally, the part content may include one or more response areas . When you think you have answered the question, enter your answer into the response area, if it exists, and press the \"Check\" button to check your work. If you are correct, the question will be marked as \"done\". If there is no response area (e.g. for a \"show that...\" question), you can manually mark the question as done using the box at the bottom right.","title":"Overview"},{"location":"student/answering_questions/#answers-and-worked-solutions","text":"If you are stuck, you can view worked solutions using the \"Worked Solutions\" option on the bottom ribbon. The steps in the solution are revealed step-by-step, so you should avoid the temptation to look at the whole solution at once, and try to complete as much as possible independantly. You can also view the answer to each question using the \"Final Answer\" option on the bottom ribbon. This contains the answer only, with no intermediate results or working. It is important that you always make your own genuine attempt to solve each problem before resorting to the final answers or the worked solutions. To help encourage this, a warning will appear if you try to access help before a question-specific time limit has elapsed.","title":"Answers and Worked Solutions"},{"location":"student/faq/","text":"Frequently Asked Questions \u00b6 Why I cannot find the module I am looking for \u00b6 Access to each module is provided by the teacher owning the module. If you cannot find the module you are looking for, please contact your teacher.","title":"FAQ"},{"location":"student/faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"student/faq/#why-i-cannot-find-the-module-i-am-looking-for","text":"Access to each module is provided by the teacher owning the module. If you cannot find the module you are looking for, please contact your teacher.","title":"Why I cannot find the module I am looking for"},{"location":"student/getting_started_student/","text":"Get started as a student using Lambda Feedback \u00b6 Accessing content \u00b6 Log in \u00b6 Use your Imperial Microsoft account to sign in and access your modules. Once you sign in you should see a list of the modules you are enrolled in: You can see in this view the current progress of each module. Select a module \u00b6 Click on the module name to select it. You should now see a list of available problem sets. If none are available your teacher may not have assigned any yet. You can see on this view the current progress of each problem set. Select a problem set \u00b6 Select the problem set you wish to work on and you should see a list of questions on the left-hand side, with the selected question on the right. If a question has sub-parts, you can select them on the right. Accessing the PDF version of a problem set \u00b6 If you prefer to work on a PDF version of the problem set, you can generate a PDF by clicking the 'pdf' button underneath the problem set title Answering questions \u00b6 You can make progress on the problem by entering correct answers or clicking the 'Mark as done' button on the bottom right of each question page. This can be useful to track progress if working on the PDF version, or for questions which do not have a response box, e.g., show that questions. See the Answering Questions page for more help with answering questions.","title":"Getting Started"},{"location":"student/getting_started_student/#get-started-as-a-student-using-lambda-feedback","text":"","title":"Get started as a student using Lambda Feedback"},{"location":"student/getting_started_student/#accessing-content","text":"","title":"Accessing content"},{"location":"student/getting_started_student/#log-in","text":"Use your Imperial Microsoft account to sign in and access your modules. Once you sign in you should see a list of the modules you are enrolled in: You can see in this view the current progress of each module.","title":"Log in"},{"location":"student/getting_started_student/#select-a-module","text":"Click on the module name to select it. You should now see a list of available problem sets. If none are available your teacher may not have assigned any yet. You can see on this view the current progress of each problem set.","title":"Select a module"},{"location":"student/getting_started_student/#select-a-problem-set","text":"Select the problem set you wish to work on and you should see a list of questions on the left-hand side, with the selected question on the right. If a question has sub-parts, you can select them on the right.","title":"Select a problem set"},{"location":"student/getting_started_student/#accessing-the-pdf-version-of-a-problem-set","text":"If you prefer to work on a PDF version of the problem set, you can generate a PDF by clicking the 'pdf' button underneath the problem set title","title":"Accessing the PDF version of a problem set"},{"location":"student/getting_started_student/#answering-questions","text":"You can make progress on the problem by entering correct answers or clicking the 'Mark as done' button on the bottom right of each question page. This can be useful to track progress if working on the PDF version, or for questions which do not have a response box, e.g., show that questions. See the Answering Questions page for more help with answering questions.","title":"Answering questions"},{"location":"student/milkdown_student/","text":"Text Editing \u00b6 The Milkdown editor is widely used in Lambda Feedback wherever rich text input is required. On the student interface, it is used to add personal solution notes, and to write comments. It accepts: Standard Markdown \\(\\LaTeX\\) Images (paste or drag and drop) Videos (paste a URL) LaTeX \u00b6 LaTeX is a typesetting system widely used in academia to produce well-formatted documents. It is mostly used in Lambda Feedback for its capability to render complex mathematical expressions clearly and accurately. As an example, the following LaTeX code: \\int_V \\nabla \\cdot \\vec{f} \\mathrm{d}V = \\oint_S \\vec{f} \\cdot \\hat{n} \\mathrm{d}S Produces the following output: \\[ \\int_V \\nabla \\cdot \\vec{f} \\mathrm{d}V = \\oint_S \\vec{f} \\cdot \\hat{n} \\mathrm{d}S \\] In the Milkdown editor, anything surrounded by dollar signs (like $ x^2 $ ) will be interpreted as LaTeX. Only the subset supported by KaTeX , which includes most common LaTeX functions, can be used. LaTeX equations in 5 minutes \u00b6 Numbers and letters \u00b6 Numbers and Latin letters can be entered as you would expect: 1, 2, 3, 3.14159, -2.5, x, y, z \\[ 1, 2, 3, 3.14159, -2.5, x, y, z \\] Subscripts can be written with _ and superscripts can be written with ^ , for example in x^2 ( \\(x^2\\) ) or x_2 ( \\(x_2\\) ). Only the first character or command after a _ or ^ will be taken. To subscript or superscript multiple characters, they can be grouped in curly braces, like this: V_{ab} ( \\(V_{ab}\\) ). Basic functions \u00b6 Functions in LaTeX start with a backslash \\ . Some common functions include Greek letters: \\pi ( \\(\\pi\\) ) \\delta ( \\(\\delta\\) ) \\Delta ( \\(\\Delta\\) ) etc. Equalities: \\approx ( \\(\\approx\\) ) \\ne ( \\(\\ne\\) ) \\gt ( \\(\\gt\\) ) etc. Symbols/operators: - \\int ( \\(\\int\\) ) - \\sum ( \\(\\sum\\) ) - \\sin ( \\(\\sin\\) ) - \\ln ( \\(\\ln\\) ) - etc. Functions with arguments \u00b6 Some functions take arguments. Arguments are given between curly braces {} . Some commonly functions with arguments are: \\sqrt{x} ( \\(\\sqrt{x}\\) ). This places a square root sign around the argument. \\frac{x}{y} ( \\(\\frac{x}{y}\\) ). This command takes two arguments, and produces a fraction with the first argument in the numerator and the second on the denominator. Diacritics: \\vec{x} ( \\(\\vec{x}\\) ). \\dot{x} ( \\(\\dot{x}\\) ). \\hat{x} ( \\(\\hat{x}\\) ). etc. \\mathrm{x} ( \\(\\mathrm{x}\\) ). This formats the argument as regular, upright text, rather than italics. Should be used for units and operators (e.g. \\(\\frac{\\mathrm{d}y}{\\mathrm{d}x}\\) ). Nesting \u00b6 Functions can be nested arbitrarily. For example, a square root may contain a fraction, which may contain another square root: \\sqrt{ \\frac{-b \\pm \\sqrt{b^2 - 4ac }}{2a} } \\[\\sqrt{ \\frac{-b \\pm \\sqrt{b^2 - 4ac }}{2a} }\\] Going further \u00b6 If you are unsure of the correct function to use to produce the desired result, there is a list of all supported KaTeX functions here .","title":"Text Editing"},{"location":"student/milkdown_student/#text-editing","text":"The Milkdown editor is widely used in Lambda Feedback wherever rich text input is required. On the student interface, it is used to add personal solution notes, and to write comments. It accepts: Standard Markdown \\(\\LaTeX\\) Images (paste or drag and drop) Videos (paste a URL)","title":"Text Editing"},{"location":"student/milkdown_student/#latex","text":"LaTeX is a typesetting system widely used in academia to produce well-formatted documents. It is mostly used in Lambda Feedback for its capability to render complex mathematical expressions clearly and accurately. As an example, the following LaTeX code: \\int_V \\nabla \\cdot \\vec{f} \\mathrm{d}V = \\oint_S \\vec{f} \\cdot \\hat{n} \\mathrm{d}S Produces the following output: \\[ \\int_V \\nabla \\cdot \\vec{f} \\mathrm{d}V = \\oint_S \\vec{f} \\cdot \\hat{n} \\mathrm{d}S \\] In the Milkdown editor, anything surrounded by dollar signs (like $ x^2 $ ) will be interpreted as LaTeX. Only the subset supported by KaTeX , which includes most common LaTeX functions, can be used.","title":"LaTeX"},{"location":"student/milkdown_student/#latex-equations-in-5-minutes","text":"","title":"LaTeX equations in 5 minutes"},{"location":"student/milkdown_student/#numbers-and-letters","text":"Numbers and Latin letters can be entered as you would expect: 1, 2, 3, 3.14159, -2.5, x, y, z \\[ 1, 2, 3, 3.14159, -2.5, x, y, z \\] Subscripts can be written with _ and superscripts can be written with ^ , for example in x^2 ( \\(x^2\\) ) or x_2 ( \\(x_2\\) ). Only the first character or command after a _ or ^ will be taken. To subscript or superscript multiple characters, they can be grouped in curly braces, like this: V_{ab} ( \\(V_{ab}\\) ).","title":"Numbers and letters"},{"location":"student/milkdown_student/#basic-functions","text":"Functions in LaTeX start with a backslash \\ . Some common functions include Greek letters: \\pi ( \\(\\pi\\) ) \\delta ( \\(\\delta\\) ) \\Delta ( \\(\\Delta\\) ) etc. Equalities: \\approx ( \\(\\approx\\) ) \\ne ( \\(\\ne\\) ) \\gt ( \\(\\gt\\) ) etc. Symbols/operators: - \\int ( \\(\\int\\) ) - \\sum ( \\(\\sum\\) ) - \\sin ( \\(\\sin\\) ) - \\ln ( \\(\\ln\\) ) - etc.","title":"Basic functions"},{"location":"student/milkdown_student/#functions-with-arguments","text":"Some functions take arguments. Arguments are given between curly braces {} . Some commonly functions with arguments are: \\sqrt{x} ( \\(\\sqrt{x}\\) ). This places a square root sign around the argument. \\frac{x}{y} ( \\(\\frac{x}{y}\\) ). This command takes two arguments, and produces a fraction with the first argument in the numerator and the second on the denominator. Diacritics: \\vec{x} ( \\(\\vec{x}\\) ). \\dot{x} ( \\(\\dot{x}\\) ). \\hat{x} ( \\(\\hat{x}\\) ). etc. \\mathrm{x} ( \\(\\mathrm{x}\\) ). This formats the argument as regular, upright text, rather than italics. Should be used for units and operators (e.g. \\(\\frac{\\mathrm{d}y}{\\mathrm{d}x}\\) ).","title":"Functions with arguments"},{"location":"student/milkdown_student/#nesting","text":"Functions can be nested arbitrarily. For example, a square root may contain a fraction, which may contain another square root: \\sqrt{ \\frac{-b \\pm \\sqrt{b^2 - 4ac }}{2a} } \\[\\sqrt{ \\frac{-b \\pm \\sqrt{b^2 - 4ac }}{2a} }\\]","title":"Nesting"},{"location":"student/milkdown_student/#going-further","text":"If you are unsure of the correct function to use to produce the desired result, there is a list of all supported KaTeX functions here .","title":"Going further"},{"location":"student/response_areas/","text":"Response Areas \u00b6 Numerical answers \u00b6 This type of response area expects a numerical answer. Usually, a tolerance is allowed, so you will still be marked as correct if your response differs from the answer by a small amount. Some questions will also require you to enter the units of the answer. In this case, any form of the same unit, with any SI prefix, should be accepted. For example, if the answer to a question is 10 MPa , 0.01 GPa and 10 MNm^-2 should both be marked correct. Entering mathematical expressions \u00b6 Entering mathematical expressions on Lambda is very similar to if you was doing it on Matlab, for example. Examples \u00b6 Expression Lambda Feedback input \\(x^2 - x - 2\\) x^2 - x - 2 Reference \u00b6 Operator Symbol Lambda Feedback input Addition \\(a + b\\) a+b Subtraction \\(a - b\\) a-b Multiplication \\(a \\times b\\) a*b Division \\(\\frac{a}{b}\\) a/b Exponentiation \\(a^b\\) a^b Square root \\(\\sqrt{a}\\) sqrt(a) Types of response area \u00b6 Name Description Question Name Here we can edit the name given to the question","title":"Response Areas"},{"location":"student/response_areas/#response-areas","text":"","title":"Response Areas"},{"location":"student/response_areas/#numerical-answers","text":"This type of response area expects a numerical answer. Usually, a tolerance is allowed, so you will still be marked as correct if your response differs from the answer by a small amount. Some questions will also require you to enter the units of the answer. In this case, any form of the same unit, with any SI prefix, should be accepted. For example, if the answer to a question is 10 MPa , 0.01 GPa and 10 MNm^-2 should both be marked correct.","title":"Numerical answers"},{"location":"student/response_areas/#entering-mathematical-expressions","text":"Entering mathematical expressions on Lambda is very similar to if you was doing it on Matlab, for example.","title":"Entering mathematical expressions"},{"location":"student/response_areas/#examples","text":"Expression Lambda Feedback input \\(x^2 - x - 2\\) x^2 - x - 2","title":"Examples"},{"location":"student/response_areas/#reference","text":"Operator Symbol Lambda Feedback input Addition \\(a + b\\) a+b Subtraction \\(a - b\\) a-b Multiplication \\(a \\times b\\) a*b Division \\(\\frac{a}{b}\\) a/b Exponentiation \\(a^b\\) a^b Square root \\(\\sqrt{a}\\) sqrt(a)","title":"Reference"},{"location":"student/response_areas/#types-of-response-area","text":"Name Description Question Name Here we can edit the name given to the question","title":"Types of response area"},{"location":"teacher/","text":"Teacher and Content Author Documentation \u00b6 In the 2022/23 academic year Lambda Feedback is in its alpha version. We are now in our Beta version. This means that some features are incompleted, and that the documentation is not ready yet. All teacher-users should be in direct contact with someone on the development team so that you can ask for support. Please provide feedback on the documentation as it develops. We will populate this area more soon. Try Getting Started ...","title":"Teacher and Content Author Documentation"},{"location":"teacher/#teacher-and-content-author-documentation","text":"In the 2022/23 academic year Lambda Feedback is in its alpha version. We are now in our Beta version. This means that some features are incompleted, and that the documentation is not ready yet. All teacher-users should be in direct contact with someone on the development team so that you can ask for support. Please provide feedback on the documentation as it develops. We will populate this area more soon. Try Getting Started ...","title":"Teacher and Content Author Documentation"},{"location":"teacher/guides/analytics/","text":"Analytics Guide \u00b6 Analytics begin when a question is published . After publishing a question for the first time it becomes available to students and their usage is logged and fed back to the student and the teacher. Analytics History \u00b6 Analytics are linked to response areas. Each question can have more response areas and they can be added or removed. When a response area is removed, then it is removed only from the \"current version\" of the question (the version that the teacher is editing) and it persists on the previous version(s) of the question. It means that all submissions and analytics remain, but they are now linked to the response area which only exists on a previous version(s) of the question. Currently it is possible to see only analytics against the published version of the question. We are now working on the improvement so that it is possible to see analytics against all reponse areas (including those that exist only on previous versions of the question). Tracking students' response \u00b6 To improve the feedback that students receive and to better understand which areas they need help with, it is possible to check the different student responses and the frequency of each response for each response area. To see these statistics: Click on the Stats tab in teacher mode Then click on Explore in the top right corner of each response area. You can even export these statistics as csv file!","title":"Analytics"},{"location":"teacher/guides/analytics/#analytics-guide","text":"Analytics begin when a question is published . After publishing a question for the first time it becomes available to students and their usage is logged and fed back to the student and the teacher.","title":"Analytics Guide"},{"location":"teacher/guides/analytics/#analytics-history","text":"Analytics are linked to response areas. Each question can have more response areas and they can be added or removed. When a response area is removed, then it is removed only from the \"current version\" of the question (the version that the teacher is editing) and it persists on the previous version(s) of the question. It means that all submissions and analytics remain, but they are now linked to the response area which only exists on a previous version(s) of the question. Currently it is possible to see only analytics against the published version of the question. We are now working on the improvement so that it is possible to see analytics against all reponse areas (including those that exist only on previous versions of the question).","title":"Analytics History"},{"location":"teacher/guides/analytics/#tracking-students-response","text":"To improve the feedback that students receive and to better understand which areas they need help with, it is possible to check the different student responses and the frequency of each response for each response area. To see these statistics: Click on the Stats tab in teacher mode Then click on Explore in the top right corner of each response area. You can even export these statistics as csv file!","title":"Tracking students' response"},{"location":"teacher/guides/content-sets-questions/","text":"Editing questions \u00b6 In this guide, we will walk through how to create sets and questions . Click on a Set in order to edit or add questions. A guide to the editor: Label Name Description 1 Question Name Here we can edit the name given to the question 2 Master content Always visible. Uses a milkdown component. 3 Current part Referenes which question part we are editing 4 Part Content Here we can edit the specific content of the part (the sub question). 5 Response Area This is the means by which the student answers a given question. It is optional to include a response area. 6 Question Help Options Here we may add a Structured Tutorial , a Final Answer (\"Show Answer\"), or Worked Solutions . These buttons are also how the student will see them, hence the name of \"Show Answer\". 7 Teacher-Student View Toggle Here we may toggle between the teacher view (\"EDIT\"), and the student view (\"PREVIEW\"). This way we can edit the question, but also see a preview of how it would look to the student. 8 Edit Guidance Here we can add further details for guidance, estimated working time and skill level 9 Part option Here we can add, duplicate or (if there are more than one) delete a question part 10 Add question Here we can add create a new blank question, duplicate a question or upload a zip file containing JSON file(s) for Lambda Feedback. 11 File, Preview and Stats Travel to pages to a) File: version management/ download as JSON and delete question b) Preview: view the question as a student would see it c) Stats: View stats of student responses of the question set.","title":"Editing questions"},{"location":"teacher/guides/content-sets-questions/#editing-questions","text":"In this guide, we will walk through how to create sets and questions . Click on a Set in order to edit or add questions. A guide to the editor: Label Name Description 1 Question Name Here we can edit the name given to the question 2 Master content Always visible. Uses a milkdown component. 3 Current part Referenes which question part we are editing 4 Part Content Here we can edit the specific content of the part (the sub question). 5 Response Area This is the means by which the student answers a given question. It is optional to include a response area. 6 Question Help Options Here we may add a Structured Tutorial , a Final Answer (\"Show Answer\"), or Worked Solutions . These buttons are also how the student will see them, hence the name of \"Show Answer\". 7 Teacher-Student View Toggle Here we may toggle between the teacher view (\"EDIT\"), and the student view (\"PREVIEW\"). This way we can edit the question, but also see a preview of how it would look to the student. 8 Edit Guidance Here we can add further details for guidance, estimated working time and skill level 9 Part option Here we can add, duplicate or (if there are more than one) delete a question part 10 Add question Here we can add create a new blank question, duplicate a question or upload a zip file containing JSON file(s) for Lambda Feedback. 11 File, Preview and Stats Travel to pages to a) File: version management/ download as JSON and delete question b) Preview: view the question as a student would see it c) Stats: View stats of student responses of the question set.","title":"Editing questions"},{"location":"teacher/guides/faq/","text":"Frequently Asked Questions \u00b6 How can I enroll students on my new Module \u00b6 Students and users are given access to a module using their college email address (from microsoft). Login and navigate to your Teacher dashboard Select the module on which you want to enroll students When on the module page, click the View Students button Enter the enrolment page by clicking the Entroll Students button Enroll students by supplying one or more student email addresses How can I move questions between problem sets? \u00b6 When creating a new question the teacher can choose to \"clone\" from an existing question. The teacher can then delete the original version. In the problem set you wish to move the question to, select the v symbol to the right of the Add Question button From the dropdown menu, select Clone From Question Select the title of the question you wish clone from the list that appear If you wish, go back and delete the question from its original location How can I share a link to a Problem Set? \u00b6 To share a link with students, open the Problem Set in STUDENT mode (light blue top bar), and copy the URL from the browser. To share a link with teachers who will access the content editor and analytics, share a link from TEACHER mode (orange top bar); students won't be able to access this link. How can I set parameters for evaluation functions? \u00b6 The most common parameters will be visible uder the EVALUATE tab in the configure panel. If there is a parameter that is not already visible it can be set using the Advanced - raw parameters (also under the EVALUATE tab) by doing the following: Hover over the list of parameters in the Advanced - raw parameters area. Click the green plus-symbol that appears. Type the name of the parameter (without quotation marks). Hover over the box that says NULL next to the newly added parameter. Click the green pen symbol that appears to the right of it. Type in the desired value in box that appears. By default it will be assumed that the parameter value is a string. The webclient will infer other possible types based on the written input. If the setting should be a string, click the green checkmark to the top right, and if you want the inferred type click the green checkmark at the bottom right. The parameter is now set. How do I reorder questions? \u00b6 It is only possible to reorder published questions in a set. This prevents inadvertently inserting new questions between two published ones. This ensures consistency to the student when viewing a published set as existing questions will remain in an unchanged order, with new questions being added to the bottom (unless manually changed by the teacher). You can tell a question is unpublished as it will take the ' 1.X ' numbering format To reorder questions: Publish the questions you wish to reorder using FILE > SAVE AND PUBLISH (alternatively click on the PUBLISH WHOLE SET button) Refresh the page Drag and drop the questions into the new order Ensure the green box pops up saying: 'questions reordered successfully' - there is no need to republish the set What to do when \\space is not showing in the pdf generated by lambda feedback? \u00b6 The Pandoc library that lambdafeedback use to generate a pdf does not support \\space . Alternatives that could be use to generate a space in math block is to use the tilde symbol ~ or \\, for thinner spacing. What to do if the pdf is not compiling my inline math equation? \u00b6 Please check if there is an additional space at the start or a the end of the equation. This is usually the cause for inline math blocks not compiling. How can I have the same font for the unit and for the number in the math block? \u00b6 You can use the code \\mathrm{} or {\\rm} . Both code will give you your units in serifed Times New Roman, which is the same font as the number in the math block when compiled.","title":"FAQ"},{"location":"teacher/guides/faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"teacher/guides/faq/#how-can-i-enroll-students-on-my-new-module","text":"Students and users are given access to a module using their college email address (from microsoft). Login and navigate to your Teacher dashboard Select the module on which you want to enroll students When on the module page, click the View Students button Enter the enrolment page by clicking the Entroll Students button Enroll students by supplying one or more student email addresses","title":"How can I enroll students on my new Module"},{"location":"teacher/guides/faq/#how-can-i-move-questions-between-problem-sets","text":"When creating a new question the teacher can choose to \"clone\" from an existing question. The teacher can then delete the original version. In the problem set you wish to move the question to, select the v symbol to the right of the Add Question button From the dropdown menu, select Clone From Question Select the title of the question you wish clone from the list that appear If you wish, go back and delete the question from its original location","title":"How can I move questions between problem sets?"},{"location":"teacher/guides/faq/#how-can-i-share-a-link-to-a-problem-set","text":"To share a link with students, open the Problem Set in STUDENT mode (light blue top bar), and copy the URL from the browser. To share a link with teachers who will access the content editor and analytics, share a link from TEACHER mode (orange top bar); students won't be able to access this link.","title":"How can I share a link to a Problem Set?"},{"location":"teacher/guides/faq/#how-can-i-set-parameters-for-evaluation-functions","text":"The most common parameters will be visible uder the EVALUATE tab in the configure panel. If there is a parameter that is not already visible it can be set using the Advanced - raw parameters (also under the EVALUATE tab) by doing the following: Hover over the list of parameters in the Advanced - raw parameters area. Click the green plus-symbol that appears. Type the name of the parameter (without quotation marks). Hover over the box that says NULL next to the newly added parameter. Click the green pen symbol that appears to the right of it. Type in the desired value in box that appears. By default it will be assumed that the parameter value is a string. The webclient will infer other possible types based on the written input. If the setting should be a string, click the green checkmark to the top right, and if you want the inferred type click the green checkmark at the bottom right. The parameter is now set.","title":"How can I set parameters for evaluation functions?"},{"location":"teacher/guides/faq/#how-do-i-reorder-questions","text":"It is only possible to reorder published questions in a set. This prevents inadvertently inserting new questions between two published ones. This ensures consistency to the student when viewing a published set as existing questions will remain in an unchanged order, with new questions being added to the bottom (unless manually changed by the teacher). You can tell a question is unpublished as it will take the ' 1.X ' numbering format To reorder questions: Publish the questions you wish to reorder using FILE > SAVE AND PUBLISH (alternatively click on the PUBLISH WHOLE SET button) Refresh the page Drag and drop the questions into the new order Ensure the green box pops up saying: 'questions reordered successfully' - there is no need to republish the set","title":"How do I reorder questions?"},{"location":"teacher/guides/faq/#what-to-do-when-space-is-not-showing-in-the-pdf-generated-by-lambda-feedback","text":"The Pandoc library that lambdafeedback use to generate a pdf does not support \\space . Alternatives that could be use to generate a space in math block is to use the tilde symbol ~ or \\, for thinner spacing.","title":"What to do when \\space is not showing in the pdf generated by lambda feedback?"},{"location":"teacher/guides/faq/#what-to-do-if-the-pdf-is-not-compiling-my-inline-math-equation","text":"Please check if there is an additional space at the start or a the end of the equation. This is usually the cause for inline math blocks not compiling.","title":"What to do if the pdf is not compiling my inline math equation?"},{"location":"teacher/guides/faq/#how-can-i-have-the-same-font-for-the-unit-and-for-the-number-in-the-math-block","text":"You can use the code \\mathrm{} or {\\rm} . Both code will give you your units in serifed Times New Roman, which is the same font as the number in the math block when compiled.","title":"How can I have the same font for the unit and for the number in the math block?"},{"location":"teacher/guides/gettingstarted/","text":"Get started as a teacher using Lambda Feedback \u00b6 Access a module \u00b6 Use your Imperial Microsoft account to sign in and access your modules. By default you are logged in as a student and the interface will be blue. If you have teacher priviliges then you will see a teacher button at the top. To enter teacher mode, click on the Teacher button , and the colour of the interface will change to orange. This is where you are able to access all your modules, as well as upload and edit problem sets. As of 07/2023, new modules can only be added to Lambda Feedback by administrators. Please speak to an admin if you wish for your module to be added to the website. To find the module you want, you can sort ASCENDING as per the image below: Image: quick sort (left) or filtering (right) As of 31/8/22 the filtering/sorting only works on the content visible on the current page (other pages are ignored). We aim to fix this by sorting at the backend. Select the module you wish to edit. Create a new problem set \u00b6 Click on your module and then click on \"content\" (upper left-hand corner). Create a new set by pressing the button seen below and this will automatically appear with a default name which you can edit by clicking 'edit set metadata': To edit the content, click on the set name . This will open the Set in a 'WYSIWYG' editor. The first question is automatically created with a default name. The question structure is described here . Below the line \u00b6 Below the main question content you can provide high quality support material for students. A student guide is here and teachers use the content as follows: Structured tutorial is a canvas to provide scaffolding to students struggling with the question. Final answer is self explanatory. Worked solutions provides detailed, step-by-step solutions. All content below the line uses milkdown functionality. Worked solutions can be branched. Future developments will add branching and response areas to structured tutorials. It is not necessary to include all three methods of help, if only one of the tabs is filled then only that one button will be included in the published student version. For general terminology, see here. To see further details on how to edit your questions, see here. Enrolling students \u00b6 In TEACHER mode, open your module home page and click 'VIEW STUDENTS' then 'ENROL STUDENTS' Enter a comma-separated list of email addresses. Press 'Enter' to confirm the addresses, and then 'SUBMIT' to enrol the students Imperial College London email addresses \u00b6 You must use the long form email address: Valid: \u00b6 first.nameYY@imperial.ac.uk (student) j.doe@imperial.ac.uk (staff) first.name@imperial.ac.uk (staff) Invalid \u00b6 abc123@ic.ac.uk abc123@imperial.ac.uk user@ic.ac.uk user@imperial.ac.uk first.nameYY@ic.ac.uk The reason for the above is because we use Azure Active Directory - i.e. Microsoft - to authorise users.","title":"Getting Started"},{"location":"teacher/guides/gettingstarted/#get-started-as-a-teacher-using-lambda-feedback","text":"","title":"Get started as a teacher using Lambda Feedback"},{"location":"teacher/guides/gettingstarted/#access-a-module","text":"Use your Imperial Microsoft account to sign in and access your modules. By default you are logged in as a student and the interface will be blue. If you have teacher priviliges then you will see a teacher button at the top. To enter teacher mode, click on the Teacher button , and the colour of the interface will change to orange. This is where you are able to access all your modules, as well as upload and edit problem sets. As of 07/2023, new modules can only be added to Lambda Feedback by administrators. Please speak to an admin if you wish for your module to be added to the website. To find the module you want, you can sort ASCENDING as per the image below: Image: quick sort (left) or filtering (right) As of 31/8/22 the filtering/sorting only works on the content visible on the current page (other pages are ignored). We aim to fix this by sorting at the backend. Select the module you wish to edit.","title":"Access a module"},{"location":"teacher/guides/gettingstarted/#create-a-new-problem-set","text":"Click on your module and then click on \"content\" (upper left-hand corner). Create a new set by pressing the button seen below and this will automatically appear with a default name which you can edit by clicking 'edit set metadata': To edit the content, click on the set name . This will open the Set in a 'WYSIWYG' editor. The first question is automatically created with a default name. The question structure is described here .","title":"Create a new problem set"},{"location":"teacher/guides/gettingstarted/#below-the-line","text":"Below the main question content you can provide high quality support material for students. A student guide is here and teachers use the content as follows: Structured tutorial is a canvas to provide scaffolding to students struggling with the question. Final answer is self explanatory. Worked solutions provides detailed, step-by-step solutions. All content below the line uses milkdown functionality. Worked solutions can be branched. Future developments will add branching and response areas to structured tutorials. It is not necessary to include all three methods of help, if only one of the tabs is filled then only that one button will be included in the published student version. For general terminology, see here. To see further details on how to edit your questions, see here.","title":"Below the line"},{"location":"teacher/guides/gettingstarted/#enrolling-students","text":"In TEACHER mode, open your module home page and click 'VIEW STUDENTS' then 'ENROL STUDENTS' Enter a comma-separated list of email addresses. Press 'Enter' to confirm the addresses, and then 'SUBMIT' to enrol the students","title":"Enrolling students"},{"location":"teacher/guides/gettingstarted/#imperial-college-london-email-addresses","text":"You must use the long form email address:","title":"Imperial College London email addresses"},{"location":"teacher/guides/gettingstarted/#valid","text":"first.nameYY@imperial.ac.uk (student) j.doe@imperial.ac.uk (staff) first.name@imperial.ac.uk (staff)","title":"Valid:"},{"location":"teacher/guides/gettingstarted/#invalid","text":"abc123@ic.ac.uk abc123@imperial.ac.uk user@ic.ac.uk user@imperial.ac.uk first.nameYY@ic.ac.uk The reason for the above is because we use Azure Active Directory - i.e. Microsoft - to authorise users.","title":"Invalid"},{"location":"teacher/guides/good-practice/","text":"Good practice \u00b6 Romanised operators \u00b6 Use romanised operators such as \\(\\sin\\) , \\(\\frac{\\mathrm{d}}{\\mathrm{d}x}\\) instead of \\(sin\\) , \\(\\frac{d}{dx}\\) , etc. Use empty lines \u00b6 Using empty lines can improve the readability and neatness of your content. Empty lines are often useful before and after an equation, and between paragraphs of text. An empty line in markdown requires two spaces on the line, otherwise the line is ignored. Space between numbers and units \u00b6 Put appropriate space between a number and its unit, such as 5 m or 3 kg , according to the SI conventions. Romanise units and check their case \u00b6 Use romanised units such as \\(\\text{m}\\) , \\(\\text{kN}\\) instead of \\(m\\) , \\(kN\\) . Ensure that the case of the unit is correct. Add tests to response areas \u00b6 In a response area, press configure then tests . Tests allow you to enter potential student responses, define whether they are correct or not, then run the evaluation function on those student responses. This allows you to quickly test whether or not the evaluation function works as expected. Save and publish as you go \u00b6 Saving and publishing work regularly is recommended to prevent accidental data loss. Use branching when relevant \u00b6 Branching is a feature for worked solutions . It allows you to have different solution pathways Usage examples: When a question can be solved via multiple different methods, branching can be used for each method. When a question has multiple parts, where each part involves substitution of different values, branching can be used for each part. Use pre-response area text to be clear what should be entered \u00b6 Pre-response area text is found under configure - INPUT in the evaluation function. You can use LaTeX in the pre-response area text. Use \\dfrac for bigger fractions when needed. \u00b6 Use $\\dfrac{numerator}{denominator}$ for bigger fractions when you need to display them more clearly or emphasize them. For example, $\\dfrac{3}{4}$ will produce a bigger fraction than $\\frac{3}{4}$ . Use \\small when smaller fonts or fractions are needed \u00b6 Use $\\small{text}$ when you need to display smaller fonts or fractions in your LaTeX expressions. For example, $\\small{\\frac{1}{2}}$ will produce a smaller fraction than $\\frac{1}{2}$ . Use audio clips \u00b6 Just drag + drop an audio file into the milkdown editor. Use live preview and permit all types of input \u00b6 Live preview and input types are found in an evaluation function under configure - INPUT . Live preview instantly renders a student's input. This is very useful for long/complicated equations, as it allows students to ensure their input is correct. Latex help \u00b6 Use \\begin{array} to generate compact table i.e \\begin{array}{|c|c|} \\hline \\theta_{2,0} & \\theta_{1,L}\\\\ \\hline -6700 & 130.5641\\\\ \\hline -6600 & 161.6086\\\\ \\hline \\end{array} Use \\begin{aligned} to keep your working formatted nicely \\begin{array}{ll} M_{d e f} &=\\dfrac{1}{2}(M+M^T)\\\\ & =\\dfrac{1}{2} \\begin{pmatrix} 4 & 14\\\\ -6 & -11 \\end{pmatrix}+\\begin{pmatrix} 4 & -6\\\\ 14 & -11 \\end{pmatrix}\\\\ & =\\begin{pmatrix} 4 & 4\\\\ 4 & -11 \\end{pmatrix} \\end{array} Use \\left and \\right for equations with multiple brackets f(x)=\\left (\\frac{(\\cos (x) -x) + i(\\sin (x) - x)}{wi} \\right) This also works for [ ] and \\{ \\} Use \\sin , \\cos etc... if you are too lazy to write out \\text{sin } everytime in equation mode.","title":"Good practice"},{"location":"teacher/guides/good-practice/#good-practice","text":"","title":"Good practice"},{"location":"teacher/guides/good-practice/#romanised-operators","text":"Use romanised operators such as \\(\\sin\\) , \\(\\frac{\\mathrm{d}}{\\mathrm{d}x}\\) instead of \\(sin\\) , \\(\\frac{d}{dx}\\) , etc.","title":"Romanised operators"},{"location":"teacher/guides/good-practice/#use-empty-lines","text":"Using empty lines can improve the readability and neatness of your content. Empty lines are often useful before and after an equation, and between paragraphs of text. An empty line in markdown requires two spaces on the line, otherwise the line is ignored.","title":"Use empty lines"},{"location":"teacher/guides/good-practice/#space-between-numbers-and-units","text":"Put appropriate space between a number and its unit, such as 5 m or 3 kg , according to the SI conventions.","title":"Space between numbers and units"},{"location":"teacher/guides/good-practice/#romanise-units-and-check-their-case","text":"Use romanised units such as \\(\\text{m}\\) , \\(\\text{kN}\\) instead of \\(m\\) , \\(kN\\) . Ensure that the case of the unit is correct.","title":"Romanise units and check their case"},{"location":"teacher/guides/good-practice/#add-tests-to-response-areas","text":"In a response area, press configure then tests . Tests allow you to enter potential student responses, define whether they are correct or not, then run the evaluation function on those student responses. This allows you to quickly test whether or not the evaluation function works as expected.","title":"Add tests to response areas"},{"location":"teacher/guides/good-practice/#save-and-publish-as-you-go","text":"Saving and publishing work regularly is recommended to prevent accidental data loss.","title":"Save and publish as you go"},{"location":"teacher/guides/good-practice/#use-branching-when-relevant","text":"Branching is a feature for worked solutions . It allows you to have different solution pathways Usage examples: When a question can be solved via multiple different methods, branching can be used for each method. When a question has multiple parts, where each part involves substitution of different values, branching can be used for each part.","title":"Use branching when relevant"},{"location":"teacher/guides/good-practice/#use-pre-response-area-text-to-be-clear-what-should-be-entered","text":"Pre-response area text is found under configure - INPUT in the evaluation function. You can use LaTeX in the pre-response area text.","title":"Use pre-response area text to be clear what should be entered"},{"location":"teacher/guides/good-practice/#use-dfrac-for-bigger-fractions-when-needed","text":"Use $\\dfrac{numerator}{denominator}$ for bigger fractions when you need to display them more clearly or emphasize them. For example, $\\dfrac{3}{4}$ will produce a bigger fraction than $\\frac{3}{4}$ .","title":"Use \\dfrac for bigger fractions when needed."},{"location":"teacher/guides/good-practice/#use-small-when-smaller-fonts-or-fractions-are-needed","text":"Use $\\small{text}$ when you need to display smaller fonts or fractions in your LaTeX expressions. For example, $\\small{\\frac{1}{2}}$ will produce a smaller fraction than $\\frac{1}{2}$ .","title":"Use \\small when smaller fonts or fractions are needed"},{"location":"teacher/guides/good-practice/#use-audio-clips","text":"Just drag + drop an audio file into the milkdown editor.","title":"Use audio clips"},{"location":"teacher/guides/good-practice/#use-live-preview-and-permit-all-types-of-input","text":"Live preview and input types are found in an evaluation function under configure - INPUT . Live preview instantly renders a student's input. This is very useful for long/complicated equations, as it allows students to ensure their input is correct.","title":"Use live preview and permit all types of input"},{"location":"teacher/guides/good-practice/#latex-help","text":"Use \\begin{array} to generate compact table i.e \\begin{array}{|c|c|} \\hline \\theta_{2,0} & \\theta_{1,L}\\\\ \\hline -6700 & 130.5641\\\\ \\hline -6600 & 161.6086\\\\ \\hline \\end{array} Use \\begin{aligned} to keep your working formatted nicely \\begin{array}{ll} M_{d e f} &=\\dfrac{1}{2}(M+M^T)\\\\ & =\\dfrac{1}{2} \\begin{pmatrix} 4 & 14\\\\ -6 & -11 \\end{pmatrix}+\\begin{pmatrix} 4 & -6\\\\ 14 & -11 \\end{pmatrix}\\\\ & =\\begin{pmatrix} 4 & 4\\\\ 4 & -11 \\end{pmatrix} \\end{array} Use \\left and \\right for equations with multiple brackets f(x)=\\left (\\frac{(\\cos (x) -x) + i(\\sin (x) - x)}{wi} \\right) This also works for [ ] and \\{ \\} Use \\sin , \\cos etc... if you are too lazy to write out \\text{sin } everytime in equation mode.","title":"Latex help"},{"location":"teacher/guides/milkdown/","text":"The milkdown editor \u00b6 The milkdown editor is widely used in Lambda Feedback. It accepts: standard markdown \\(\\LaTeX\\) (delimited by $ and limited to KaTeX functionality) images (paste or drag and drop) videos (paste a URL) Common needs in milkdown \u00b6 Here's a walkthrough to create some basic content: Empty lines \u00b6 Two blank spaces in a line will ensure it persists (as in standard markdown). Inline maths \u00b6 Use the $ sign to delimited inline maths. For example type the following: This is inline maths, $\\alpha<0$, and it is useful Equation mode \u00b6 Start a blank line with $$ then press the space bar. This will introduce an equation editor. Type raw \\(\\LaTeX\\) into the shaded part and see the live preview in the lower part. *Press ctrl+enter (Mac: cmd+enter ) to exit the equation editing box. For example, type the following after typing $$ [space] into a fresh line: f(x) = \\int_{-\\infty}^\\infty \\hat{f}(\\xi)\\,e^{2 \\pi i \\xi x} \\,\\mathrm{d}\\xi Steps in worked solutions \u00b6 If you begin a fresh line with --- (three dashes) then a horizontal rule appears. Alternatively click the button on the toolbar to insert a horizontal rule. If you are editing a worked solution, then Lambda Feedback will split the worked solution into steps according to the location of horizontal rules. You can delete and add the rules and the solution steps will update. For example: This is the first step of the solution - which is a good hint towards solving --- This is a second step, which makes it more obvious --- Finally we reach the solution When viewing the worked solutions, this is how it looks: This is the process to create the solution steps:","title":"Milkdown"},{"location":"teacher/guides/milkdown/#the-milkdown-editor","text":"The milkdown editor is widely used in Lambda Feedback. It accepts: standard markdown \\(\\LaTeX\\) (delimited by $ and limited to KaTeX functionality) images (paste or drag and drop) videos (paste a URL)","title":"The milkdown editor"},{"location":"teacher/guides/milkdown/#common-needs-in-milkdown","text":"Here's a walkthrough to create some basic content:","title":"Common needs in milkdown"},{"location":"teacher/guides/milkdown/#empty-lines","text":"Two blank spaces in a line will ensure it persists (as in standard markdown).","title":"Empty lines"},{"location":"teacher/guides/milkdown/#inline-maths","text":"Use the $ sign to delimited inline maths. For example type the following: This is inline maths, $\\alpha<0$, and it is useful","title":"Inline maths"},{"location":"teacher/guides/milkdown/#equation-mode","text":"Start a blank line with $$ then press the space bar. This will introduce an equation editor. Type raw \\(\\LaTeX\\) into the shaded part and see the live preview in the lower part. *Press ctrl+enter (Mac: cmd+enter ) to exit the equation editing box. For example, type the following after typing $$ [space] into a fresh line: f(x) = \\int_{-\\infty}^\\infty \\hat{f}(\\xi)\\,e^{2 \\pi i \\xi x} \\,\\mathrm{d}\\xi","title":"Equation mode"},{"location":"teacher/guides/milkdown/#steps-in-worked-solutions","text":"If you begin a fresh line with --- (three dashes) then a horizontal rule appears. Alternatively click the button on the toolbar to insert a horizontal rule. If you are editing a worked solution, then Lambda Feedback will split the worked solution into steps according to the location of horizontal rules. You can delete and add the rules and the solution steps will update. For example: This is the first step of the solution - which is a good hint towards solving --- This is a second step, which makes it more obvious --- Finally we reach the solution When viewing the worked solutions, this is how it looks: This is the process to create the solution steps:","title":"Steps in worked solutions"},{"location":"teacher/guides/question-export-import/","text":"Question export and import \u00b6 Export a question \u00b6 Under the File menu, select the Export as JSON option: The question and images (if any) will be downloaded into your download folder. Import a question \u00b6 Click Add question menu and select Import questions from file option: The file explorer opens. Select the zip file containing the question and click open. The question will be added as the last question. The zip file must contain question data in a valid JSON format. The best way to obtain a valid JSON format is to export a question, unzip the download file and open the JSON file. If the question contains media, they must be in the media folder inside of the zip file. Import more than 1 question \u00b6 The zip file can contain more than one question. Each of the questions must be in the JSON file and in the correct format. All media must be in the media folder. It is possible to e.g. export 2 questions, then unzip the exported zip files and then zip both questions and their medias into one zip file and then import the one zip file. Here is an example of a folder containing 2 questions and their medias: The name of the folder and names of json files are not important. However, the name of media files must correspond with the names used in the json files when referring the media.","title":"Exporting and importing questions"},{"location":"teacher/guides/question-export-import/#question-export-and-import","text":"","title":"Question export and import"},{"location":"teacher/guides/question-export-import/#export-a-question","text":"Under the File menu, select the Export as JSON option: The question and images (if any) will be downloaded into your download folder.","title":"Export a question"},{"location":"teacher/guides/question-export-import/#import-a-question","text":"Click Add question menu and select Import questions from file option: The file explorer opens. Select the zip file containing the question and click open. The question will be added as the last question. The zip file must contain question data in a valid JSON format. The best way to obtain a valid JSON format is to export a question, unzip the download file and open the JSON file. If the question contains media, they must be in the media folder inside of the zip file.","title":"Import a question"},{"location":"teacher/guides/question-export-import/#import-more-than-1-question","text":"The zip file can contain more than one question. Each of the questions must be in the JSON file and in the correct format. All media must be in the media folder. It is possible to e.g. export 2 questions, then unzip the exported zip files and then zip both questions and their medias into one zip file and then import the one zip file. Here is an example of a folder containing 2 questions and their medias: The name of the folder and names of json files are not important. However, the name of media files must correspond with the names used in the json files when referring the media.","title":"Import more than 1 question"},{"location":"teacher/reference/access_control/","text":"Access control \u00b6 Modules \u00b6 Module access for students is controlled by enrolling student users. More details to be added here. Sets \u00b6 Set access is granted to all users enrolled on a module, but the Set can be hidden by the teacher. Two methods can be used to hide a Set: Start and end dates (both optional) can be created in the Set Metadata The Set can me manually hidden , which overrides the above settings. Support material within questions \u00b6 The following types of support materials are available to students in the help section: Sructured tutorial Final answer Worked solutions Two methods can be used to hide support material: Configuring student access at the set level \u00b6 Open the Edit Set Metadata page by clicking on the Edit Set Metadata button in the list of sets: The page contains the Student access to support material section: Access to each support material type can be set to one of the following options: Available \u00b6 Students can open this support material type without any restrictions. This is valid for all questions in the set except those for which the support material access is set to be unavailable at the question level (see below). Available with warnings \u00b6 A warning window appears if the studen opens the content before the recommended time. The recommended time is the Minimum time estimate (mins) which can be set on the question Guidance page: However, the option will be changed to Available , if any of the following is true: The student has downloaded the PDF The part is marked as done There is no minimum time estimate set for the question The time now minus the time the student first accessed the question is more than the minimum time estimate This is valid for all questions in the set except those for which the support material access is set to be unavailable at the question level (see below). Unavailable \u00b6 Students cannot open any support material for any question in the set. This is valid for all questions in the set, even those for which the support material access is set to Available at the question level (see below). Configuring student access at the question level \u00b6 The support material access configuration at the question level is located on the File tab: All support material is available by default, it can be changed: If the switch is off , then the support material is available If the switch is on , then the support material is unavailable Summary overview \u00b6 Set level setting Question level setting Result (using Final answer as an example) Description Comment Unavailable N/A The Final answer is disabled The setting at the question level is ignored Available Unavailable The Final answer is disabled Available with warnings Unavailable The Final answer is disabled The same result as above Available with warnings Available When the Final answer is clicked, a warning message appears Additional conditions must be met: PDF not downloaded Part not marked as done The minimum time estimate is set for the question The time now minus the time the student first accessed the question is more than the minimum time estimate If any of them is not met, then the support material will be available with no warnings.","title":"Access control"},{"location":"teacher/reference/access_control/#access-control","text":"","title":"Access control"},{"location":"teacher/reference/access_control/#modules","text":"Module access for students is controlled by enrolling student users. More details to be added here.","title":"Modules"},{"location":"teacher/reference/access_control/#sets","text":"Set access is granted to all users enrolled on a module, but the Set can be hidden by the teacher. Two methods can be used to hide a Set: Start and end dates (both optional) can be created in the Set Metadata The Set can me manually hidden , which overrides the above settings.","title":"Sets"},{"location":"teacher/reference/access_control/#support-material-within-questions","text":"The following types of support materials are available to students in the help section: Sructured tutorial Final answer Worked solutions Two methods can be used to hide support material:","title":"Support material within questions"},{"location":"teacher/reference/access_control/#configuring-student-access-at-the-set-level","text":"Open the Edit Set Metadata page by clicking on the Edit Set Metadata button in the list of sets: The page contains the Student access to support material section: Access to each support material type can be set to one of the following options:","title":"Configuring student access at the set level"},{"location":"teacher/reference/access_control/#available","text":"Students can open this support material type without any restrictions. This is valid for all questions in the set except those for which the support material access is set to be unavailable at the question level (see below).","title":"Available"},{"location":"teacher/reference/access_control/#available-with-warnings","text":"A warning window appears if the studen opens the content before the recommended time. The recommended time is the Minimum time estimate (mins) which can be set on the question Guidance page: However, the option will be changed to Available , if any of the following is true: The student has downloaded the PDF The part is marked as done There is no minimum time estimate set for the question The time now minus the time the student first accessed the question is more than the minimum time estimate This is valid for all questions in the set except those for which the support material access is set to be unavailable at the question level (see below).","title":"Available with warnings"},{"location":"teacher/reference/access_control/#unavailable","text":"Students cannot open any support material for any question in the set. This is valid for all questions in the set, even those for which the support material access is set to Available at the question level (see below).","title":"Unavailable"},{"location":"teacher/reference/access_control/#configuring-student-access-at-the-question-level","text":"The support material access configuration at the question level is located on the File tab: All support material is available by default, it can be changed: If the switch is off , then the support material is available If the switch is on , then the support material is unavailable","title":"Configuring student access at the question level"},{"location":"teacher/reference/access_control/#summary-overview","text":"Set level setting Question level setting Result (using Final answer as an example) Description Comment Unavailable N/A The Final answer is disabled The setting at the question level is ignored Available Unavailable The Final answer is disabled Available with warnings Unavailable The Final answer is disabled The same result as above Available with warnings Available When the Final answer is clicked, a warning message appears Additional conditions must be met: PDF not downloaded Part not marked as done The minimum time estimate is set for the question The time now minus the time the student first accessed the question is more than the minimum time estimate If any of them is not met, then the support material will be available with no warnings.","title":"Summary overview"},{"location":"teacher/reference/latex_functionality/","text":"Latex functionality \u00b6 Lambda feedback uses one content source to serve two outputs: web and PDF . Each output has different requirements, and content must meet both requirements in order to serve both outputs. Content formatting \u00b6 All content is formatted in markdown . Headings, font style, lists, tables, images, \\(\\LaTeX\\) , can all be created using standard markdown. Special attention is required when formatting \\(\\LaTeX\\) which, although it is formatted using standard markdown (i.e. delimited by the $ for 'inline formulas', and $$ for an equation environment), must use a subset of \\(\\LaTeX\\) in order to compile for both outputs. This sometimes requires a compromise by the author. The milkdown editor \u00b6 All content in Lambda Feedback is stored as markdown (ASCII content), however it is always input/edited using the milkdown editor. This editor has the advantage of providing live interactive previews of content, including \\(\\LaTeX\\) via katex. Web requirements: katex \u00b6 \\(\\LaTeX\\) content is rendered in the web browser using the katex Javascript library. The katex home page has an intereactive input where content is rendered and can be checked for validity. The documentation lists which functions are supported. Katex is a subset of \\(\\LaTeX\\) . Therefore some functions that work in \\(\\LaTeX\\) do not work in katex and won't render on the web. For example, the tikz package - which is a complex graphics package - is not supported by katex. Unsupported packages have knock-on effects, for example while the \\cancel{} function is supported, \\cancelto{}{} is not because it relies on tikz . Ensuring that content renders correctly on the web is straightforward because the editor gives a live preview - and will indicate when errors occur. PDF requirements: pandoc and PDFlatex/XeTeX \u00b6 When a Problem Set is saved in the editor, the PDF is automatically compiled by sending the markdown content to Pandoc , which internally uses \\(\\LaTeX\\) (either PDFlatex of xelatex - depending on the settings within the Lambda Feedback stack) to render a PDF. Problems can occur with PDF compilation even if the web rendering is successful, because it uses different libraries to the web browser. If the PDF fails to compile, a red error bar will appear and provide the location of the error within the Problem Set (identifying which question), and the error that Pandoc returned. One key limitation of the Lambda Feedback system is that it uses markdown content, so cannot produce all the richness of a full \\(\\LaTeX\\) document (and the AMS math library). All equation environments in markdown are signalled by the $$ delimiter which is equivalent to \\begin{equation*} . This rules out using alternative primary environments , such as align , gather , multiline , alignat , falign . You can use subordinate environments within an equation environment, for example the following is valid: $$ \\begin{aligned} a & b\\\\ c & d \\end{aligned} $$ Here the suffix -ed in aligned implies a subordinate environment; likewise gathered , alignedat etc. are all valid within an equation environment. Warning: No blank lines allowed in aligned subordinate environment \u00b6 If a blank line is present within a subordinate environment \\begin{aligned} then Pandoc will fail to compile the PDF. For example: ```Faulty code example: $$ \\begin{aligned} a & b\\ c & d \\end{aligned} $$ ``` The above will fail to compile the PDF. But removing the blank line will solve the problem. For further reading search for the AMS math package and related literature. Numbering equations \u00b6 Equation numbering is problematic and our advice is to use manual numbering. Automatic numbering is possible, for example using \\begin{equation} within a $$ environment. Note that sometimes the equation numbers continue counting from one environment to the next, while at other times they don't. You cannot use \\ref to refer to automatic equation numbers. An alternative approach is to use an unnumbered aligned environment, and to add an extra column with the equation number in (e.g. &(2) ). For some ad hoc good practice tips, see good practice .","title":"Latex functionality"},{"location":"teacher/reference/latex_functionality/#latex-functionality","text":"Lambda feedback uses one content source to serve two outputs: web and PDF . Each output has different requirements, and content must meet both requirements in order to serve both outputs.","title":"Latex functionality"},{"location":"teacher/reference/latex_functionality/#content-formatting","text":"All content is formatted in markdown . Headings, font style, lists, tables, images, \\(\\LaTeX\\) , can all be created using standard markdown. Special attention is required when formatting \\(\\LaTeX\\) which, although it is formatted using standard markdown (i.e. delimited by the $ for 'inline formulas', and $$ for an equation environment), must use a subset of \\(\\LaTeX\\) in order to compile for both outputs. This sometimes requires a compromise by the author.","title":"Content formatting"},{"location":"teacher/reference/latex_functionality/#the-milkdown-editor","text":"All content in Lambda Feedback is stored as markdown (ASCII content), however it is always input/edited using the milkdown editor. This editor has the advantage of providing live interactive previews of content, including \\(\\LaTeX\\) via katex.","title":"The milkdown editor"},{"location":"teacher/reference/latex_functionality/#web-requirements-katex","text":"\\(\\LaTeX\\) content is rendered in the web browser using the katex Javascript library. The katex home page has an intereactive input where content is rendered and can be checked for validity. The documentation lists which functions are supported. Katex is a subset of \\(\\LaTeX\\) . Therefore some functions that work in \\(\\LaTeX\\) do not work in katex and won't render on the web. For example, the tikz package - which is a complex graphics package - is not supported by katex. Unsupported packages have knock-on effects, for example while the \\cancel{} function is supported, \\cancelto{}{} is not because it relies on tikz . Ensuring that content renders correctly on the web is straightforward because the editor gives a live preview - and will indicate when errors occur.","title":"Web requirements: katex"},{"location":"teacher/reference/latex_functionality/#pdf-requirements-pandoc-and-pdflatexxetex","text":"When a Problem Set is saved in the editor, the PDF is automatically compiled by sending the markdown content to Pandoc , which internally uses \\(\\LaTeX\\) (either PDFlatex of xelatex - depending on the settings within the Lambda Feedback stack) to render a PDF. Problems can occur with PDF compilation even if the web rendering is successful, because it uses different libraries to the web browser. If the PDF fails to compile, a red error bar will appear and provide the location of the error within the Problem Set (identifying which question), and the error that Pandoc returned. One key limitation of the Lambda Feedback system is that it uses markdown content, so cannot produce all the richness of a full \\(\\LaTeX\\) document (and the AMS math library). All equation environments in markdown are signalled by the $$ delimiter which is equivalent to \\begin{equation*} . This rules out using alternative primary environments , such as align , gather , multiline , alignat , falign . You can use subordinate environments within an equation environment, for example the following is valid: $$ \\begin{aligned} a & b\\\\ c & d \\end{aligned} $$ Here the suffix -ed in aligned implies a subordinate environment; likewise gathered , alignedat etc. are all valid within an equation environment.","title":"PDF requirements: pandoc and PDFlatex/XeTeX"},{"location":"teacher/reference/latex_functionality/#warning-no-blank-lines-allowed-in-aligned-subordinate-environment","text":"If a blank line is present within a subordinate environment \\begin{aligned} then Pandoc will fail to compile the PDF. For example: ```Faulty code example: $$ \\begin{aligned} a & b\\ c & d \\end{aligned} $$ ``` The above will fail to compile the PDF. But removing the blank line will solve the problem. For further reading search for the AMS math package and related literature.","title":"Warning: No blank lines allowed in aligned subordinate environment"},{"location":"teacher/reference/latex_functionality/#numbering-equations","text":"Equation numbering is problematic and our advice is to use manual numbering. Automatic numbering is possible, for example using \\begin{equation} within a $$ environment. Note that sometimes the equation numbers continue counting from one environment to the next, while at other times they don't. You cannot use \\ref to refer to automatic equation numbers. An alternative approach is to use an unnumbered aligned environment, and to add an extra column with the equation number in (e.g. &(2) ). For some ad hoc good practice tips, see good practice .","title":"Numbering equations"},{"location":"teacher/reference/evaluation_functions/","text":"Evaluation Functions \u00b6 Evaluation functions are responsible for taking in a user's response, comparing it with a correct answer, and providing feedback to the frontend application. Living as containserized Lambda functions on the cloud, they are infinitely customisable and language-agnostic. Content authors should be able to create their own at will. However, we are aware that in a lot of cases, this grading logic will be similar, which is why a few functions have already been created.","title":"Evaluation Functions"},{"location":"teacher/reference/evaluation_functions/#evaluation-functions","text":"Evaluation functions are responsible for taking in a user's response, comparing it with a correct answer, and providing feedback to the frontend application. Living as containserized Lambda functions on the cloud, they are infinitely customisable and language-agnostic. Content authors should be able to create their own at will. However, we are aware that in a lot of cases, this grading logic will be similar, which is why a few functions have already been created.","title":"Evaluation Functions"},{"location":"teacher/reference/response_area_components/","text":"Response Area Components \u00b6 These are what the user interacts with on the front-end. As React components, they admit a certain number of parameters which are described in this section. Response Area creation \u00b6 The user can add, delete, edit or switch response area. Add Response Area \u00b6 The user can add a response area without any restrictions. Duplicate \u00b6 The user can duplicate the response area within the same part using the duplicate icon. When a response area is duplicated, then only data entered in the teacher edit mode are copied (such as input type, evaluation function, input symbols, pre and post text, answer, tests, cases, ...). Data entered in the teacher preview mode (such as comments) and student mode (comments, flags, likes, student solutions, ...) are not copied. Reorder \u00b6 It is possible to reorder response areas within the part using the \"drag and drop\" feature. It works in the same way a reordering of parts: Move your mouse cursor over the response are you want to move. Then, press and hold down the left mouse button, move the object to the location you desire, and release the mouse button to set it down. Delete Response Area \u00b6 The user can delete a response area without any restrictions. A popup message appears to confirm the deletion. However, it is important to remember that there might be submissions against the response area you are trying to delete. If this is the case, the popup message will contain the relevant information. See more information about analytics against deleted response areas in Analytics Response Area configuration \u00b6 The user can configure a response area using the configure button: which opens the 'Response Area Panel'. The panel follows a workflow designed around the way a teacher thinks: Input Evaluation Feedback Tests Each stage is in a separate tab. Teachers are recommended to be mindful of this process when creating a response area. Input \u00b6 Select an input style for the student by scrolling or filtering. Most needs are met by 'MCQ', 'NUMERIC' and 'EXPRESSION'. INPUT: configure the Student facing options for the input: Enable live preview. Default TRUE for the EXPRESSION input type, to validate student input before submitting for feedback. Display input symbols. Default FALSE. Enable it to give students guidance on how to express themselves. Teachers configure the input symbols in the 'Evaluate' tab Include in PDF. Default FALSE except for MCQ. Only affects the PDF version. Includes pre/post response text in the PDF, with a blank space between. Pre/post response text (optional). To clarify to students what to input in the response area. Accepts plain text, including single-dollar-delimited latex. E.g. Estimate $f(x)=$ is acceptable. Answer. Enter a reference answer. Configure the answer where relevant (e.g. number of rows and columns). Response Area Preview: for teachers to verify the configuration EVALUATE: configure how student expressions are evaluated. This is a 'no code' parametric configuration. Settings will be upgraded as the system improves. Evaluation Function - select from the list. Parameters - configure as provided, and add new parameters as required. Details depend on the Evaluation Function. Input symbols - define a dictionary of symbols and their equialevnt in code form. These symbols are used even if they are not displayed to students. All inputs are plain text. For example, the symbol $f(x)$ may have code fx and alternatives f_x , f(x) , f . This dictionary will be provided to the evaluation function, even if the teacher has not displayed it to the student. Input Symbols allows teachers to pickup on multiple acceptable ways to express concepts, including less-rigorous methods that are not encouraged but will still be dealt with as well as possible. The configuration of input symbols is a very important part of providing high quality feedback. Note that the 'visibility' Boolean applies if input symbols are displayed to students, otherwise it is irrelevant. It allows Teachers to communicate some symbols to students, while keeping others hidden but still effective. FEEDBACK: add 'cases'. Each case is an alternative reference answer, with customised parameters, so that multiple cases can be dealt with independently. Cases can be used to capture multiple correct approaches that are not equivalent. Cases can also be used to identify common incorrect approaches and to provide customised feedback. The FEEDBACK tab is typically populated after students start using the system, and when data is available to point to common expressions. Configuring a case works similarly to setting up the default answer in the INPUT tab, with added options for changing the colour of the given feedback, give costum feedback and toggling whether the case answer should be considered correct or not. Adding costum feedback will overwrite the feedback produced by the evaluation function. The exceptiong When a response is submitted it is evaluated for all cases and feedback for the first case that matches will be displayed. When determining which matched case is first, the default answer described in the INPUT tab will take precedence over all other cases, otherwise the feedback for the matched case with the lowest number will be displayed (i.e. the answer given in the INPUT tab can be considered to be Case 0). TESTS: tests provide a systematic way to log what behaviour the teacher expects. It provides a useful record and an efficient way to retest the bevhaiour of a response area over time (e.g. as evaluation functions evolve, or as the subject matter itself changes). Restrictions on changes: the input type \u00b6 It is possible to change the input type (e.g. from Text to Number ) without any restrictions until the response area is saved (with or without publishing) to students. After the response area is saved, it is still possible to change the input type, but it will result into replacing the response area by a new one. The previous response area will still exist, but only on the previous version of the question. When replacing the response area, all response area content data (those entered by teachers including tutorials, final answer and worked solutions) are copied, but any existing response area event data (student answers, click events and statistics) will remain linked only to the previous response area. Student answers, click events and statisticsthose data are never lost by deleting a response area, they are always preserved. Once a question is saved (with or without publishing), then any new changes are saved into a new (draft) version of the question. So, if e.g. a response area is deleted after a question was published, then it is deleted from the draft version only. And if this draft version is later published, then the previously published version is preserved (and with it the \"deleted\" response area and linked submissions). The reason why the input type change is restricted is to preserve high quality data analytics as explained in the examples below. An Example 1 - changing input type on PUBLISHED response area \u00b6 FIRST Part \u00b6 The teacher creates a new question with Response Areas RA1 and RA2. -> The version of the question is QV1 with status DRAFT. The teacher is making changes (including the change of the input type if needed). -> The changes are being saved into QV1 with no restrictions The teacher performs PUBLISH action (let's assume RA1 and RA2 input types are both Number for this example). -> The QV1 version status is changed to PUBLISHED Students are submitting their answers -> submissions are recorder against Response Area RA1 and RA2 (in the single Number format for both response areas) The teacher clicks on a question to edit it -> with the first click a new question version QV2 is created with status DRAFT The teacher makes following changes (which are being recorded against QV2): In RA1: adds a new Input Symbol -> the change is recorded against RA1 In RA2: the teacher unlocks and changes the input type -> RA2 is deleted (from the question version QV2) and a new response area RA3 is created (let's assume input type Table for this example) The teacher performs PUBLISH action -> the QV2 is published (with response area RA1 with input type Number and response area RA3 with input type Table ) Students are submitting their answers -> submissions are recorder against Response Area RA1 (in the single Number format) and against Response Area RA3 (in the Table format) => No submissions are lost. The original submissions (in the Number format) are linked to the RA2, which is preserved on the question version QV1. New submissions (in the table format) are linked to the RA3 which is recorded on the question version QV2. Please note: All statistics and submissions are currently displayed against the published question version only. So, though the submissions against RA2 are preserved, it is not currently possible to see them. We are working on the improvement to make this possible. SECOND Part - this is an extension of the FIRST Part \u00b6 The teacher decides to REVERT (the question created in the FIRST Part) to the question version QV1 -> a new question version QV3 is created and the content of the QV1 is copied to QV3 -> QV3 is DRAFT version which contains RA1 (input type Number ) and RA2 (input type Number ) The teacher performs PUBLISH action -> the QV3 is published and the teacher can now see submissions against RA1 and RA2, but he cannot see anymore submissions against RA3 (these are preserved against the QV2 version) An Example 2 - changing input type on SAVED response area \u00b6 The teacher creates a new question with Response Areas RA1 and RA2. -> The version of the question is QV1 with status DRAFT. The teacher is making changes (including the change of the input type if needed). -> The changes are being saved into QV1 with no restrictions The teacher performs SAVE action (let's assume RA1 and RA2 input types are both Number for this example). -> The QV1 version status is changed to SAVED The teacher clicks on a question to edit it -> with the first click a new question version QV2 is created with status DRAFT The teacher makes following changes (which are being recorded against QV2): In RA1: adds a new Input Symbol -> the change is recorded against RA1 In RA2: the teacher unlocks and changes the input type -> RA2 is deleted (from the question version QV2) and a new response area RA3 is created (let's assume input type Table for this example) The teacher performs PUBLISH action -> the QV2 is published (with response area RA1 with input type Number and response area RA3 with input type Table ) Students are submitting their answers -> submissions are recorder against Response Area RA1 (in the single Number format) and against Response Area RA3 (in the Table format) The teacher decides to REVERT to the question version QV1 -> a new question version QV3 is created and the content of the QV1 is copied to QV3 -> QV3 is DRAFT version which contains RA1 (input type Number ) and RA2 (input type Number ) The teacher performs PUBLISH action -> the QV3 is published and the teacher can now see submissions against RA1. There are no submissions against RA2 as it has not been (until now) published. Submissions against RA3 are not visible, but they are preserved against the question version QV2. An Example 3 - adding new response area to a published question \u00b6 The teacher creates a new question with Response Areas RA1 and RA2. -> The version of the question is QV1 with status DRAFT. The teacher is making changes (including the change of the input type if needed). -> The changes are being saved into QV1 with no restrictions The teacher performs SAVE (or PUBLISH) action (let's assume RA1 and RA2 input types are both Number for this example). -> The QV1 version status is changed to SAVE (or PUBLISHED) The teacher clicks on a question to edit it -> with the first click a new question version QV2 is created with status DRAFT The teacher adds a new response area RA3 => At this point the teacher is making changes in the question version QV2 (DRAFT) in which: - The input types of RA1 and RA2 are locked, because they exist on a saved version QV1. It does not matter if QV1 is (only) saved or published or if there are or there are not existing submissions. The reason why it is locked is that the teacher can revert into this version later after submissions are created. By locking it we are making sure that the \"unlock\" process will be triggered which will preserve the original response area and it will create a new response area and it will make sure that the submissions are linked to response area with compatible input type. - The input type of RA3 is not locked at this point, because RA3 does not (yet) exist on any saved question version.","title":"Response Area Components"},{"location":"teacher/reference/response_area_components/#response-area-components","text":"These are what the user interacts with on the front-end. As React components, they admit a certain number of parameters which are described in this section.","title":"Response Area Components"},{"location":"teacher/reference/response_area_components/#response-area-creation","text":"The user can add, delete, edit or switch response area.","title":"Response Area creation"},{"location":"teacher/reference/response_area_components/#add-response-area","text":"The user can add a response area without any restrictions.","title":"Add Response Area"},{"location":"teacher/reference/response_area_components/#duplicate","text":"The user can duplicate the response area within the same part using the duplicate icon. When a response area is duplicated, then only data entered in the teacher edit mode are copied (such as input type, evaluation function, input symbols, pre and post text, answer, tests, cases, ...). Data entered in the teacher preview mode (such as comments) and student mode (comments, flags, likes, student solutions, ...) are not copied.","title":"Duplicate"},{"location":"teacher/reference/response_area_components/#reorder","text":"It is possible to reorder response areas within the part using the \"drag and drop\" feature. It works in the same way a reordering of parts: Move your mouse cursor over the response are you want to move. Then, press and hold down the left mouse button, move the object to the location you desire, and release the mouse button to set it down.","title":"Reorder"},{"location":"teacher/reference/response_area_components/#delete-response-area","text":"The user can delete a response area without any restrictions. A popup message appears to confirm the deletion. However, it is important to remember that there might be submissions against the response area you are trying to delete. If this is the case, the popup message will contain the relevant information. See more information about analytics against deleted response areas in Analytics","title":"Delete Response Area"},{"location":"teacher/reference/response_area_components/#response-area-configuration","text":"The user can configure a response area using the configure button: which opens the 'Response Area Panel'. The panel follows a workflow designed around the way a teacher thinks: Input Evaluation Feedback Tests Each stage is in a separate tab. Teachers are recommended to be mindful of this process when creating a response area.","title":"Response Area configuration"},{"location":"teacher/reference/response_area_components/#input","text":"Select an input style for the student by scrolling or filtering. Most needs are met by 'MCQ', 'NUMERIC' and 'EXPRESSION'. INPUT: configure the Student facing options for the input: Enable live preview. Default TRUE for the EXPRESSION input type, to validate student input before submitting for feedback. Display input symbols. Default FALSE. Enable it to give students guidance on how to express themselves. Teachers configure the input symbols in the 'Evaluate' tab Include in PDF. Default FALSE except for MCQ. Only affects the PDF version. Includes pre/post response text in the PDF, with a blank space between. Pre/post response text (optional). To clarify to students what to input in the response area. Accepts plain text, including single-dollar-delimited latex. E.g. Estimate $f(x)=$ is acceptable. Answer. Enter a reference answer. Configure the answer where relevant (e.g. number of rows and columns). Response Area Preview: for teachers to verify the configuration EVALUATE: configure how student expressions are evaluated. This is a 'no code' parametric configuration. Settings will be upgraded as the system improves. Evaluation Function - select from the list. Parameters - configure as provided, and add new parameters as required. Details depend on the Evaluation Function. Input symbols - define a dictionary of symbols and their equialevnt in code form. These symbols are used even if they are not displayed to students. All inputs are plain text. For example, the symbol $f(x)$ may have code fx and alternatives f_x , f(x) , f . This dictionary will be provided to the evaluation function, even if the teacher has not displayed it to the student. Input Symbols allows teachers to pickup on multiple acceptable ways to express concepts, including less-rigorous methods that are not encouraged but will still be dealt with as well as possible. The configuration of input symbols is a very important part of providing high quality feedback. Note that the 'visibility' Boolean applies if input symbols are displayed to students, otherwise it is irrelevant. It allows Teachers to communicate some symbols to students, while keeping others hidden but still effective. FEEDBACK: add 'cases'. Each case is an alternative reference answer, with customised parameters, so that multiple cases can be dealt with independently. Cases can be used to capture multiple correct approaches that are not equivalent. Cases can also be used to identify common incorrect approaches and to provide customised feedback. The FEEDBACK tab is typically populated after students start using the system, and when data is available to point to common expressions. Configuring a case works similarly to setting up the default answer in the INPUT tab, with added options for changing the colour of the given feedback, give costum feedback and toggling whether the case answer should be considered correct or not. Adding costum feedback will overwrite the feedback produced by the evaluation function. The exceptiong When a response is submitted it is evaluated for all cases and feedback for the first case that matches will be displayed. When determining which matched case is first, the default answer described in the INPUT tab will take precedence over all other cases, otherwise the feedback for the matched case with the lowest number will be displayed (i.e. the answer given in the INPUT tab can be considered to be Case 0). TESTS: tests provide a systematic way to log what behaviour the teacher expects. It provides a useful record and an efficient way to retest the bevhaiour of a response area over time (e.g. as evaluation functions evolve, or as the subject matter itself changes).","title":"Input"},{"location":"teacher/reference/response_area_components/#restrictions-on-changes-the-input-type","text":"It is possible to change the input type (e.g. from Text to Number ) without any restrictions until the response area is saved (with or without publishing) to students. After the response area is saved, it is still possible to change the input type, but it will result into replacing the response area by a new one. The previous response area will still exist, but only on the previous version of the question. When replacing the response area, all response area content data (those entered by teachers including tutorials, final answer and worked solutions) are copied, but any existing response area event data (student answers, click events and statistics) will remain linked only to the previous response area. Student answers, click events and statisticsthose data are never lost by deleting a response area, they are always preserved. Once a question is saved (with or without publishing), then any new changes are saved into a new (draft) version of the question. So, if e.g. a response area is deleted after a question was published, then it is deleted from the draft version only. And if this draft version is later published, then the previously published version is preserved (and with it the \"deleted\" response area and linked submissions). The reason why the input type change is restricted is to preserve high quality data analytics as explained in the examples below.","title":"Restrictions on changes: the input type"},{"location":"teacher/reference/response_area_components/#an-example-1-changing-input-type-on-published-response-area","text":"","title":"An Example 1 - changing input type on PUBLISHED response area"},{"location":"teacher/reference/response_area_components/#first-part","text":"The teacher creates a new question with Response Areas RA1 and RA2. -> The version of the question is QV1 with status DRAFT. The teacher is making changes (including the change of the input type if needed). -> The changes are being saved into QV1 with no restrictions The teacher performs PUBLISH action (let's assume RA1 and RA2 input types are both Number for this example). -> The QV1 version status is changed to PUBLISHED Students are submitting their answers -> submissions are recorder against Response Area RA1 and RA2 (in the single Number format for both response areas) The teacher clicks on a question to edit it -> with the first click a new question version QV2 is created with status DRAFT The teacher makes following changes (which are being recorded against QV2): In RA1: adds a new Input Symbol -> the change is recorded against RA1 In RA2: the teacher unlocks and changes the input type -> RA2 is deleted (from the question version QV2) and a new response area RA3 is created (let's assume input type Table for this example) The teacher performs PUBLISH action -> the QV2 is published (with response area RA1 with input type Number and response area RA3 with input type Table ) Students are submitting their answers -> submissions are recorder against Response Area RA1 (in the single Number format) and against Response Area RA3 (in the Table format) => No submissions are lost. The original submissions (in the Number format) are linked to the RA2, which is preserved on the question version QV1. New submissions (in the table format) are linked to the RA3 which is recorded on the question version QV2. Please note: All statistics and submissions are currently displayed against the published question version only. So, though the submissions against RA2 are preserved, it is not currently possible to see them. We are working on the improvement to make this possible.","title":"FIRST Part"},{"location":"teacher/reference/response_area_components/#second-part-this-is-an-extension-of-the-first-part","text":"The teacher decides to REVERT (the question created in the FIRST Part) to the question version QV1 -> a new question version QV3 is created and the content of the QV1 is copied to QV3 -> QV3 is DRAFT version which contains RA1 (input type Number ) and RA2 (input type Number ) The teacher performs PUBLISH action -> the QV3 is published and the teacher can now see submissions against RA1 and RA2, but he cannot see anymore submissions against RA3 (these are preserved against the QV2 version)","title":"SECOND Part - this is an extension of the FIRST Part"},{"location":"teacher/reference/response_area_components/#an-example-2-changing-input-type-on-saved-response-area","text":"The teacher creates a new question with Response Areas RA1 and RA2. -> The version of the question is QV1 with status DRAFT. The teacher is making changes (including the change of the input type if needed). -> The changes are being saved into QV1 with no restrictions The teacher performs SAVE action (let's assume RA1 and RA2 input types are both Number for this example). -> The QV1 version status is changed to SAVED The teacher clicks on a question to edit it -> with the first click a new question version QV2 is created with status DRAFT The teacher makes following changes (which are being recorded against QV2): In RA1: adds a new Input Symbol -> the change is recorded against RA1 In RA2: the teacher unlocks and changes the input type -> RA2 is deleted (from the question version QV2) and a new response area RA3 is created (let's assume input type Table for this example) The teacher performs PUBLISH action -> the QV2 is published (with response area RA1 with input type Number and response area RA3 with input type Table ) Students are submitting their answers -> submissions are recorder against Response Area RA1 (in the single Number format) and against Response Area RA3 (in the Table format) The teacher decides to REVERT to the question version QV1 -> a new question version QV3 is created and the content of the QV1 is copied to QV3 -> QV3 is DRAFT version which contains RA1 (input type Number ) and RA2 (input type Number ) The teacher performs PUBLISH action -> the QV3 is published and the teacher can now see submissions against RA1. There are no submissions against RA2 as it has not been (until now) published. Submissions against RA3 are not visible, but they are preserved against the question version QV2.","title":"An Example 2 - changing input type on SAVED response area"},{"location":"teacher/reference/response_area_components/#an-example-3-adding-new-response-area-to-a-published-question","text":"The teacher creates a new question with Response Areas RA1 and RA2. -> The version of the question is QV1 with status DRAFT. The teacher is making changes (including the change of the input type if needed). -> The changes are being saved into QV1 with no restrictions The teacher performs SAVE (or PUBLISH) action (let's assume RA1 and RA2 input types are both Number for this example). -> The QV1 version status is changed to SAVE (or PUBLISHED) The teacher clicks on a question to edit it -> with the first click a new question version QV2 is created with status DRAFT The teacher adds a new response area RA3 => At this point the teacher is making changes in the question version QV2 (DRAFT) in which: - The input types of RA1 and RA2 are locked, because they exist on a saved version QV1. It does not matter if QV1 is (only) saved or published or if there are or there are not existing submissions. The reason why it is locked is that the teacher can revert into this version later after submissions are created. By locking it we are making sure that the \"unlock\" process will be triggered which will preserve the original response area and it will create a new response area and it will make sure that the submissions are linked to response area with compatible input type. - The input type of RA3 is not locked at this point, because RA3 does not (yet) exist on any saved question version.","title":"An Example 3 - adding new response area to a published question"},{"location":"teacher/reference/response_area_components/Boolean/","text":"","title":"Boolean"},{"location":"teacher/reference/response_area_components/Expression/","text":"Expression \u00b6 This response area is very similar to Text , differing in that it can display how the user's response was interpreted back to them through the 'live preview' feature. This works using the grading function, providing a feedback.response_latex field, which gets rendered. Evaluation Function Options \u00b6 isSimilar \u00b6 Calculates the difference between the teacher answer (ans) and the student response (res); compares this to an allowable difference comprising an absolute tolerance (atol) and a relative tolerance (rtol). symbolicEqual \u00b6 Compares two symbolic expressions for mathematical equivalence, using SymPy. See SymPy for further information. Component Parameters \u00b6 post_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. pre_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. Allow Handwrite (Experimental) \u00b6 Enables a handwriting canvas in the browser, which allows a student can use to draw their expression, rather than type using Sympy's syntax. Photo (Experimental) \u00b6 Allows a student to upload their expression as an image, as an alternative to handwriting if the student isn't using a phone or tablet. Setting The Answer \u00b6 Type the correct answer into the 'Response Area Answer' using standard syntax. As the student enters the answer, this will be rendered using the 'live preview' feature, to ensure the correct expression has been entered. Use the 'Response Area Preview' to check the answer has been set correctly. Example Student Response area \u00b6 Correct response given Incorrect response given","title":"Expression"},{"location":"teacher/reference/response_area_components/Expression/#expression","text":"This response area is very similar to Text , differing in that it can display how the user's response was interpreted back to them through the 'live preview' feature. This works using the grading function, providing a feedback.response_latex field, which gets rendered.","title":"Expression"},{"location":"teacher/reference/response_area_components/Expression/#evaluation-function-options","text":"","title":"Evaluation Function Options"},{"location":"teacher/reference/response_area_components/Expression/#issimilar","text":"Calculates the difference between the teacher answer (ans) and the student response (res); compares this to an allowable difference comprising an absolute tolerance (atol) and a relative tolerance (rtol).","title":"isSimilar"},{"location":"teacher/reference/response_area_components/Expression/#symbolicequal","text":"Compares two symbolic expressions for mathematical equivalence, using SymPy. See SymPy for further information.","title":"symbolicEqual"},{"location":"teacher/reference/response_area_components/Expression/#component-parameters","text":"","title":"Component Parameters"},{"location":"teacher/reference/response_area_components/Expression/#post_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"post_response_text (optional)"},{"location":"teacher/reference/response_area_components/Expression/#pre_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"pre_response_text (optional)"},{"location":"teacher/reference/response_area_components/Expression/#allow-handwrite-experimental","text":"Enables a handwriting canvas in the browser, which allows a student can use to draw their expression, rather than type using Sympy's syntax.","title":"Allow Handwrite (Experimental)"},{"location":"teacher/reference/response_area_components/Expression/#photo-experimental","text":"Allows a student to upload their expression as an image, as an alternative to handwriting if the student isn't using a phone or tablet.","title":"Photo (Experimental)"},{"location":"teacher/reference/response_area_components/Expression/#setting-the-answer","text":"Type the correct answer into the 'Response Area Answer' using standard syntax. As the student enters the answer, this will be rendered using the 'live preview' feature, to ensure the correct expression has been entered. Use the 'Response Area Preview' to check the answer has been set correctly.","title":"Setting The Answer"},{"location":"teacher/reference/response_area_components/Expression/#example-student-response-area","text":"Correct response given Incorrect response given","title":"Example Student Response area"},{"location":"teacher/reference/response_area_components/Matrix/","text":"Matrix \u00b6 Matrix response area. Will populate the component with a grid of text input fields, in order to facilitate inputing matrices. Evaluation Function Options \u00b6 ArrayEqual \u00b6 Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently. ArraySymbolicEqual \u00b6 Very similar to the SymbolicEqual grading function, but grading any list of expressions instead. This algorithm can take any level of nesting for \"response\" and \"answer\" fields, as grading is done recursively (as long as both shapes are identical). Symbolic grading is done using the SymPy library. See SymPy for further information. Component Parameters \u00b6 rows and cols (required) \u00b6 Required paramter, describes the shape of the Matrix to be displayed. In the 'Response area answer' section, the number of rows and columns can either be typed directly into the corresponding boxes, or adjusted using the up and down arrows, which appear once the mouse hovers over the input box. post_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. pre_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. Setting The Answer \u00b6 Once the required number of rows and cols has been selected, Each element of the matrix can be entered by clicking the individual input boxes and typing in the correct numbers. Example Student Response \u00b6 Correct response given Incorrect response given","title":"Matrix"},{"location":"teacher/reference/response_area_components/Matrix/#matrix","text":"Matrix response area. Will populate the component with a grid of text input fields, in order to facilitate inputing matrices.","title":"Matrix"},{"location":"teacher/reference/response_area_components/Matrix/#evaluation-function-options","text":"","title":"Evaluation Function Options"},{"location":"teacher/reference/response_area_components/Matrix/#arrayequal","text":"Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently.","title":"ArrayEqual"},{"location":"teacher/reference/response_area_components/Matrix/#arraysymbolicequal","text":"Very similar to the SymbolicEqual grading function, but grading any list of expressions instead. This algorithm can take any level of nesting for \"response\" and \"answer\" fields, as grading is done recursively (as long as both shapes are identical). Symbolic grading is done using the SymPy library. See SymPy for further information.","title":"ArraySymbolicEqual"},{"location":"teacher/reference/response_area_components/Matrix/#component-parameters","text":"","title":"Component Parameters"},{"location":"teacher/reference/response_area_components/Matrix/#rows-and-cols-required","text":"Required paramter, describes the shape of the Matrix to be displayed. In the 'Response area answer' section, the number of rows and columns can either be typed directly into the corresponding boxes, or adjusted using the up and down arrows, which appear once the mouse hovers over the input box.","title":"rows and cols (required)"},{"location":"teacher/reference/response_area_components/Matrix/#post_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"post_response_text (optional)"},{"location":"teacher/reference/response_area_components/Matrix/#pre_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"pre_response_text (optional)"},{"location":"teacher/reference/response_area_components/Matrix/#setting-the-answer","text":"Once the required number of rows and cols has been selected, Each element of the matrix can be entered by clicking the individual input boxes and typing in the correct numbers.","title":"Setting The Answer"},{"location":"teacher/reference/response_area_components/Matrix/#example-student-response","text":"Correct response given Incorrect response given","title":"Example Student Response"},{"location":"teacher/reference/response_area_components/MultipleChoice/","text":"MultipleChoice \u00b6 General multiple choice response area. Features multiple options for single answer and randomising the order. Evaluation Function Options \u00b6 ArrayEqual \u00b6 Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently. Parameters \u00b6 options (required) \u00b6 This is an array containing strings, each representing an option in the multiple choice component. These are parsed using the parseEquations function, meaning they can support markdown styling and LaTeX. Example \"options\" : [ \"\\\\( 4x^2 + 2 = \\\\frac{\\\\delta y}{\\\\delta x} \\\\)\" , \"\\\\( \\\\pi = 3 \\\\)\" , \"\\\\( K_{iakb} U^{b}_{k} = f^{a}_{i} \\\\)\" , \"\\\\( 3 = \\\\pi \\\\)\" , ] Randomise (optional) \u00b6 This is an optional boolean which will shuffle the options array on each render of this component. switch to 'randomise' singleAnswer (optional) \u00b6 By default, each item options is rendered using the html checkbox input type. Setting the singleAnswer boolean flag will turn those into radio buttons, allowing the student to select only one option at a time. Note Changing this flag will alter the shape of the Response Structure , and potentially require changing the grading function type and settings. Response Structure \u00b6 This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. This structure is different depending on if the singleAnswer option was used: singleAnswer == False (or undefined) \u00b6 In this case, the user data is saved as an array with the same length as options , where each item is either a 1 or a 0 depending on if the corresponding option was selected. Example If for an instance where there are 4 options, and the first and third options were selected, the response field would be: \"response\" : [ 1 , 0 , 1 , 0 ] Example Screenshot: SingleAnswer = False singleAnswer == True \u00b6 In this case, there is only one correct answer, and each option is displayed as a radio button. Therefore the response field contains only one integer, corresponding to the index of the selected option. Example If for an instance where there are 4 options, and the third option was selected, the response field would be: \"response\" : 2 Example Screenshot: SingleAnswer = True Example Student Response \u00b6 This shows a response where singleAnswer was set to False, since each option is displayed as a checkbox","title":"MultipleChoice"},{"location":"teacher/reference/response_area_components/MultipleChoice/#multiplechoice","text":"General multiple choice response area. Features multiple options for single answer and randomising the order.","title":"MultipleChoice"},{"location":"teacher/reference/response_area_components/MultipleChoice/#evaluation-function-options","text":"","title":"Evaluation Function Options"},{"location":"teacher/reference/response_area_components/MultipleChoice/#arrayequal","text":"Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently.","title":"ArrayEqual"},{"location":"teacher/reference/response_area_components/MultipleChoice/#parameters","text":"","title":"Parameters"},{"location":"teacher/reference/response_area_components/MultipleChoice/#options-required","text":"This is an array containing strings, each representing an option in the multiple choice component. These are parsed using the parseEquations function, meaning they can support markdown styling and LaTeX. Example \"options\" : [ \"\\\\( 4x^2 + 2 = \\\\frac{\\\\delta y}{\\\\delta x} \\\\)\" , \"\\\\( \\\\pi = 3 \\\\)\" , \"\\\\( K_{iakb} U^{b}_{k} = f^{a}_{i} \\\\)\" , \"\\\\( 3 = \\\\pi \\\\)\" , ]","title":"options (required)"},{"location":"teacher/reference/response_area_components/MultipleChoice/#randomise-optional","text":"This is an optional boolean which will shuffle the options array on each render of this component. switch to 'randomise'","title":"Randomise (optional)"},{"location":"teacher/reference/response_area_components/MultipleChoice/#singleanswer-optional","text":"By default, each item options is rendered using the html checkbox input type. Setting the singleAnswer boolean flag will turn those into radio buttons, allowing the student to select only one option at a time. Note Changing this flag will alter the shape of the Response Structure , and potentially require changing the grading function type and settings.","title":"singleAnswer (optional)"},{"location":"teacher/reference/response_area_components/MultipleChoice/#response-structure","text":"This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. This structure is different depending on if the singleAnswer option was used:","title":"Response Structure"},{"location":"teacher/reference/response_area_components/MultipleChoice/#singleanswer-false-or-undefined","text":"In this case, the user data is saved as an array with the same length as options , where each item is either a 1 or a 0 depending on if the corresponding option was selected. Example If for an instance where there are 4 options, and the first and third options were selected, the response field would be: \"response\" : [ 1 , 0 , 1 , 0 ] Example Screenshot: SingleAnswer = False","title":"singleAnswer == False (or undefined)"},{"location":"teacher/reference/response_area_components/MultipleChoice/#singleanswer-true","text":"In this case, there is only one correct answer, and each option is displayed as a radio button. Therefore the response field contains only one integer, corresponding to the index of the selected option. Example If for an instance where there are 4 options, and the third option was selected, the response field would be: \"response\" : 2 Example Screenshot: SingleAnswer = True","title":"singleAnswer == True"},{"location":"teacher/reference/response_area_components/MultipleChoice/#example-student-response","text":"This shows a response where singleAnswer was set to False, since each option is displayed as a checkbox","title":"Example Student Response"},{"location":"teacher/reference/response_area_components/Number/","text":"Number \u00b6 Very similar to the Text response area, except the user response is parsed as a float. Evaluation Function Options \u00b6 isSimilar \u00b6 Calculates the difference between the teacher answer (ans) and the student response (res); compares this to an allowable difference comprising an absolute tolerance (atol) and a relative tolerance (rtol). isExactEqual \u00b6 A strict comparison in Python using '=='. This function is a basic utility but often not the function you really want to use because it is quite brittle. Setting The Answer \u00b6 In the 'Response Area Answer' section, enter the required float into the input field. To test this, try typing correct and incorrect answers into the 'Response Area Preview' section. Component Parameters \u00b6 post_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. pre_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. grading_parameters (optional) \u00b6 atol : Absolute tolerance parameter rtol : Relative tolerance parameter Valid params include atol and rtol, which can be used in combination, or alone. As the comparison made is the following: is_correct = abs(res - ans) <= (atol + rtol*abs(ans)) Response Structure \u00b6 This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. The response is simply sent as a float, parsed from the input field using the JavaScript parseFloat function. Example \"response\" : 15.8 Example Student Response \u00b6 Correct response: Incorrect response:","title":"Number"},{"location":"teacher/reference/response_area_components/Number/#number","text":"Very similar to the Text response area, except the user response is parsed as a float.","title":"Number"},{"location":"teacher/reference/response_area_components/Number/#evaluation-function-options","text":"","title":"Evaluation Function Options"},{"location":"teacher/reference/response_area_components/Number/#issimilar","text":"Calculates the difference between the teacher answer (ans) and the student response (res); compares this to an allowable difference comprising an absolute tolerance (atol) and a relative tolerance (rtol).","title":"isSimilar"},{"location":"teacher/reference/response_area_components/Number/#isexactequal","text":"A strict comparison in Python using '=='. This function is a basic utility but often not the function you really want to use because it is quite brittle.","title":"isExactEqual"},{"location":"teacher/reference/response_area_components/Number/#setting-the-answer","text":"In the 'Response Area Answer' section, enter the required float into the input field. To test this, try typing correct and incorrect answers into the 'Response Area Preview' section.","title":"Setting The Answer"},{"location":"teacher/reference/response_area_components/Number/#component-parameters","text":"","title":"Component Parameters"},{"location":"teacher/reference/response_area_components/Number/#post_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"post_response_text (optional)"},{"location":"teacher/reference/response_area_components/Number/#pre_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"pre_response_text (optional)"},{"location":"teacher/reference/response_area_components/Number/#grading_parameters-optional","text":"atol : Absolute tolerance parameter rtol : Relative tolerance parameter Valid params include atol and rtol, which can be used in combination, or alone. As the comparison made is the following: is_correct = abs(res - ans) <= (atol + rtol*abs(ans))","title":"grading_parameters (optional)"},{"location":"teacher/reference/response_area_components/Number/#response-structure","text":"This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. The response is simply sent as a float, parsed from the input field using the JavaScript parseFloat function. Example \"response\" : 15.8","title":"Response Structure"},{"location":"teacher/reference/response_area_components/Number/#example-student-response","text":"Correct response: Incorrect response:","title":"Example Student Response"},{"location":"teacher/reference/response_area_components/NumericUnits/","text":"NumericUnits \u00b6 Provides two input fields with Number and Units placeholder texts. This area will also display its associated grading function (as seen in the screenshot below). Note this area will display how the user's response was interpred using the interp_string field provided in the feedback object returned by that function (if it exists). Component Parameters \u00b6 pre_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. Response Structure \u00b6 This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. In this case, the response is a single string which features the user's response to both fields separated by a space. Example \"response\" : \"150 g\" Example Screenshot \u00b6","title":"NumericUnits"},{"location":"teacher/reference/response_area_components/NumericUnits/#numericunits","text":"Provides two input fields with Number and Units placeholder texts. This area will also display its associated grading function (as seen in the screenshot below). Note this area will display how the user's response was interpred using the interp_string field provided in the feedback object returned by that function (if it exists).","title":"NumericUnits"},{"location":"teacher/reference/response_area_components/NumericUnits/#component-parameters","text":"","title":"Component Parameters"},{"location":"teacher/reference/response_area_components/NumericUnits/#pre_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"pre_response_text (optional)"},{"location":"teacher/reference/response_area_components/NumericUnits/#response-structure","text":"This is how the react component will structure the user's input to the Grading Gateway, when they press the check button. In this case, the response is a single string which features the user's response to both fields separated by a space. Example \"response\" : \"150 g\"","title":"Response Structure"},{"location":"teacher/reference/response_area_components/NumericUnits/#example-screenshot","text":"","title":"Example Screenshot"},{"location":"teacher/reference/response_area_components/Table/","text":"Table \u00b6 Table response area. Will populate the component with a grid of text input fields, in order to facilitate inputing elements of a table. The number of rows and columns can be specified, along with their corresponding names. Evaluation Function Options \u00b6 ArrayEqual \u00b6 Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently. ArraySymbolicEqual \u00b6 Very similar to the SymbolicEqual grading function, but grading any list of expressions instead. This algorithm can take any level of nesting for \"response\" and \"answer\" fields, as grading is done recursively (as long as both shapes are identical). Symbolic grading is done using the SymPy library. See SymPy for further information. Component Parameters \u00b6 rows \u00b6 The number of rows required for the table can be entered into this input field, either through typing directly, or using thr up and down arrows located inside the box. cols \u00b6 The number of columns required for the table can be entered into this input field, either through typing directly, or using thr up and down arrows located inside the box. post_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. pre_response_text (optional) \u00b6 Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax. Setting The Answer \u00b6 Once the required number of rows and cols has been inputted, The names of each row and column should be changed depending on their corresponding variables. The value of each grid element can then be entered into individual input fields. If the row or column names are not changed, these will appear blank in the student response area. Example Student Response \u00b6 Correct response: Incorrect response:","title":"Table"},{"location":"teacher/reference/response_area_components/Table/#table","text":"Table response area. Will populate the component with a grid of text input fields, in order to facilitate inputing elements of a table. The number of rows and columns can be specified, along with their corresponding names.","title":"Table"},{"location":"teacher/reference/response_area_components/Table/#evaluation-function-options","text":"","title":"Evaluation Function Options"},{"location":"teacher/reference/response_area_components/Table/#arrayequal","text":"Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently.","title":"ArrayEqual"},{"location":"teacher/reference/response_area_components/Table/#arraysymbolicequal","text":"Very similar to the SymbolicEqual grading function, but grading any list of expressions instead. This algorithm can take any level of nesting for \"response\" and \"answer\" fields, as grading is done recursively (as long as both shapes are identical). Symbolic grading is done using the SymPy library. See SymPy for further information.","title":"ArraySymbolicEqual"},{"location":"teacher/reference/response_area_components/Table/#component-parameters","text":"","title":"Component Parameters"},{"location":"teacher/reference/response_area_components/Table/#rows","text":"The number of rows required for the table can be entered into this input field, either through typing directly, or using thr up and down arrows located inside the box.","title":"rows"},{"location":"teacher/reference/response_area_components/Table/#cols","text":"The number of columns required for the table can be entered into this input field, either through typing directly, or using thr up and down arrows located inside the box.","title":"cols"},{"location":"teacher/reference/response_area_components/Table/#post_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"post_response_text (optional)"},{"location":"teacher/reference/response_area_components/Table/#pre_response_text-optional","text":"Text block to be displayed to the left of the input field. Markdown and LaTeX are allowed following the usual syntax.","title":"pre_response_text (optional)"},{"location":"teacher/reference/response_area_components/Table/#setting-the-answer","text":"Once the required number of rows and cols has been inputted, The names of each row and column should be changed depending on their corresponding variables. The value of each grid element can then be entered into individual input fields. If the row or column names are not changed, these will appear blank in the student response area.","title":"Setting The Answer"},{"location":"teacher/reference/response_area_components/Table/#example-student-response","text":"Correct response: Incorrect response:","title":"Example Student Response"},{"location":"teacher/reference/response_area_components/Text/","text":"","title":"Text"},{"location":"dev_eval_function_docs/isExactEqual/","text":"IsExactEqual \u00b6 Could be qualified as the simplest form of evaluation function, testing exact equality. This function will use the default python == test to compare answer and responses. It doesn't infer any types - meaning it requires a params.type to be supplied. Inputs \u00b6 This function requires a parameter to function properly: { \"params\" : { \"type\" : \"<string>\" (a n y o f [ \"int\" , \"float\" , \"str\" , \"dict\" ] ) } \"response\" : <> , \"answer\" : <> } Outputs \u00b6 Outputs to the grade command will feature: { \"command\" : \"eval\" , \"result\" : { \"is_correct\" : \"<bool>\" } } Examples \u00b6 Simple String Comparison \u00b6 { \"answer\" : \"hydrophobic\" , \"response\" \"hydrophobic\" , \"params\" : { \"type\" : \"str\" } } { \"example\" : { \"Something\" : \"something\" } }","title":"isExactEqual"},{"location":"dev_eval_function_docs/isExactEqual/#isexactequal","text":"Could be qualified as the simplest form of evaluation function, testing exact equality. This function will use the default python == test to compare answer and responses. It doesn't infer any types - meaning it requires a params.type to be supplied.","title":"IsExactEqual"},{"location":"dev_eval_function_docs/isExactEqual/#inputs","text":"This function requires a parameter to function properly: { \"params\" : { \"type\" : \"<string>\" (a n y o f [ \"int\" , \"float\" , \"str\" , \"dict\" ] ) } \"response\" : <> , \"answer\" : <> }","title":"Inputs"},{"location":"dev_eval_function_docs/isExactEqual/#outputs","text":"Outputs to the grade command will feature: { \"command\" : \"eval\" , \"result\" : { \"is_correct\" : \"<bool>\" } }","title":"Outputs"},{"location":"dev_eval_function_docs/isExactEqual/#examples","text":"","title":"Examples"},{"location":"dev_eval_function_docs/isExactEqual/#simple-string-comparison","text":"{ \"answer\" : \"hydrophobic\" , \"response\" \"hydrophobic\" , \"params\" : { \"type\" : \"str\" } } { \"example\" : { \"Something\" : \"something\" } }","title":"Simple String Comparison"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/","text":"ComparePhysicalQuantities \u00b6 Evaluation function which proveds some basic some dimensional analysis functionality. DEPRECATED Comparing physical quantities RECOMMENDED ALTERNATIVE: CompareExpressions with the physical_quantity parameter set to true Substitutions of symbols before comparison of expressions is done Checking if a comma separated list of expressions can be interpreted as a set groups that satisfies the Buckingham Pi theorem Note: When the quantities grading parameter is set, this function cannot handle short form symbols for units. Thus when defining quantities all units must be given with full names in lower-case letters. For example Nm/s or Newton*metre/SECOND will not be handled correctly, but newton*metre/second will. Note: Prefixes have lower precedence than exponentiation, e.g. 10*cm**2 will be interpreted as \\(10 \\cdot 10^{-2} \\mathrm{metre}^2\\) rather than \\(10 (10^(-2)\\mathrm{metre})^2\\) . Note: This function allows omitting * and using ^ instead of ** if the grading parameter strict_syntax is set to false. In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols. Note: Only the short forms listed in the tables below are accepted. Not all units that are supported have short forms (since this leads to ambiguities). Note: When using the short forms the following convention is assumed: - Long form names takes precedence over sequences of short forms, e.g. e.g. mN will be interpreted as milli newton , Nm as newton metre , mmN as milli metre newton , mNm as milli newton metre and Nmm as newton milli metre . - Short form symbols of prefixes will take precedence over short form symbols of units from the left, e.g. - If there is a short form symbol for a prefix that collides with the short form for a unit (i.e. m ) then it is assumed the that unit will always be placed to the right of another unit in compound units, e.g. mN will be interpreted as milli newton , Nm as newton metre , mmN as milli metre newton , mNm as milli newton metre and Nmm as newton milli metre . - Longer short form symbols take precedence over shorter short forms, e.g. sr will be interpreted as steradian instead of second radian . Note: setting elementary_functions to true will disable using short forms symbols for units. Note: When running the unit test some tests are expected to take much longer than the other. These tests can be skipped by adding skip_resource_intensive_tests as a command line argument to improve iteration times. Changing default feedback messages \u00b6 The feedback messages can be set on a per-task basis (see description of the custom_feedback input parameter). The default feedback messages are defined in feedback_responses_list defined near the top of evaulation.py , which contains a list of dictionaries of feedback responses that are used througout the code. All feedback messages visible to learners are defined in these dictionaries. The entries in the dictionaries are either be string of functions that return strings. Inputs \u00b6 All input parameters need to be supplied via the Grading parameters panel. There are seven optional parameters that can be set: elementary_functions , substitutions , quantities , strict_syntax , rtol , atol and comparison . custom_feedback \u00b6 Custom feedback can be set on a per-task basis. Note: Custom feedback only supports fixed strings, this means that for some situations the custom feedback cannot be as detailed as the default feedback. The parameter must be set as a dictionary with keys from the feedback tags listed below. The value for each key can be any string. Feedback tags for all comparisons \u00b6 PARSE_ERROR_WARNING Response cannot be parsed as an expression or physical quantity. PER_FOR_DIVISION Warns about risk of ambiguity when using per instead / for division. STRICT_SYNTAX_EXPONENTIATION Warns that ^ cannot be used for exponentiation when strict_syntax is set to true . QUANTITIES_NOT_WRITTEN_CORRECTLY Text in error message that appears if list of quantities could not be parsed. SUBSTITUTIONS_NOT_WRITTEN_CORRECTLY Text in error message that appears if list of substitutions could not be parsed. Feedback tags for buckinghamPi comparison \u00b6 VALID_CANDIDATE_SET Message that is displayed when a response is found to be a valid set of groups. Note: setting this will not affect the Correct/Incorrect message, it will only add further text. NOT_DIMENSIONLESS Message displayed when at least one groups is not dimensionless. MORE_GROUPS_THAN_REFERENCE_SET Message displayed when the response contains more groups than necessary. CANDIDATE_GROUPS_NOT_INDEPENDENT Message displayed when the groups in the response are not independent. TOO_FEW_INDEPENDENT_GROUPS Message displayed when the response contains fewer groups than necessary. UNKNOWN_SYMBOL Message displayed when the response contains some undefined symbol. SUM_WITH_INDEPENDENT_TERMS Message displayed when the response has too few groups but one (or more) of the groups is a sum with independent terms. elementary_functions \u00b6 When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting elementary_functions to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview. sin , sinc , csc (alternative cosec ), cos , sec , tan , cot (alternative cotan ), asin (alternative arcsin ), acsc (alternatives arccsc , arccosec ), acos (alternative arccos ), asec (alternative arcsec ), atan (alternative arctan ), acot (alternatives arccot , arccotan ), atan2 (alternative arctan2 ), sinh , cosh , tanh , csch (alternative cosech ), sech , asinh (alternative arcsinh ), acosh (alternative arccosh ), atanh (alternative arctanh ), acsch (alternatives arccsch , arcosech ), asech (alternative arcsech ), exp (alternative Exp ), E (equivalent to exp(1) , alternative e ), log , sqrt , sign , Abs (alternative abs ), Max (alternative max ), Min (alternative min ), arg , ceiling (alternative ceil ), floor Note: setting elementary_functions to true will disable using short forms symbols for units. substitutions \u00b6 String that lists all substitutions that should be done to the answer and response inputs before processing. Each substitution should be written in the form ('original string','substitution string') and all pairs concatenated into a single string. Substitutions can be grouped by adding | between two substitutions. Then all substitutions before | will be performed before the substitutions after | . The input can contain an arbitrary number of substitutions and | symbols. Note that using substitutions will replace all default definitions of quantities and dimensions. quantities \u00b6 String that lists all quantities that can be used in the answer and response. Each quantity should be written in the form ('quantity name','(units)') and all pairs concatenated into a single string. See tables below for available default units. Whenever units are used they must be written exactly as in the left columns of the tables given below (no short forms or single-character symbols) and units must be multiplied (or divided) by each other, as well as any accompanying quantities. NOTE: Using units and predefined quantities at the same time in an answer or response can cause problems (especially if quantities are denoted using single characters). Ideally it should be clear that either predefined quantities, or units should only be used from the question. If the comparison parameter is set to dimensions , it is not necessary to give exact units for each quantity, but the dimensions must be given instead. See tables below for available default dimensions. If the comparison parameter is set to buckinghamPi , then quantities should be set in a different way. See the detailed description of buckinghamPi further down. Table: Base SI units \u00b6 SI base units taken from Table 1 of https://physics.nist.gov/cuu/Units/units.html Note that gram is used as a base unit instead of kilogram. SI base unit Symbol Dimension name metre m length gram g mass second s time ampere A electriccurrent kelvin k temperature mole mol amountofsubstance candela cd luminousintensity Table: SI prefixes \u00b6 SI base units taken from Table 5 of https://physics.nist.gov/cuu/Units/prefixes.html SI Prefix Symbol Factor SI Prefix Symbol Factor yotta Y \\(10^{24}\\) deci d \\(10^{-1}\\) zetta Z \\(10^{21}\\) centi c \\(10^{-2}\\) exa' E \\(10^{18}\\) milli m \\(10^{-3}\\) peta P \\(10^{15}\\) micro mu \\(10^{-6}\\) tera T \\(10^{12}\\) nano n \\(10^{-9}\\) giga G \\(10^{9}\\) pico p \\(10^{-12}\\) mega M \\(10^{6}\\) femto f \\(10^{-15}\\) kilo k \\(10^{3}\\) atto a \\(10^{-18}\\) hecto h \\(10^{2}\\) zepto z \\(10^{-21}\\) deka da \\(10^{1}\\) yocto y \\(10^{-24}\\) Table: Derived SI units \u00b6 Derived SI units taken from Table 3 of https://physics.nist.gov/cuu/Units/units.html Note that degrees Celsius is omitted. Note that the function treats radians and steradians as dimensionless values. Unit name Symbol Expressed in base SI units radian r 1 steradian sr 1 hertz Hz \\(\\mathrm{second}^{-1}\\) newton N \\(\\mathrm{metre}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) pascal Pa \\(\\mathrm{metre}^{-1}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) joule J \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}\\) watt W \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-3}\\) coulomb C \\(\\mathrm{second~ampere}\\) volt V \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-1}\\) farad F \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^4~\\mathrm{ampere}^2\\) ohm O \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-2}\\) siemens S \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^3~\\mathrm{ampere}^2\\) weber Wb \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-1}\\) tesla T \\(\\mathrm{kilogram~second}^{-2} \\mathrm{ampere}^{-1}\\) henry H \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-2}\\) lumen lm \\(\\mathrm{candela}\\) lux lx \\(\\mathrm{metre}^{-2}~\\mathrm{candela}\\) becquerel Bq \\(\\mathrm{second}^{-1}\\) gray Gy \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) sievert Sv \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) katal kat \\(\\mathrm{mole~second}^{-1}\\) Table: Common non-SI units \u00b6 Commonly used non-SI units taken from Table 6 and 7 of https://physics.nist.gov/cuu/Units/outside.html Note that the function treats angles, neper and bel as dimensionless values. Note that only the first table in this section has short form symbols defined, the second table does not. Unit name Symbol Expressed in SI units minute min \\(60~\\mathrm{second}\\) hour h \\(3600~\\mathrm{second}\\) degree deg \\(\\frac{\\pi}{180}\\) liter l \\(10^{-3}~\\mathrm{metre}^3\\) metric_ton t \\(10^3~\\mathrm{kilogram}\\) neper Np \\(1\\) bel B \\(\\frac{1}{2}~\\ln(10)\\) electronvolt eV \\(1.60218 \\cdot 10^{-19}~\\mathrm{joule}\\) atomic_mass_unit u \\(1.66054 \\cdot 10^{-27}~\\mathrm{kilogram}\\) angstrom \u00e5 \\(10^{-10}~\\mathrm{metre}\\) Unit name Expressed in SI units day \\(86400~\\mathrm{second}\\) angleminute \\(\\frac{\\pi}{10800}\\) anglesecond \\(\\frac{\\pi}{648000}\\) astronomicalunit \\(149597870700~\\mathrm{metre}\\) nauticalmile \\(1852~\\mathrm{metre}\\) knot \\(\\frac{1852}{3600}~\\mathrm{metre~second}^{-1}\\) are \\(10^2~\\mathrm{metre}^2\\) hectare \\(10^4~\\mathrm{metre}^2\\) bar \\(10^5~\\mathrm{pascal}\\) barn \\(10^{-28}~\\mathrm{metre}\\) curie $3.7 \\cdot 10^{10}~\\mathrm{becquerel} roentgen \\(2.58 \\cdot 10^{-4}~\\mathrm{kelvin~(kilogram)}^{-1}\\) rad \\(10^{-2}~\\mathrm{gray}\\) rem \\(10^{-2}~\\mathrm{sievert}\\) Table: Imperial units \u00b6 Commonly imperial units taken from https://en.wikipedia.org/wiki/Imperial_units Unit name Symbol Expressed in SI units inch in \\(0.0254~\\mathrm{metre}\\) foot ft \\(0.3048~\\mathrm{metre}\\) yard yd \\(0.9144~\\mathrm{metre}\\) mile mi \\(1609.344~\\mathrm{metre}\\) fluid ounce fl oz \\(28.4130625~\\mathrm{millilitre}\\) gill gi \\(142.0653125~\\mathrm{millilitre}\\) pint pt \\(568.26125~\\mathrm{millilitre}\\) quart qt \\(1.1365225~\\mathrm{litre}\\) gallon gal \\(4546.09~\\mathrm{litre}\\) ounce oz \\(28.349523125~\\mathrm{gram}\\) pound lb \\(0.45359237~\\mathrm{kilogram}\\) stone st \\(6.35029318~\\mathrm{kilogram}\\) strict_syntax \u00b6 If strict_syntax is set to true then the answer and response must have * or / between each part of the expressions and exponentiation must be done using ** , e.g. 10*kilo*metre/second**2 is accepted but 10 kilometre/second^2 is not. If strict_syntax is set to false, then * can be omitted and ^ used instead of ** . In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols. By default strict_syntax is set to true. rtol \u00b6 Maximum relative error allowed when comparing expressions. atol \u00b6 Maximum absolute error allowed when comparing expressions. comparison \u00b6 Parameter that determines what kind of comparison is done. There are four possible options: expression Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close (as specified by the atol and rtol parameters). expressionExact Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is identical to as high precision as possible. dimensions Checks that the answer and response have the same dimensions, does not compare the values of the physical quantities. buckinghamPi Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem. For more details on each options see the description below and the corresponding examples. If comparison is not specified it defaults to expression . expression \u00b6 Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close. How big the difference is between the value of the answer and the value of the response is decided by the rtol and atol parameters. If neither atol nor rtol is specified the function will allow a relative error of \\(10^{-12}\\) . If atol is specified its value will be interpreted as the maximum allowed absolute error. If rtol is specified its value will be interpreted as the maximum allowed relative error. If both atol and rtol the function will check both the absolute and relative error. expressionExact \u00b6 Converts the expression to base SI units and checks that the answer and response are identical to the highest precision possible (note that some unit conversions are not exact and that using decimal numbers in the answer or response limits this to floating point precision). dimensions \u00b6 Checks that the answer and response have the same dimensions, but does not compare the values of the physical quantities. With this option the quantities (specified by the quantities parameter) can be given either dimension only, or units. buckinghamPi \u00b6 Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem. There are three different ways of supplying this function with the necessary information. - In the answer, provide an example set of groups as a comma seprated list. When used this way the function assumes that the given list is correct and contains at least the minimum number of groups. - In the quantities parameter, supply a list of what the dimensions for each quantity is and set answer to - . The function will then compute a list of sufficiently many independen dimensionless quantities and compare to the response. - In the quantities parameter, supply a list of what the dimensions for each quantity is and in the answer, supply a list of groups as in the first option. The function will then check that the supplied answer is dimensionless and has a sufficient number of independent groups before comparing it to the response. Note that in lists of groups the items should ideally be written on the form \\(q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities and \\(c_1, c_2 \\ldots c_n\\) are integers, but the function can also handle item that are sums with terms written on the form \\(a \\cdot q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities, \\(c_1, c_2 \\ldots c_n\\) rational numbers and \\(a\\) a constant. If the total number of groups is less than required the set of groups is considered invalid, even if there is a sufficient number of terms with independent power products in the response.","title":"comparePhysicalQuantities"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#comparephysicalquantities","text":"Evaluation function which proveds some basic some dimensional analysis functionality. DEPRECATED Comparing physical quantities RECOMMENDED ALTERNATIVE: CompareExpressions with the physical_quantity parameter set to true Substitutions of symbols before comparison of expressions is done Checking if a comma separated list of expressions can be interpreted as a set groups that satisfies the Buckingham Pi theorem Note: When the quantities grading parameter is set, this function cannot handle short form symbols for units. Thus when defining quantities all units must be given with full names in lower-case letters. For example Nm/s or Newton*metre/SECOND will not be handled correctly, but newton*metre/second will. Note: Prefixes have lower precedence than exponentiation, e.g. 10*cm**2 will be interpreted as \\(10 \\cdot 10^{-2} \\mathrm{metre}^2\\) rather than \\(10 (10^(-2)\\mathrm{metre})^2\\) . Note: This function allows omitting * and using ^ instead of ** if the grading parameter strict_syntax is set to false. In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols. Note: Only the short forms listed in the tables below are accepted. Not all units that are supported have short forms (since this leads to ambiguities). Note: When using the short forms the following convention is assumed: - Long form names takes precedence over sequences of short forms, e.g. e.g. mN will be interpreted as milli newton , Nm as newton metre , mmN as milli metre newton , mNm as milli newton metre and Nmm as newton milli metre . - Short form symbols of prefixes will take precedence over short form symbols of units from the left, e.g. - If there is a short form symbol for a prefix that collides with the short form for a unit (i.e. m ) then it is assumed the that unit will always be placed to the right of another unit in compound units, e.g. mN will be interpreted as milli newton , Nm as newton metre , mmN as milli metre newton , mNm as milli newton metre and Nmm as newton milli metre . - Longer short form symbols take precedence over shorter short forms, e.g. sr will be interpreted as steradian instead of second radian . Note: setting elementary_functions to true will disable using short forms symbols for units. Note: When running the unit test some tests are expected to take much longer than the other. These tests can be skipped by adding skip_resource_intensive_tests as a command line argument to improve iteration times.","title":"ComparePhysicalQuantities"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#changing-default-feedback-messages","text":"The feedback messages can be set on a per-task basis (see description of the custom_feedback input parameter). The default feedback messages are defined in feedback_responses_list defined near the top of evaulation.py , which contains a list of dictionaries of feedback responses that are used througout the code. All feedback messages visible to learners are defined in these dictionaries. The entries in the dictionaries are either be string of functions that return strings.","title":"Changing default feedback messages"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#inputs","text":"All input parameters need to be supplied via the Grading parameters panel. There are seven optional parameters that can be set: elementary_functions , substitutions , quantities , strict_syntax , rtol , atol and comparison .","title":"Inputs"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#custom_feedback","text":"Custom feedback can be set on a per-task basis. Note: Custom feedback only supports fixed strings, this means that for some situations the custom feedback cannot be as detailed as the default feedback. The parameter must be set as a dictionary with keys from the feedback tags listed below. The value for each key can be any string.","title":"custom_feedback"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#feedback-tags-for-all-comparisons","text":"PARSE_ERROR_WARNING Response cannot be parsed as an expression or physical quantity. PER_FOR_DIVISION Warns about risk of ambiguity when using per instead / for division. STRICT_SYNTAX_EXPONENTIATION Warns that ^ cannot be used for exponentiation when strict_syntax is set to true . QUANTITIES_NOT_WRITTEN_CORRECTLY Text in error message that appears if list of quantities could not be parsed. SUBSTITUTIONS_NOT_WRITTEN_CORRECTLY Text in error message that appears if list of substitutions could not be parsed.","title":"Feedback tags for all comparisons"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#feedback-tags-for-buckinghampi-comparison","text":"VALID_CANDIDATE_SET Message that is displayed when a response is found to be a valid set of groups. Note: setting this will not affect the Correct/Incorrect message, it will only add further text. NOT_DIMENSIONLESS Message displayed when at least one groups is not dimensionless. MORE_GROUPS_THAN_REFERENCE_SET Message displayed when the response contains more groups than necessary. CANDIDATE_GROUPS_NOT_INDEPENDENT Message displayed when the groups in the response are not independent. TOO_FEW_INDEPENDENT_GROUPS Message displayed when the response contains fewer groups than necessary. UNKNOWN_SYMBOL Message displayed when the response contains some undefined symbol. SUM_WITH_INDEPENDENT_TERMS Message displayed when the response has too few groups but one (or more) of the groups is a sum with independent terms.","title":"Feedback tags for buckinghamPi comparison"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#elementary_functions","text":"When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting elementary_functions to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview. sin , sinc , csc (alternative cosec ), cos , sec , tan , cot (alternative cotan ), asin (alternative arcsin ), acsc (alternatives arccsc , arccosec ), acos (alternative arccos ), asec (alternative arcsec ), atan (alternative arctan ), acot (alternatives arccot , arccotan ), atan2 (alternative arctan2 ), sinh , cosh , tanh , csch (alternative cosech ), sech , asinh (alternative arcsinh ), acosh (alternative arccosh ), atanh (alternative arctanh ), acsch (alternatives arccsch , arcosech ), asech (alternative arcsech ), exp (alternative Exp ), E (equivalent to exp(1) , alternative e ), log , sqrt , sign , Abs (alternative abs ), Max (alternative max ), Min (alternative min ), arg , ceiling (alternative ceil ), floor Note: setting elementary_functions to true will disable using short forms symbols for units.","title":"elementary_functions"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#substitutions","text":"String that lists all substitutions that should be done to the answer and response inputs before processing. Each substitution should be written in the form ('original string','substitution string') and all pairs concatenated into a single string. Substitutions can be grouped by adding | between two substitutions. Then all substitutions before | will be performed before the substitutions after | . The input can contain an arbitrary number of substitutions and | symbols. Note that using substitutions will replace all default definitions of quantities and dimensions.","title":"substitutions"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#quantities","text":"String that lists all quantities that can be used in the answer and response. Each quantity should be written in the form ('quantity name','(units)') and all pairs concatenated into a single string. See tables below for available default units. Whenever units are used they must be written exactly as in the left columns of the tables given below (no short forms or single-character symbols) and units must be multiplied (or divided) by each other, as well as any accompanying quantities. NOTE: Using units and predefined quantities at the same time in an answer or response can cause problems (especially if quantities are denoted using single characters). Ideally it should be clear that either predefined quantities, or units should only be used from the question. If the comparison parameter is set to dimensions , it is not necessary to give exact units for each quantity, but the dimensions must be given instead. See tables below for available default dimensions. If the comparison parameter is set to buckinghamPi , then quantities should be set in a different way. See the detailed description of buckinghamPi further down.","title":"quantities"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-base-si-units","text":"SI base units taken from Table 1 of https://physics.nist.gov/cuu/Units/units.html Note that gram is used as a base unit instead of kilogram. SI base unit Symbol Dimension name metre m length gram g mass second s time ampere A electriccurrent kelvin k temperature mole mol amountofsubstance candela cd luminousintensity","title":"Table: Base SI units"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-si-prefixes","text":"SI base units taken from Table 5 of https://physics.nist.gov/cuu/Units/prefixes.html SI Prefix Symbol Factor SI Prefix Symbol Factor yotta Y \\(10^{24}\\) deci d \\(10^{-1}\\) zetta Z \\(10^{21}\\) centi c \\(10^{-2}\\) exa' E \\(10^{18}\\) milli m \\(10^{-3}\\) peta P \\(10^{15}\\) micro mu \\(10^{-6}\\) tera T \\(10^{12}\\) nano n \\(10^{-9}\\) giga G \\(10^{9}\\) pico p \\(10^{-12}\\) mega M \\(10^{6}\\) femto f \\(10^{-15}\\) kilo k \\(10^{3}\\) atto a \\(10^{-18}\\) hecto h \\(10^{2}\\) zepto z \\(10^{-21}\\) deka da \\(10^{1}\\) yocto y \\(10^{-24}\\)","title":"Table: SI prefixes"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-derived-si-units","text":"Derived SI units taken from Table 3 of https://physics.nist.gov/cuu/Units/units.html Note that degrees Celsius is omitted. Note that the function treats radians and steradians as dimensionless values. Unit name Symbol Expressed in base SI units radian r 1 steradian sr 1 hertz Hz \\(\\mathrm{second}^{-1}\\) newton N \\(\\mathrm{metre}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) pascal Pa \\(\\mathrm{metre}^{-1}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) joule J \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}\\) watt W \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-3}\\) coulomb C \\(\\mathrm{second~ampere}\\) volt V \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-1}\\) farad F \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^4~\\mathrm{ampere}^2\\) ohm O \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-2}\\) siemens S \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^3~\\mathrm{ampere}^2\\) weber Wb \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-1}\\) tesla T \\(\\mathrm{kilogram~second}^{-2} \\mathrm{ampere}^{-1}\\) henry H \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-2}\\) lumen lm \\(\\mathrm{candela}\\) lux lx \\(\\mathrm{metre}^{-2}~\\mathrm{candela}\\) becquerel Bq \\(\\mathrm{second}^{-1}\\) gray Gy \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) sievert Sv \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) katal kat \\(\\mathrm{mole~second}^{-1}\\)","title":"Table: Derived SI units"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-common-non-si-units","text":"Commonly used non-SI units taken from Table 6 and 7 of https://physics.nist.gov/cuu/Units/outside.html Note that the function treats angles, neper and bel as dimensionless values. Note that only the first table in this section has short form symbols defined, the second table does not. Unit name Symbol Expressed in SI units minute min \\(60~\\mathrm{second}\\) hour h \\(3600~\\mathrm{second}\\) degree deg \\(\\frac{\\pi}{180}\\) liter l \\(10^{-3}~\\mathrm{metre}^3\\) metric_ton t \\(10^3~\\mathrm{kilogram}\\) neper Np \\(1\\) bel B \\(\\frac{1}{2}~\\ln(10)\\) electronvolt eV \\(1.60218 \\cdot 10^{-19}~\\mathrm{joule}\\) atomic_mass_unit u \\(1.66054 \\cdot 10^{-27}~\\mathrm{kilogram}\\) angstrom \u00e5 \\(10^{-10}~\\mathrm{metre}\\) Unit name Expressed in SI units day \\(86400~\\mathrm{second}\\) angleminute \\(\\frac{\\pi}{10800}\\) anglesecond \\(\\frac{\\pi}{648000}\\) astronomicalunit \\(149597870700~\\mathrm{metre}\\) nauticalmile \\(1852~\\mathrm{metre}\\) knot \\(\\frac{1852}{3600}~\\mathrm{metre~second}^{-1}\\) are \\(10^2~\\mathrm{metre}^2\\) hectare \\(10^4~\\mathrm{metre}^2\\) bar \\(10^5~\\mathrm{pascal}\\) barn \\(10^{-28}~\\mathrm{metre}\\) curie $3.7 \\cdot 10^{10}~\\mathrm{becquerel} roentgen \\(2.58 \\cdot 10^{-4}~\\mathrm{kelvin~(kilogram)}^{-1}\\) rad \\(10^{-2}~\\mathrm{gray}\\) rem \\(10^{-2}~\\mathrm{sievert}\\)","title":"Table: Common non-SI units"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#table-imperial-units","text":"Commonly imperial units taken from https://en.wikipedia.org/wiki/Imperial_units Unit name Symbol Expressed in SI units inch in \\(0.0254~\\mathrm{metre}\\) foot ft \\(0.3048~\\mathrm{metre}\\) yard yd \\(0.9144~\\mathrm{metre}\\) mile mi \\(1609.344~\\mathrm{metre}\\) fluid ounce fl oz \\(28.4130625~\\mathrm{millilitre}\\) gill gi \\(142.0653125~\\mathrm{millilitre}\\) pint pt \\(568.26125~\\mathrm{millilitre}\\) quart qt \\(1.1365225~\\mathrm{litre}\\) gallon gal \\(4546.09~\\mathrm{litre}\\) ounce oz \\(28.349523125~\\mathrm{gram}\\) pound lb \\(0.45359237~\\mathrm{kilogram}\\) stone st \\(6.35029318~\\mathrm{kilogram}\\)","title":"Table: Imperial units"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#strict_syntax","text":"If strict_syntax is set to true then the answer and response must have * or / between each part of the expressions and exponentiation must be done using ** , e.g. 10*kilo*metre/second**2 is accepted but 10 kilometre/second^2 is not. If strict_syntax is set to false, then * can be omitted and ^ used instead of ** . In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols. By default strict_syntax is set to true.","title":"strict_syntax"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#rtol","text":"Maximum relative error allowed when comparing expressions.","title":"rtol"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#atol","text":"Maximum absolute error allowed when comparing expressions.","title":"atol"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#comparison","text":"Parameter that determines what kind of comparison is done. There are four possible options: expression Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close (as specified by the atol and rtol parameters). expressionExact Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is identical to as high precision as possible. dimensions Checks that the answer and response have the same dimensions, does not compare the values of the physical quantities. buckinghamPi Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem. For more details on each options see the description below and the corresponding examples. If comparison is not specified it defaults to expression .","title":"comparison"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#expression","text":"Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close. How big the difference is between the value of the answer and the value of the response is decided by the rtol and atol parameters. If neither atol nor rtol is specified the function will allow a relative error of \\(10^{-12}\\) . If atol is specified its value will be interpreted as the maximum allowed absolute error. If rtol is specified its value will be interpreted as the maximum allowed relative error. If both atol and rtol the function will check both the absolute and relative error.","title":"expression"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#expressionexact","text":"Converts the expression to base SI units and checks that the answer and response are identical to the highest precision possible (note that some unit conversions are not exact and that using decimal numbers in the answer or response limits this to floating point precision).","title":"expressionExact"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#dimensions","text":"Checks that the answer and response have the same dimensions, but does not compare the values of the physical quantities. With this option the quantities (specified by the quantities parameter) can be given either dimension only, or units.","title":"dimensions"},{"location":"dev_eval_function_docs/comparePhysicalQuantities/#buckinghampi","text":"Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem. There are three different ways of supplying this function with the necessary information. - In the answer, provide an example set of groups as a comma seprated list. When used this way the function assumes that the given list is correct and contains at least the minimum number of groups. - In the quantities parameter, supply a list of what the dimensions for each quantity is and set answer to - . The function will then compute a list of sufficiently many independen dimensionless quantities and compare to the response. - In the quantities parameter, supply a list of what the dimensions for each quantity is and in the answer, supply a list of groups as in the first option. The function will then check that the supplied answer is dimensionless and has a sufficient number of independent groups before comparing it to the response. Note that in lists of groups the items should ideally be written on the form \\(q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities and \\(c_1, c_2 \\ldots c_n\\) are integers, but the function can also handle item that are sums with terms written on the form \\(a \\cdot q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities, \\(c_1, c_2 \\ldots c_n\\) rational numbers and \\(a\\) a constant. If the total number of groups is less than required the set of groups is considered invalid, even if there is a sufficient number of terms with independent power products in the response.","title":"buckinghamPi"},{"location":"dev_eval_function_docs/symbolicEqual/","text":"SymbolicEqual \u00b6 Evaluates the equality between two symbolic expressions using the python SymPy package. Note that pi is a reserved constant and cannot be used as a symbol name. Inputs \u00b6 Optional grading parameters \u00b6 There are eight optional parameters that can be set: complexNumbers , elementary_functions , specialFunctions , strict_syntax , symbol_assumptions , multiple_answers_criteria , plus_minus and minus_plus . complexNumbers \u00b6 If you want to use I for the imaginary constant, set the grading parameter complexNumbers to True. elementary_functions \u00b6 When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting elementary_functions to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview. sin , sinc , csc (alternative cosec ), cos , sec , tan , cot (alternative cotan ), asin (alternative arcsin ), acsc (alternatives arccsc , arccosec ), acos (alternative arccos ), asec (alternative arcsec ), atan (alternative arctan ), acot (alternatives arccot , arccotan ), atan2 (alternative arctan2 ), sinh , cosh , tanh , csch (alternative cosech ), sech , asinh (alternative arcsinh ), acosh (alternative arccosh ), atanh (alternative arctanh ), acsch (alternatives arccsch , arcosech ), asech (alternative arcsech ), exp (alternative Exp ), E (equivalent to exp(1) , alternative e ), log , sqrt , sign , Abs (alternative abs ), Max (alternative max ), Min (alternative min ), arg , ceiling (alternative ceil ), floor specialFunctions \u00b6 If you want to use the special functions beta (Euler Beta function), gamma (Gamma function) and zeta (Riemann Zeta function), set the grading parameter specialFunctions to True. strict_syntax \u00b6 If strict_syntax is set to true then the answer and response must have * or / between each part of the expressions and exponentiation must be done using ** , e.g. 10*x*y/z**2 is accepted but 10xy/z^2 is not. If strict_syntax is set to false, then * can be omitted and ^ used instead of ** . In this case it is also recommended to list any multicharacter symbols expected to appear in the response as input symbols. By default strict_syntax is set to true. symbol_assumptions \u00b6 This input parameter allows the author to set an extra assumption each symbol. Each assumption should be written on the form ('symbol','assumption name') and all pairs concatenated into a single string. The possible assumption names can be found in this list: SymPy Assumption Predicates multiple_answers_criteria \u00b6 The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. Answers or responses that contain \\(\\pm\\) or \\(\\mp\\) has two possible interpretations which requires further criteria for equality. The grading parameter multiple_answers_criteria controls this. The default setting, all , is that each answer must have a corresponding answer and vice versa. The setting all_responses check that all responses are valid answers and the setting all_answers checks that all answers are found among the responses. plus_minus and minus_plus \u00b6 The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters plus_minus and minus_plus to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences. Outputs \u00b6 Outputs to the eval command will feature: { \"command\" : \"eval\" , \"result\" : { \"is_correct\" : \"<bool>\" , \"response_latex\" : \"<str>\" , \"response_simplified\" : \"<str>\" , \"level\" : \"<int>\" } } response_latex \u00b6 This is a latex string, indicating how the user's response was understood by SymPy. It can be used to provide feedback in the front-end. level \u00b6 The function tests equality using three levels, of increasing complexity. This parameter indicates the level at which equality was found. It is not present if the result is incorrect. response_simplified \u00b6 This is a math-simplified string of the given response. All mathematically-equivalent expressions will yield identical strings under this field. This can be used by teacher dashboards when aggregating common student errors. Examples \u00b6","title":"symbolicEqual"},{"location":"dev_eval_function_docs/symbolicEqual/#symbolicequal","text":"Evaluates the equality between two symbolic expressions using the python SymPy package. Note that pi is a reserved constant and cannot be used as a symbol name.","title":"SymbolicEqual"},{"location":"dev_eval_function_docs/symbolicEqual/#inputs","text":"","title":"Inputs"},{"location":"dev_eval_function_docs/symbolicEqual/#optional-grading-parameters","text":"There are eight optional parameters that can be set: complexNumbers , elementary_functions , specialFunctions , strict_syntax , symbol_assumptions , multiple_answers_criteria , plus_minus and minus_plus .","title":"Optional grading parameters"},{"location":"dev_eval_function_docs/symbolicEqual/#complexnumbers","text":"If you want to use I for the imaginary constant, set the grading parameter complexNumbers to True.","title":"complexNumbers"},{"location":"dev_eval_function_docs/symbolicEqual/#elementary_functions","text":"When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting elementary_functions to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview. sin , sinc , csc (alternative cosec ), cos , sec , tan , cot (alternative cotan ), asin (alternative arcsin ), acsc (alternatives arccsc , arccosec ), acos (alternative arccos ), asec (alternative arcsec ), atan (alternative arctan ), acot (alternatives arccot , arccotan ), atan2 (alternative arctan2 ), sinh , cosh , tanh , csch (alternative cosech ), sech , asinh (alternative arcsinh ), acosh (alternative arccosh ), atanh (alternative arctanh ), acsch (alternatives arccsch , arcosech ), asech (alternative arcsech ), exp (alternative Exp ), E (equivalent to exp(1) , alternative e ), log , sqrt , sign , Abs (alternative abs ), Max (alternative max ), Min (alternative min ), arg , ceiling (alternative ceil ), floor","title":"elementary_functions"},{"location":"dev_eval_function_docs/symbolicEqual/#specialfunctions","text":"If you want to use the special functions beta (Euler Beta function), gamma (Gamma function) and zeta (Riemann Zeta function), set the grading parameter specialFunctions to True.","title":"specialFunctions"},{"location":"dev_eval_function_docs/symbolicEqual/#strict_syntax","text":"If strict_syntax is set to true then the answer and response must have * or / between each part of the expressions and exponentiation must be done using ** , e.g. 10*x*y/z**2 is accepted but 10xy/z^2 is not. If strict_syntax is set to false, then * can be omitted and ^ used instead of ** . In this case it is also recommended to list any multicharacter symbols expected to appear in the response as input symbols. By default strict_syntax is set to true.","title":"strict_syntax"},{"location":"dev_eval_function_docs/symbolicEqual/#symbol_assumptions","text":"This input parameter allows the author to set an extra assumption each symbol. Each assumption should be written on the form ('symbol','assumption name') and all pairs concatenated into a single string. The possible assumption names can be found in this list: SymPy Assumption Predicates","title":"symbol_assumptions"},{"location":"dev_eval_function_docs/symbolicEqual/#multiple_answers_criteria","text":"The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. Answers or responses that contain \\(\\pm\\) or \\(\\mp\\) has two possible interpretations which requires further criteria for equality. The grading parameter multiple_answers_criteria controls this. The default setting, all , is that each answer must have a corresponding answer and vice versa. The setting all_responses check that all responses are valid answers and the setting all_answers checks that all answers are found among the responses.","title":"multiple_answers_criteria"},{"location":"dev_eval_function_docs/symbolicEqual/#plus_minus-and-minus_plus","text":"The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters plus_minus and minus_plus to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences.","title":"plus_minus and minus_plus"},{"location":"dev_eval_function_docs/symbolicEqual/#outputs","text":"Outputs to the eval command will feature: { \"command\" : \"eval\" , \"result\" : { \"is_correct\" : \"<bool>\" , \"response_latex\" : \"<str>\" , \"response_simplified\" : \"<str>\" , \"level\" : \"<int>\" } }","title":"Outputs"},{"location":"dev_eval_function_docs/symbolicEqual/#response_latex","text":"This is a latex string, indicating how the user's response was understood by SymPy. It can be used to provide feedback in the front-end.","title":"response_latex"},{"location":"dev_eval_function_docs/symbolicEqual/#level","text":"The function tests equality using three levels, of increasing complexity. This parameter indicates the level at which equality was found. It is not present if the result is incorrect.","title":"level"},{"location":"dev_eval_function_docs/symbolicEqual/#response_simplified","text":"This is a math-simplified string of the given response. All mathematically-equivalent expressions will yield identical strings under this field. This can be used by teacher dashboards when aggregating common student errors.","title":"response_simplified"},{"location":"dev_eval_function_docs/symbolicEqual/#examples","text":"","title":"Examples"},{"location":"dev_eval_function_docs/isSimilar/","text":"IsSimilar \u00b6 This simple evaluation function checks if the supplied response is within a tolerance range defined in params . Works exactly like the numpy.isclose function. Valid params include atol and rtol , which can be used in combination, or alone. As the comparison made is the following: is_correct = abs ( res - ans ) <= ( atol + rtol * abs ( ans )) Inputs \u00b6 { \"response\" : \"<number>\" , \"answer\" : \"<number>\" , \"params\" : { \"atol\" : \"<number>\" , \"rtol\" : \"<number>\" } } atol \u00b6 Absolute tolerance parameter rtol \u00b6 Relative tolerance parameter Outputs \u00b6 { \"is_correct\" : \"<bool>\" , \"real_diff\" : \"<number>\" , \"allowed_diff\" : \"<number>\" , } real_diff \u00b6 Real difference between the given answer and response allowed_diff \u00b6 Allowed difference between answer and response, calculated using the supplied atol and rtol parameters Examples \u00b6","title":"isSimilar"},{"location":"dev_eval_function_docs/isSimilar/#issimilar","text":"This simple evaluation function checks if the supplied response is within a tolerance range defined in params . Works exactly like the numpy.isclose function. Valid params include atol and rtol , which can be used in combination, or alone. As the comparison made is the following: is_correct = abs ( res - ans ) <= ( atol + rtol * abs ( ans ))","title":"IsSimilar"},{"location":"dev_eval_function_docs/isSimilar/#inputs","text":"{ \"response\" : \"<number>\" , \"answer\" : \"<number>\" , \"params\" : { \"atol\" : \"<number>\" , \"rtol\" : \"<number>\" } }","title":"Inputs"},{"location":"dev_eval_function_docs/isSimilar/#atol","text":"Absolute tolerance parameter","title":"atol"},{"location":"dev_eval_function_docs/isSimilar/#rtol","text":"Relative tolerance parameter","title":"rtol"},{"location":"dev_eval_function_docs/isSimilar/#outputs","text":"{ \"is_correct\" : \"<bool>\" , \"real_diff\" : \"<number>\" , \"allowed_diff\" : \"<number>\" , }","title":"Outputs"},{"location":"dev_eval_function_docs/isSimilar/#real_diff","text":"Real difference between the given answer and response","title":"real_diff"},{"location":"dev_eval_function_docs/isSimilar/#allowed_diff","text":"Allowed difference between answer and response, calculated using the supplied atol and rtol parameters","title":"allowed_diff"},{"location":"dev_eval_function_docs/isSimilar/#examples","text":"","title":"Examples"},{"location":"dev_eval_function_docs/arrayEqual/","text":"ArrayEqual \u00b6 Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently. Inputs \u00b6 Valid params include atol and rtol , which can be used in combination, or alone. (just like the IsSimilar grading function) { \"response\" : \"<array>\" , \"answer\" : \"<array>\" , \"params\" : { \"atol\" : \"<number>\" , \"rtol\" : \"<number>\" } } Note: response and answer arrays are parsed using np.array(dtype=np.float32) , any errors this causes are returned and the comparison fails. atol \u00b6 Absolute tolerance parameter rtol \u00b6 Relative tolerance parameter Outputs \u00b6 Examples \u00b6","title":"arrayEqual"},{"location":"dev_eval_function_docs/arrayEqual/#arrayequal","text":"Evaluation function checks if the supplied response and answer arrays are within the optionally supplied tolerances. This is based on the numpy.allclose function. Numpy is a dependancy for this function, but it means that arrays of any shape (regular) can be compared efficiently.","title":"ArrayEqual"},{"location":"dev_eval_function_docs/arrayEqual/#inputs","text":"Valid params include atol and rtol , which can be used in combination, or alone. (just like the IsSimilar grading function) { \"response\" : \"<array>\" , \"answer\" : \"<array>\" , \"params\" : { \"atol\" : \"<number>\" , \"rtol\" : \"<number>\" } } Note: response and answer arrays are parsed using np.array(dtype=np.float32) , any errors this causes are returned and the comparison fails.","title":"Inputs"},{"location":"dev_eval_function_docs/arrayEqual/#atol","text":"Absolute tolerance parameter","title":"atol"},{"location":"dev_eval_function_docs/arrayEqual/#rtol","text":"Relative tolerance parameter","title":"rtol"},{"location":"dev_eval_function_docs/arrayEqual/#outputs","text":"","title":"Outputs"},{"location":"dev_eval_function_docs/arrayEqual/#examples","text":"","title":"Examples"},{"location":"dev_eval_function_docs/arraySymbolicEqual/","text":"ArraySymbolicEqual \u00b6 This evaluation function can take any level of nesting for \"response\" and \"answer\" fields, as comparison is done recursively (as long as both shapes are identical). Symbolic grading is done using the SymbolicEqual function, called using the experimental EvaluationFunctionClient from the evaluation-function-utils library. Inputs \u00b6 This compares cells using the symbolicEqual function. Please consult that function's documentation for details on it's allowable parameters, as the ones provided to this function are fed through as they are. { \"response\" : \"<array (of arrays) of strings>\" , \"answer\" : \"<array (of arrays) of strings>\" , \"params\" : { \"Any params accepted by symbolicEqual\" } } Note: response and answer arrays should ultimately have string elements, even though they can have any level of nesting. Outputs \u00b6 Outputs to the grade command look like the following: { \"command\" : \"eval\" , \"result\" : { \"is_correct\" : \"<bool>\" , \"detailed_feedback\" : [ { \"is_correct\" : \"<bool>\" , \"level\" : \"<sympy correctness level>\" }, { \"...\" } ] } } Note : The detailed_feedback result field is of the same shape as the answer, giving specific information for the correctness of each cell in the evaluated array Examples \u00b6 Simple Arrays \u00b6 Correct behaviour Input { \"response\" : [ \"a\" , \"b + c\" ], \"answer\" : [ \"a\" , \"c + b\" ] } Output { \"command\" : \"eval\" , \"result\" : { \"is_correct\" : true , \"detailed_feedback\" : [ { \"is_correct\" : true , \"level\" : \"1\" }, { \"is_correct\" : true , \"level\" : \"1\" } ] } } Incorrect behaviour Input { \"response\" : [ \"a\" , \"b + 2*c\" ], \"answer\" : [ \"a\" , \"c + b\" ] } Output { \"command\" : \"eval\" , \"result\" : { \"is_correct\" : false , \"detailed_feedback\" : [ { \"is_correct\" : true , \"level\" : \"1\" }, { \"is_correct\" : false } ] } }","title":"arraySymbolicEqual"},{"location":"dev_eval_function_docs/arraySymbolicEqual/#arraysymbolicequal","text":"This evaluation function can take any level of nesting for \"response\" and \"answer\" fields, as comparison is done recursively (as long as both shapes are identical). Symbolic grading is done using the SymbolicEqual function, called using the experimental EvaluationFunctionClient from the evaluation-function-utils library.","title":"ArraySymbolicEqual"},{"location":"dev_eval_function_docs/arraySymbolicEqual/#inputs","text":"This compares cells using the symbolicEqual function. Please consult that function's documentation for details on it's allowable parameters, as the ones provided to this function are fed through as they are. { \"response\" : \"<array (of arrays) of strings>\" , \"answer\" : \"<array (of arrays) of strings>\" , \"params\" : { \"Any params accepted by symbolicEqual\" } } Note: response and answer arrays should ultimately have string elements, even though they can have any level of nesting.","title":"Inputs"},{"location":"dev_eval_function_docs/arraySymbolicEqual/#outputs","text":"Outputs to the grade command look like the following: { \"command\" : \"eval\" , \"result\" : { \"is_correct\" : \"<bool>\" , \"detailed_feedback\" : [ { \"is_correct\" : \"<bool>\" , \"level\" : \"<sympy correctness level>\" }, { \"...\" } ] } } Note : The detailed_feedback result field is of the same shape as the answer, giving specific information for the correctness of each cell in the evaluated array","title":"Outputs"},{"location":"dev_eval_function_docs/arraySymbolicEqual/#examples","text":"","title":"Examples"},{"location":"dev_eval_function_docs/arraySymbolicEqual/#simple-arrays","text":"Correct behaviour Input { \"response\" : [ \"a\" , \"b + c\" ], \"answer\" : [ \"a\" , \"c + b\" ] } Output { \"command\" : \"eval\" , \"result\" : { \"is_correct\" : true , \"detailed_feedback\" : [ { \"is_correct\" : true , \"level\" : \"1\" }, { \"is_correct\" : true , \"level\" : \"1\" } ] } } Incorrect behaviour Input { \"response\" : [ \"a\" , \"b + 2*c\" ], \"answer\" : [ \"a\" , \"c + b\" ] } Output { \"command\" : \"eval\" , \"result\" : { \"is_correct\" : false , \"detailed_feedback\" : [ { \"is_correct\" : true , \"level\" : \"1\" }, { \"is_correct\" : false } ] } }","title":"Simple Arrays"},{"location":"dev_eval_function_docs/shortTextAnswer/","text":"ShortTextAnswer \u00b6 This function evaluates the similarity value between two short texts, as well as identifying certain key strings in a student's answer. Inputs \u00b6 keystrings - Optional parameter. Represents a list of keystring objects which the function will search for in the answer. keystring object \u00b6 The keystring object contains several fields which affect how it will be interpreted: string - Required. The actual keystring being searched for. exact_match - Optional. A boolean value indicating whether to search for the exact string or for a semantically similar one. Defaults to false should_contain - Optional. A boolean value indicating whether it is expected for the keystring to be found in the answer or not. Defaults to true . Setting this flag to false indicates that a correct response will not contain the specified keystring. custom_feedback - Optional. A feedback string to be returned if the string was not found (or if it was, in case should_contain was set to false ). Defaults to None , in which case a generic response will be generated containing the string searched for. Outputs \u00b6 The function will return an object with 3 fields of interest. the is_correct and feedback fields are required by LambdaFeedback to present feedback to the user. The result field is only used for development. { \"is_correct\" : \"<bool>\" , \"result\" : { \"response\" : \"<string>\" , \"processing_time\" : \"<double>\" , }, \"feedback\" : \"string\" } response - The student answer. USed for debugging purposes. processing_time - The time it took for the function to evaluate If the function identified a problematic keystring, the result object will have an additional field: * keystring-scores - list(string, double). List of the provided keystrings and their best similarity scores that were found in the answer. Otherwise, it will have the additional fields: * method - string. Either \"w2v\" or \"BOW vector similarity\". * similarity_value - double. The similarity value between the response and the answer. If the method is w2v, it means the two texts were found to be similar. Otherwise, a BOW vector similarity check is performed in order to identify the most likely word that caused the texts to be found dissimilar. Examples \u00b6 List of example inputs and outputs for this function, each under a different sub-heading Example simple input, no keystring \u00b6 Input { \"response\" : \"Density, velocity, viscosity, length\" , \"answer\" : \"Density, speed, Viscosity, Length\" , } Output { 'is_correct' : True , 'result' : { 'response' : 'Density, speed, Viscosity, Length' , 'processing_time' : 0.022912219000000178 , 'method' : 'w2v' , 'similarity_value' : 0.9326027035713196 }, 'feedback' : 'Confidence: 0.933%' } Example keystring input \u00b6 Input { \"response\" : \"Molecules are made out of atoms\" , \"answer\" : \"Many atoms form a molecule\" , 'keystrings' : [ { 'string' : 'molecule' }, { 'string' : 'proton' , 'exact_match' : True } ] } Output { 'is_correct' : False , 'result' : { 'response' : 'Molecules are made out of atoms' , 'processing_time' : 0.30640586500000033 , 'keystring-scores' : [ ( 'molecule' , 0.990715997949492 ), ( 'proton' , 0.9186190596675989 ) # Searched for with exact match, therefore not a match. ] }, 'feedback' : \"Cannot determine if the answer is correct. Please provide more information about 'proton'\" }","title":"shortTextAnswer"},{"location":"dev_eval_function_docs/shortTextAnswer/#shorttextanswer","text":"This function evaluates the similarity value between two short texts, as well as identifying certain key strings in a student's answer.","title":"ShortTextAnswer"},{"location":"dev_eval_function_docs/shortTextAnswer/#inputs","text":"keystrings - Optional parameter. Represents a list of keystring objects which the function will search for in the answer.","title":"Inputs"},{"location":"dev_eval_function_docs/shortTextAnswer/#keystring-object","text":"The keystring object contains several fields which affect how it will be interpreted: string - Required. The actual keystring being searched for. exact_match - Optional. A boolean value indicating whether to search for the exact string or for a semantically similar one. Defaults to false should_contain - Optional. A boolean value indicating whether it is expected for the keystring to be found in the answer or not. Defaults to true . Setting this flag to false indicates that a correct response will not contain the specified keystring. custom_feedback - Optional. A feedback string to be returned if the string was not found (or if it was, in case should_contain was set to false ). Defaults to None , in which case a generic response will be generated containing the string searched for.","title":"keystring object"},{"location":"dev_eval_function_docs/shortTextAnswer/#outputs","text":"The function will return an object with 3 fields of interest. the is_correct and feedback fields are required by LambdaFeedback to present feedback to the user. The result field is only used for development. { \"is_correct\" : \"<bool>\" , \"result\" : { \"response\" : \"<string>\" , \"processing_time\" : \"<double>\" , }, \"feedback\" : \"string\" } response - The student answer. USed for debugging purposes. processing_time - The time it took for the function to evaluate If the function identified a problematic keystring, the result object will have an additional field: * keystring-scores - list(string, double). List of the provided keystrings and their best similarity scores that were found in the answer. Otherwise, it will have the additional fields: * method - string. Either \"w2v\" or \"BOW vector similarity\". * similarity_value - double. The similarity value between the response and the answer. If the method is w2v, it means the two texts were found to be similar. Otherwise, a BOW vector similarity check is performed in order to identify the most likely word that caused the texts to be found dissimilar.","title":"Outputs"},{"location":"dev_eval_function_docs/shortTextAnswer/#examples","text":"List of example inputs and outputs for this function, each under a different sub-heading","title":"Examples"},{"location":"dev_eval_function_docs/shortTextAnswer/#example-simple-input-no-keystring","text":"Input { \"response\" : \"Density, velocity, viscosity, length\" , \"answer\" : \"Density, speed, Viscosity, Length\" , } Output { 'is_correct' : True , 'result' : { 'response' : 'Density, speed, Viscosity, Length' , 'processing_time' : 0.022912219000000178 , 'method' : 'w2v' , 'similarity_value' : 0.9326027035713196 }, 'feedback' : 'Confidence: 0.933%' }","title":"Example simple input, no keystring"},{"location":"dev_eval_function_docs/shortTextAnswer/#example-keystring-input","text":"Input { \"response\" : \"Molecules are made out of atoms\" , \"answer\" : \"Many atoms form a molecule\" , 'keystrings' : [ { 'string' : 'molecule' }, { 'string' : 'proton' , 'exact_match' : True } ] } Output { 'is_correct' : False , 'result' : { 'response' : 'Molecules are made out of atoms' , 'processing_time' : 0.30640586500000033 , 'keystring-scores' : [ ( 'molecule' , 0.990715997949492 ), ( 'proton' , 0.9186190596675989 ) # Searched for with exact match, therefore not a match. ] }, 'feedback' : \"Cannot determine if the answer is correct. Please provide more information about 'proton'\" }","title":"Example keystring input"},{"location":"dev_eval_function_docs/chatGPT%20%28experimental%29/","text":"chatGPT Evaluation Function \u00b6 Overview \u00b6 This chatGPT evaluation function is designed to automatically evaluate student responses to questions. It currently uses the openAI API to determine the correctness (true/false) of the student's answer and can also provide them with feedback. Setup \u00b6 To successfully run this function, ensure you set your OpenAI API key. The code fetches this key from environment variables, so ensure it's set up in your environment or .env file. Inputs \u00b6 Parameters dictionary: \u00b6 model : Deinfes the AI model used for evaluation. Currently, \"gpt-3.5-turbo\" is the only model available. main_prompt : Description : This prompt provides context to the AI, detailing the nature of the question and the expected answer(s). default_prompt : Description : A standardised instruction directing the AI to output a boolean correctness of the stident's answer. feedback_prompt : This prompt guides the AI on how feedback should be given. If left blank, only a binary correctness assessment is returned without detailed feedback. Note that an input of a variable called answer is also required. This can be any value. This is to ensure compatibility with LambdaFeedback. Example Input: \u00b6 parameters = { 'model' : 'gpt-3.5-turbo' , 'main_prompt' : \"Evaluate the student's response regarding the definition of photosynthesis\" , 'default_prompt' : \"Output a Boolean: True if the student is correct and False if they are incorrect.\" , 'feedback_prompt' : \"You are an AI tutor. Provide feedback based on the student's answer.\" } response = \"Photosynthesis is the process by which plants convert light energy into chemical energy to fuel their growth.\" Outputs \u00b6 The function will yield a dictionary with the following structure: { 'is_correct' : bool , 'feedback' : string ( Optional ) } Usage Examples \u00b6 Capital City Assessment \u00b6 parameters = { 'model' : 'gpt-3.5-turbo' , 'main_prompt' : \"Analyze the student's response about the capital of France.\" , 'default_prompt' : \"Output a Boolean: True if the student is correct and False if they are incorrect.\" , 'feedback_prompt' : \"You are an AI tutor. Offer constructive feedback.\" } response = \"The capital of France is Berlin.\" output = evaluation_function ( response , answer , parameters ) Expected Output: { 'is_correct' : False , 'feedback' : \"The actual capital of France is Paris. Please revisit your geography notes.\" }","title":"chatGPT (experimental)"},{"location":"dev_eval_function_docs/chatGPT%20%28experimental%29/#chatgpt-evaluation-function","text":"","title":"chatGPT Evaluation Function"},{"location":"dev_eval_function_docs/chatGPT%20%28experimental%29/#overview","text":"This chatGPT evaluation function is designed to automatically evaluate student responses to questions. It currently uses the openAI API to determine the correctness (true/false) of the student's answer and can also provide them with feedback.","title":"Overview"},{"location":"dev_eval_function_docs/chatGPT%20%28experimental%29/#setup","text":"To successfully run this function, ensure you set your OpenAI API key. The code fetches this key from environment variables, so ensure it's set up in your environment or .env file.","title":"Setup"},{"location":"dev_eval_function_docs/chatGPT%20%28experimental%29/#inputs","text":"","title":"Inputs"},{"location":"dev_eval_function_docs/chatGPT%20%28experimental%29/#parameters-dictionary","text":"model : Deinfes the AI model used for evaluation. Currently, \"gpt-3.5-turbo\" is the only model available. main_prompt : Description : This prompt provides context to the AI, detailing the nature of the question and the expected answer(s). default_prompt : Description : A standardised instruction directing the AI to output a boolean correctness of the stident's answer. feedback_prompt : This prompt guides the AI on how feedback should be given. If left blank, only a binary correctness assessment is returned without detailed feedback. Note that an input of a variable called answer is also required. This can be any value. This is to ensure compatibility with LambdaFeedback.","title":"Parameters dictionary:"},{"location":"dev_eval_function_docs/chatGPT%20%28experimental%29/#example-input","text":"parameters = { 'model' : 'gpt-3.5-turbo' , 'main_prompt' : \"Evaluate the student's response regarding the definition of photosynthesis\" , 'default_prompt' : \"Output a Boolean: True if the student is correct and False if they are incorrect.\" , 'feedback_prompt' : \"You are an AI tutor. Provide feedback based on the student's answer.\" } response = \"Photosynthesis is the process by which plants convert light energy into chemical energy to fuel their growth.\"","title":"Example Input:"},{"location":"dev_eval_function_docs/chatGPT%20%28experimental%29/#outputs","text":"The function will yield a dictionary with the following structure: { 'is_correct' : bool , 'feedback' : string ( Optional ) }","title":"Outputs"},{"location":"dev_eval_function_docs/chatGPT%20%28experimental%29/#usage-examples","text":"","title":"Usage Examples"},{"location":"dev_eval_function_docs/chatGPT%20%28experimental%29/#capital-city-assessment","text":"parameters = { 'model' : 'gpt-3.5-turbo' , 'main_prompt' : \"Analyze the student's response about the capital of France.\" , 'default_prompt' : \"Output a Boolean: True if the student is correct and False if they are incorrect.\" , 'feedback_prompt' : \"You are an AI tutor. Offer constructive feedback.\" } response = \"The capital of France is Berlin.\" output = evaluation_function ( response , answer , parameters ) Expected Output: { 'is_correct' : False , 'feedback' : \"The actual capital of France is Paris. Please revisit your geography notes.\" }","title":"Capital City Assessment"},{"location":"dev_eval_function_docs/riskAssessment/","text":"riskAssessment \u00b6 This evaluation function is used to provide feedback to students on their risk assessment inputs. The response input for the evaluation function is 12 risk assessment inputs, 6 of which are text inputs (activity, hazard, how it harms, who it harms, prevention and mitigation) and 6 of which are number inputs used to fill out a risk matrix (uncontrolled likelihood, uncontrolled severity, uncontrolled risk, controlled likelihood, controlled severity, controlled risk). The evaluation function checks that the risk matrix number inputs follow the convention (i.e. risk = likelihood x severity, & uncontrolled risk <= controlled risk). If the risk matrix inputs are correct, then the above prompt structure is followed to provide feedback on the text inputs. Textboxes with a blue border are prompts to an LLM while those with a purple border contain the logic shown below. The logic above starts by checking for blank input fields; if there are no blank input fields, a prompt to the LLM is used to determine whether the \u201chow it harms\u201d risk assessment input is correct given the context of other relevant inputs. If it is correct, a similar prompt is run for the \u201cwho it harms\u201d input. If either of these prompts suggest that the corresponding input is incorrect, then appropriate feedback is given and the prompts that follow are not run. The inner logic of the purple \u201ccontrol measure logic\u201d is shown above. The control measure logic is used to provide feedback on a control measure input; the same logic is used to provide feedback on the prevention and mitigation inputs. Since it is possible that a prevention or mitigation does not exist for a particular hazard, it is only mandatory that either one of the prevention or mitigation fields is filled; as such, the initial logic checks whether the control measure input is either blank or whether no information is entered, e.g. \u201cN.A.\u201d. If information is provided, a prompt is used to extract the \u201cevent that leads to harm\u201d (hazard event) and the \u201charm caused\u201d from the risk assessment inputs. The inferences of the \u201chazard event\u201d and \u201charm caused\u201d are then used by the control measure classification prompt which classifies the control measure as either \u201cprevention\u201d, \u201cmitigation\u201d, \u201cneither\u201d or \u201cboth\u201d; if the control measure reduces the likelihood of the \u201chazard event\u201d then it is a \u201cprevention\u201d and, assuming the \u201chazard event\u201d has occurred, if the control measures reduces the severity of this event, then it is a \u201cmitigation\u201d. Setup \u00b6 Clone the github repository. Create an API key for Open AI, Mistral and Anthropic Store the API key strings in a .env file that is created in the app folder. The variable names should be the same as those from the .env.example file. Running tests \u00b6 To test the different prompts to LLMs in the prompt structure diagrams shown above, a series of scripts have been created in the app/test_scripts folder. To run these tests, copy and paste the commented out command from the top of the file into the terminal. (e.g. python -m app.test_scripts.risk_domain_test_for_how_it_harms_prompt for the app/test_scripts/risk_domain_test_for_how_it_harms_prompt file). Make sure the current working directory in the terminal is the one above the app folder. Prompt Results \u00b6 A spreadsheet showing the results from testing different prompts can be found here . To see which test script matches up to which prompt results tab, press ctrl + shift + f and then search for the name of the prompt results tab. Inputs \u00b6 The input parameters which should be specified in the Lambda Feedback \"Teacher\" mode are: - is_risk_matrix (bool) - is_feedback_text (bool) - is_risk_assessment (bool) - LLM (string) Note: only 1 of the boolean parameters should be True: - If is_risk_matrix is True, then the input should be a 2x3 array of integers (a risk matrix) and feedback is provided on the risk matrix. This is used in one of the questions from part 1 of the risk assessment exercise. - If is_feedback_text is True, then no matter the input, the feedback is \"Thank you for your feedback\". This is used in the student feedback section of the exercise. - If is_risk_assessment is True, the input should be a 12x1 array of risk assessment inputs in the following order: activity, hazard, how it harms, who it harms, uncontrolled likelihood, uncontrolled severity, uncontrolled risk, prevention, mitigation, controlled likelihood, controlled severity, controlled risk. The logic described above is then used to provide feedback on the risk assessment inputs. The LLM parameter allows the teacher to choose which Large Language Model (LLM) they would like to use. The options are currently: - \"Claude Opus\" - \"Claude Sonnet\" - \"Claude Haiku\" - \"GPT-4 Turbo\" - \"GPT-3.5 Turbo 1106\" - \"Mistral Large\" - \"Mixtral 8x22B\" - \"Mixtral 8x7B\" Outputs \u00b6 Example risk assessment inputs Coresponding risk assessment outputs","title":"riskAssessment"},{"location":"dev_eval_function_docs/riskAssessment/#riskassessment","text":"This evaluation function is used to provide feedback to students on their risk assessment inputs. The response input for the evaluation function is 12 risk assessment inputs, 6 of which are text inputs (activity, hazard, how it harms, who it harms, prevention and mitigation) and 6 of which are number inputs used to fill out a risk matrix (uncontrolled likelihood, uncontrolled severity, uncontrolled risk, controlled likelihood, controlled severity, controlled risk). The evaluation function checks that the risk matrix number inputs follow the convention (i.e. risk = likelihood x severity, & uncontrolled risk <= controlled risk). If the risk matrix inputs are correct, then the above prompt structure is followed to provide feedback on the text inputs. Textboxes with a blue border are prompts to an LLM while those with a purple border contain the logic shown below. The logic above starts by checking for blank input fields; if there are no blank input fields, a prompt to the LLM is used to determine whether the \u201chow it harms\u201d risk assessment input is correct given the context of other relevant inputs. If it is correct, a similar prompt is run for the \u201cwho it harms\u201d input. If either of these prompts suggest that the corresponding input is incorrect, then appropriate feedback is given and the prompts that follow are not run. The inner logic of the purple \u201ccontrol measure logic\u201d is shown above. The control measure logic is used to provide feedback on a control measure input; the same logic is used to provide feedback on the prevention and mitigation inputs. Since it is possible that a prevention or mitigation does not exist for a particular hazard, it is only mandatory that either one of the prevention or mitigation fields is filled; as such, the initial logic checks whether the control measure input is either blank or whether no information is entered, e.g. \u201cN.A.\u201d. If information is provided, a prompt is used to extract the \u201cevent that leads to harm\u201d (hazard event) and the \u201charm caused\u201d from the risk assessment inputs. The inferences of the \u201chazard event\u201d and \u201charm caused\u201d are then used by the control measure classification prompt which classifies the control measure as either \u201cprevention\u201d, \u201cmitigation\u201d, \u201cneither\u201d or \u201cboth\u201d; if the control measure reduces the likelihood of the \u201chazard event\u201d then it is a \u201cprevention\u201d and, assuming the \u201chazard event\u201d has occurred, if the control measures reduces the severity of this event, then it is a \u201cmitigation\u201d.","title":"riskAssessment"},{"location":"dev_eval_function_docs/riskAssessment/#setup","text":"Clone the github repository. Create an API key for Open AI, Mistral and Anthropic Store the API key strings in a .env file that is created in the app folder. The variable names should be the same as those from the .env.example file.","title":"Setup"},{"location":"dev_eval_function_docs/riskAssessment/#running-tests","text":"To test the different prompts to LLMs in the prompt structure diagrams shown above, a series of scripts have been created in the app/test_scripts folder. To run these tests, copy and paste the commented out command from the top of the file into the terminal. (e.g. python -m app.test_scripts.risk_domain_test_for_how_it_harms_prompt for the app/test_scripts/risk_domain_test_for_how_it_harms_prompt file). Make sure the current working directory in the terminal is the one above the app folder.","title":"Running tests"},{"location":"dev_eval_function_docs/riskAssessment/#prompt-results","text":"A spreadsheet showing the results from testing different prompts can be found here . To see which test script matches up to which prompt results tab, press ctrl + shift + f and then search for the name of the prompt results tab.","title":"Prompt Results"},{"location":"dev_eval_function_docs/riskAssessment/#inputs","text":"The input parameters which should be specified in the Lambda Feedback \"Teacher\" mode are: - is_risk_matrix (bool) - is_feedback_text (bool) - is_risk_assessment (bool) - LLM (string) Note: only 1 of the boolean parameters should be True: - If is_risk_matrix is True, then the input should be a 2x3 array of integers (a risk matrix) and feedback is provided on the risk matrix. This is used in one of the questions from part 1 of the risk assessment exercise. - If is_feedback_text is True, then no matter the input, the feedback is \"Thank you for your feedback\". This is used in the student feedback section of the exercise. - If is_risk_assessment is True, the input should be a 12x1 array of risk assessment inputs in the following order: activity, hazard, how it harms, who it harms, uncontrolled likelihood, uncontrolled severity, uncontrolled risk, prevention, mitigation, controlled likelihood, controlled severity, controlled risk. The logic described above is then used to provide feedback on the risk assessment inputs. The LLM parameter allows the teacher to choose which Large Language Model (LLM) they would like to use. The options are currently: - \"Claude Opus\" - \"Claude Sonnet\" - \"Claude Haiku\" - \"GPT-4 Turbo\" - \"GPT-3.5 Turbo 1106\" - \"Mistral Large\" - \"Mixtral 8x22B\" - \"Mixtral 8x7B\"","title":"Inputs"},{"location":"dev_eval_function_docs/riskAssessment/#outputs","text":"Example risk assessment inputs Coresponding risk assessment outputs","title":"Outputs"},{"location":"dev_eval_function_docs/compareExpressions/","text":"YourFunctionName \u00b6 Brief description of what this evaluation function does, from the developer perspective Inputs \u00b6 Specific input parameters which can be supplied when the eval command is supplied to this function. Outputs \u00b6 Output schema/values for this function Examples \u00b6 List of example inputs and outputs for this function, each under a different sub-heading Simple Evaluation \u00b6 { \"example\" : { \"Something\" : \"something\" } } { \"example\" : { \"Something\" : \"something\" } }","title":"compareExpressions"},{"location":"dev_eval_function_docs/compareExpressions/#yourfunctionname","text":"Brief description of what this evaluation function does, from the developer perspective","title":"YourFunctionName"},{"location":"dev_eval_function_docs/compareExpressions/#inputs","text":"Specific input parameters which can be supplied when the eval command is supplied to this function.","title":"Inputs"},{"location":"dev_eval_function_docs/compareExpressions/#outputs","text":"Output schema/values for this function","title":"Outputs"},{"location":"dev_eval_function_docs/compareExpressions/#examples","text":"List of example inputs and outputs for this function, each under a different sub-heading","title":"Examples"},{"location":"dev_eval_function_docs/compareExpressions/#simple-evaluation","text":"{ \"example\" : { \"Something\" : \"something\" } } { \"example\" : { \"Something\" : \"something\" } }","title":"Simple Evaluation"},{"location":"user_eval_function_docs/isExactEqual/","text":"Supported Response Area Types This evaluation function is supported by the following Response Area components: NUMBER TEXT BOOLEAN IsExactEqual \u00b6 Use this function to check the exact equality between the student response and answer. This function requires a 'type' parameter which specifies how each of the two inputs should be cast before direct comparison in python.","title":"isExactEqual"},{"location":"user_eval_function_docs/isExactEqual/#isexactequal","text":"Use this function to check the exact equality between the student response and answer. This function requires a 'type' parameter which specifies how each of the two inputs should be cast before direct comparison in python.","title":"IsExactEqual"},{"location":"user_eval_function_docs/comparePhysicalQuantities/","text":"Supported Response Area Types This evaluation function is supported by the following Response Area components: NUMERIC_UNITS TEXT EXPRESSION ComparePhysicalQuantities \u00b6 Evaluation function which proveds some basic some dimensional analysis functionality. DEPRECATED Comparing physical quantities RECOMMENDED ALTERNATIVE: CompareExpressions with the physical_quantity parameter set to true Substitutions of symbols before comparison of expressions is done Checking if a comma separated list of expressions can be interpreted as a set groups that satisfies the Buckingham Pi theorem Note: When the quantities grading parameter is set, this function cannot handle short form symbols for units. Thus when defining quantities all units must be given with full names in lower-case letters. For example Nm/s or Newton*metre/SECOND will not be handled correctly, but newton*metre/second will. Note: Prefixes have lower precedence than exponentiation, e.g. 10*cm**2 will be interpreted as \\(10 \\cdot 10^{-2}~\\mathrm{metre}^2\\) rather than \\(10 \\cdot (10^{-2}~\\mathrm{metre})^2\\) . Note: This function allows omitting * and using ^ instead of ** if the grading parameter strict_syntax is set to false. In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols. Note: Only the short forms listed in the tables below are accepted. Not all units that are supported have short forms (since this leads to ambiguities). Note: When using the short forms the following convention is assumed: - Long form names takes precedence over sequences of short forms, e.g. e.g. mN will be interpreted as milli newton , Nm as newton metre , mmN as milli metre newton , mNm as milli newton metre and Nmm as newton milli metre . - Short form symbols of prefixes will take precedence over short form symbols of units from the left, e.g. mug will be interpreted as micro*gram instead metre*astronomicalunit*gram . - If there is a short form symbol for a prefix that collides with the short form for a unit (i.e. m ) then it is assumed the that unit will always be placed to the right of another unit in compound units, e.g. mN will be interpreted as milli newton , Nm as newton metre , mmN as milli metre newton , mNm as milli newton metre and Nmm as newton milli metre . - Longer short form symbols take precedence over shorter short forms, e.g. sr will be interpreted as steradian instead of second radian . Note: setting elementary_functions to true will disable using short forms symbols for units. Inputs \u00b6 All input parameters need to be supplied via the Grading parameters panel. There are seven optional parameters that can be set: elementary_functions , substitutions , quantities , strict_syntax , rtol , atol and comparison . custom_feedback \u00b6 Custom feedback can be set on a per-task basis. Note: Custom feedback only supports fixed strings, this means that for some situations the custom feedback cannot be as detailed as the default feedback. The parameter must be set as a dictionary with keys from the feedback tags listed below. The value for each key can be any string. Feedback tags for all comparisons \u00b6 PARSE_ERROR_WARNING Response cannot be parsed as an expression or physical quantity. PER_FOR_DIVISION Warns about risk of ambiguity when using per instead / for division. STRICT_SYNTAX_EXPONENTIATION Warns that ^ cannot be used for exponentiation when strict_syntax is set to true . QUANTITIES_NOT_WRITTEN_CORRECTLY Text in error message that appears if list of quantities could not be parsed. SUBSTITUTIONS_NOT_WRITTEN_CORRECTLY Text in error message that appears if list of substitutions could not be parsed. Feedback tags for buckinghamPi comparison \u00b6 VALID_CANDIDATE_SET Message that is displayed when a response is found to be a valid set of groups. Note: setting this will not affect the Correct/Incorrect message, it will only add further text. NOT_DIMENSIONLESS Message displayed when at least one groups is not dimensionless. MORE_GROUPS_THAN_REFERENCE_SET Message displayed when the response contains more groups than necessary. CANDIDATE_GROUPS_NOT_INDEPENDENT Message displayed when the groups in the response are not independent. TOO_FEW_INDEPENDENT_GROUPS Message displayed when the response contains fewer groups than necessary. UNKNOWN_SYMBOL Message displayed when the response contains some undefined symbol. SUM_WITH_INDEPENDENT_TERMS Message displayed when the response has too few groups but one (or more) of the groups is a sum with independent terms. elementary_functions \u00b6 When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting elementary_functions to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview. sin , sinc , csc (alternative cosec ), cos , sec , tan , cot (alternative cotan ), asin (alternative arcsin ), acsc (alternatives arccsc , arccosec ), acos (alternative arccos ), asec (alternative arcsec ), atan (alternative arctan ), acot (alternatives arccot , arccotan ), atan2 (alternative arctan2 ), sinh , cosh , tanh , csch (alternative cosech ), sech , asinh (alternative arcsinh ), acosh (alternative arccosh ), atanh (alternative arctanh ), acsch (alternatives arccsch , arcosech ), asech (alternative arcsech ), exp (alternative Exp ), E (equivalent to exp(1) , alternative e ), log , sqrt , sign , Abs (alternative abs ), Max (alternative max ), Min (alternative min ), arg , ceiling (alternative ceil ), floor Note: setting elementary_functions to true will disable using short forms symbols for units. substitutions \u00b6 String that lists all substitutions that should be done to the answer and response inputs before processing. Each substitution should be written in the form ('original string','substitution string') and all pairs concatenated into a single string. Substitutions can be grouped by adding | between two substitutions. Then all substitutions before | will be performed before the substitutions after | . The input can contain an arbitrary number of substitutions and | symbols. Note that using substitutions will replace all default definitions of quantities and dimensions. quantities \u00b6 String that lists all quantities that can be used in the answer and response. Each quantity should be written in the form ('quantity name','(units)') and all pairs concatenated into a single string. See tables below for available default units. Whenever units are used they must be written exactly as in the left columns of the tables given below (no short forms or single-character symbols) and units must be multiplied (or divided) by each other, as well as any accompanying quantities. NOTE: Using units and predefined quantities at the same time in an answer or response can cause problems (especially if quantities are denoted using single characters). Ideally it should be clear that either predefined quantities, or units should only be used from the question. If the comparison parameter is set to dimensions , it is not necessary to give exact units for each quantity, but the dimensions must be given instead. See tables below for available default dimensions. If the comparison parameter is set to buckinghamPi , then quantities should be set in a different way. See the detailed description of buckinghamPi further down. Table: Base SI units \u00b6 SI base units taken from Table 1 of https://physics.nist.gov/cuu/Units/units.html Note that gram is used as a base unit instead of kilogram. SI base unit Symbol Dimension name metre m length gram g mass second s time ampere A electriccurrent kelvin k temperature mole mol amountofsubstance candela cd luminousintensity Table: SI prefixes \u00b6 SI base units taken from Table 5 of https://physics.nist.gov/cuu/Units/prefixes.html SI Prefix Symbol Factor SI Prefix Symbol Factor yotta Y \\(10^{24}\\) deci d \\(10^{-1}\\) zetta Z \\(10^{21}\\) centi c \\(10^{-2}\\) exa' E \\(10^{18}\\) milli m \\(10^{-3}\\) peta P \\(10^{15}\\) micro mu \\(10^{-6}\\) tera T \\(10^{12}\\) nano n \\(10^{-9}\\) giga G \\(10^{9}\\) pico p \\(10^{-12}\\) mega M \\(10^{6}\\) femto f \\(10^{-15}\\) kilo k \\(10^{3}\\) atto a \\(10^{-18}\\) hecto h \\(10^{2}\\) zepto z \\(10^{-21}\\) deka da \\(10^{1}\\) yocto y \\(10^{-24}\\) Table: Derived SI units \u00b6 Derived SI units taken from Table 3 of https://physics.nist.gov/cuu/Units/units.html Note that degrees Celsius is omitted. Note that the function treats radians and steradians as dimensionless values. Unit name Symbol Expressed in base SI units radian r 1 steradian sr 1 hertz Hz \\(\\mathrm{second}^{-1}\\) newton N \\(\\mathrm{metre}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) pascal Pa \\(\\mathrm{metre}^{-1}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) joule J \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}\\) watt W \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-3}\\) coulomb C \\(\\mathrm{second~ampere}\\) volt V \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-1}\\) farad F \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^4~\\mathrm{ampere}^2\\) ohm O \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-2}\\) siemens S \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^3~\\mathrm{ampere}^2\\) weber Wb \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-1}\\) tesla T \\(\\mathrm{kilogram~second}^{-2} \\mathrm{ampere}^{-1}\\) henry H \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-2}\\) lumen lm \\(\\mathrm{candela}\\) lux lx \\(\\mathrm{metre}^{-2}~\\mathrm{candela}\\) becquerel Bq \\(\\mathrm{second}^{-1}\\) gray Gy \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) sievert Sv \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) katal kat \\(\\mathrm{mole~second}^{-1}\\) Table: Common non-SI units \u00b6 Commonly used non-SI units taken from Table 6 and 7 of https://physics.nist.gov/cuu/Units/outside.html Note that the function treats angles, neper and bel as dimensionless values. Note that only the first table in this section has short form symbols defined, the second table does not. Unit name Symbol Expressed in SI units minute min \\(60~\\mathrm{second}\\) hour h \\(3600~\\mathrm{second}\\) degree deg \\(\\frac{\\pi}{180}\\) liter l \\(10^{-3}~\\mathrm{metre}^3\\) metric_ton t \\(10^3~\\mathrm{kilogram}\\) neper Np \\(1\\) bel B \\(\\frac{1}{2}~\\ln(10)\\) electronvolt eV \\(1.60218 \\cdot 10^{-19}~\\mathrm{joule}\\) atomic_mass_unit u \\(1.66054 \\cdot 10^{-27}~\\mathrm{kilogram}\\) angstrom \u00e5 \\(10^{-10}~\\mathrm{metre}\\) Unit name Expressed in SI units day \\(86400~\\mathrm{second}\\) angleminute \\(\\frac{\\pi}{10800}\\) anglesecond \\(\\frac{\\pi}{648000}\\) astronomicalunit \\(149597870700~\\mathrm{metre}\\) nauticalmile \\(1852~\\mathrm{metre}\\) knot \\(\\frac{1852}{3600}~\\mathrm{metre~second}^{-1}\\) are \\(10^2~\\mathrm{metre}^2\\) hectare \\(10^4~\\mathrm{metre}^2\\) bar \\(10^5~\\mathrm{pascal}\\) barn \\(10^{-28}~\\mathrm{metre}\\) curie $3.7 \\cdot 10^{10}~\\mathrm{becquerel} roentgen \\(2.58 \\cdot 10^{-4}~\\mathrm{kelvin~(kilogram)}^{-1}\\) rad \\(10^{-2}~\\mathrm{gray}\\) rem \\(10^{-2}~\\mathrm{sievert}\\) Table: Imperial units \u00b6 Commonly imperial units taken from https://en.wikipedia.org/wiki/Imperial_units Unit name Symbol Expressed in SI units inch in \\(0.0254~\\mathrm{metre}\\) foot ft \\(0.3048~\\mathrm{metre}\\) yard yd \\(0.9144~\\mathrm{metre}\\) mile mi \\(1609.344~\\mathrm{metre}\\) fluid ounce fl oz \\(28.4130625~\\mathrm{millilitre}\\) gill gi \\(142.0653125~\\mathrm{millilitre}\\) pint pt \\(568.26125~\\mathrm{millilitre}\\) quart qt \\(1.1365225~\\mathrm{litre}\\) gallon gal \\(4546.09~\\mathrm{litre}\\) ounce oz \\(28.349523125~\\mathrm{gram}\\) pound lb \\(0.45359237~\\mathrm{kilogram}\\) stone st \\(6.35029318~\\mathrm{kilogram}\\) strict_syntax \u00b6 If strict_syntax is set to true then the answer and response must have * or / between each part of the expressions and exponentiation must be done using ** , e.g. 10*kilo*metre/second**2 is accepted but 10 kilometre/second^2 is not. If strict_syntax is set to false, then * can be omitted and ^ used instead of ** . In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols. By default strict_syntax is set to true. rtol \u00b6 Maximum relative error allowed when comparing expressions. atol \u00b6 Maximum absolute error allowed when comparing expressions. comparison \u00b6 Parameter that determines what kind of comparison is done. There are four possible options: expression Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close (as specified by the atol and rtol parameters). expressionExact Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is identical to as high precision as possible. dimensions Checks that the answer and response have the same dimensions, does not compare the values of the physical quantities. buckinghamPi Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem. For more details on each options see the description below and the corresponding examples. If comparison is not specified it defaults to expression . expression \u00b6 Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close. How big the difference is between the value of the answer and the value of the response is decided by the rtol and atol parameters. If neither atol nor rtol is specified the function will allow a relative error of \\(10^{-12}\\) . If atol is specified its value will be interpreted as the maximum allowed absolute error. If rtol is specified its value will be interpreted as the maximum allowed relative error. If both atol and rtol the function will check both the absolute and relative error. expressionExact \u00b6 Converts the expression to base SI units and checks that the answer and response are identical to the highest precision possible (note that some unit conversions are not exact and that using decimal numbers in the answer or response limits this to floating point precision). dimensions \u00b6 Checks that the answer and response have the same dimensions, but does not compare the values of the physical quantities. With this option the quantities (specified by the quantities parameter) can be given either dimension only, or units. buckinghamPi \u00b6 Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem. There are three different ways of supplying this function with the necessary information. - In the answer, provide an example set of groups as a comma seprated list. When used this way the function assumes that the given list is correct and contains at least the minimum number of groups. - In the quantities parameter, supply a list of what the dimensions for each quantity is and set answer to - . The function will then compute a list of sufficiently many independen dimensionless quantities and compare to the response. - In the quantities parameter, supply a list of what the dimensions for each quantity is and in the answer, supply a list of groups as in the first option. The function will then check that the supplied answer is dimensionless and has a sufficient number of independent groups before comparing it to the response. Note that in lists of groups the items should ideally be written on the form \\(q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities and \\(c_1, c_2 \\ldots c_n\\) are integers, but the function can also handle item that are sums with terms written on the form \\(a \\cdot q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities, \\(c_1, c_2 \\ldots c_n\\) rational numbers and \\(a\\) a constant. If the total number of groups is less than required the set of groups is considered invalid, even if there is a sufficient number of terms with independent power products in the response. Examples \u00b6 Implemented versions of these examples can be found in the module 'Examples: Evaluation Functions'. 1 Checking the dimensions of an expression or physical quantity \u00b6 DEPRECATED RECOMMENDED ALTERNATIVE: CompareExpressions with the physical_quantity parameter set to true This example will check if the response has dimensions \\(\\frac{\\mathrm{length}^2}{\\mathrm{time}^2}\\) . a) \u00b6 To check an expression there needs to be some predefined quantities that can be used in the expression. Since only dimensions will be checked units are not necessary (but could be used as well). Here a response area with input type TEXT and two grading parameters, quantities and comparison , will be used. quantities is defined as follows: ('d','(length)') ('t','(time)') ('v','(length/time)') comparison is set to dimensions . The answer is set two some expression with the right dimensions, e.g. v**2 . With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. In the example given in the example problem set, the following responses are tested and evaluated as correct: Strict syntax Relaxed syntax v**2 v^2 5*v**2 5v^2 (d/t)**2+v**2 (d/t)^2+v^2 d**2/t**2 d^2/t^2 d**2*t**(-2) d^2 t^(-2) d/t*v vd/t b) \u00b6 Checking the dimensions of a quantity directly, i.e. the dimensions of an expression of the form number*units , no predefined quantities are necessary. Here a response area with input type TEXT and one grading parameter, comparison , will be used. comparison is set to dimensions . The answer is set two some expression with the right dimensions, e.g. length**2/time**2 . With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since only default SI units are expected in the answer we do not need to set any input symbols. In the example given in the example problem set, the following responses are tested and evaluated as correct: Strict syntax Relaxed syntax Using symbols metre**2/second**2 metre^2/second^2 m^2/s^2 (centi*metre)**2/hour**2 (centimetre)^2/h^2 (cm)^2/h^2 246*ohm/(kilo*gram)*coulomb**2/second 246 ohm/(kilogram) coulomb^2/second 246 O/(kg) c^2/s 2 Checking the value of an expression or a physical quantity \u00b6 DEPRECATED RECOMMENDED ALTERNATIVE: CompareExpressions with the physical_quantity parameter set to true This examples checks if your expression is equal to \\(2~\\frac{\\mathrm{kilometre}}{\\mathrm{hour}}\\) . a) \u00b6 Here an expression with predefined quantities is checked as exactly as possible. This is done with a TEXT response area with the following parameters: quantities is set to: ('d','(length)') ('t','(time)') ('v','(length/time)') Note that short form symbols cannot be used when defining quantities. comparison is set to expressionExact . The response area answer is set to 2*v but there are many other expressions that would work just as well. Note that we cannot write 2*kilo*metre/second as response or answer since the predefined quantity t will substitute the t in metre which results in unparseable input. With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since only default SI units and single character symbols are expected in the answer we will not set the grading parameter symbols . In the example given in the example problem set, the following responses are tested and evaluated as correct: Strict syntax Relaxed syntax 2*v 2v 2000/3600*d/t 2000/3600 d/t 1/1.8*d/t d/(1.8t) v+1/3.6*d/t v+d/(3.6t) b) \u00b6 Checking if a quantity is equal to \\(2~\\frac{kilometre}{hour}\\) with a fixed absolute tolerance of \\(0.05 \\frac{metre}{second}\\) can be done with a TEXT response area with atol set to 0.05 and the answer set to 2*kilo*metre/hour . Note: atol is always assumed to be given in the base SI units version of the expression. This is likely to change in future versions of the function. The comparison parameter could also be set to expression but since this is the default it is not necessary. With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since only default SI units are expected in the answer no input symbols are necessary. In the example given in the example problem set, the following responses are tested and evaluated as correct: Strict syntax Relaxed syntax Using symbols 0.556*metre/second 0.556 metre/second 0.556 m/s 0.560*metre/second 0.560 metre/second 0.560 m/s 0.6*metre/second 0.6 metre/second 0.6 m/s 2*kilo*metre/hour 2 kilometre/hour 2 km/h 1.9*kilo*metre/hour 1.9 kilometre/hour 1.9 km/h 2.1*kilo*metre/hour 2.1 kilometre/hour 2.1 km/h In the example given in the example problem set, the following responses are tested and evaluated as incorrect: Strict syntax Relaxed syntax Using symbols 0.61*metre/second 0.61 metre/second 0.61 m/s 2.2*kilo*metre/hour 2.2 kilometre/hour 2.2 km/h c) \u00b6 Checking if a quantity is equal to \\(2~\\frac{kilometre}{hour}\\) with a fixed relative tolerance of \\(0.05\\) can be done with a TEXT response area with rtol set to 0.05 and the answer set to 2*kilo*metre/hour . The comparison parameter could also be set to expression but since this is the default it is not necessary. In the example given in the example problem set, the following responses are tested and evaluated as correct: Strict syntax Relaxed syntax Using symbols 0.533*metre/second 0.533 metre/second 0.533 m/s 2.08*kilo*metre/hour 2.08 kilometre/hour 2.08 km/h With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since only default SI units are expected it is not necessary to set any input symbols. In the example given in the example problem set, the following responses are tested and evaluated as incorrect: Strict syntax Relaxed syntax Using symbols 0.522*metre/second 0.522 metre/second 0.522 m/s 2.11*kilo*metre/hour 2.11 kilometre/hour 2.11 km/h 3 Checking if a set of quantities match the Buckingham pi theorem \u00b6 a) \u00b6 In this example the task is: Given \\(U\\) , \\(L\\) and \\(\\nu\\) , suggest a dimensionless group. For this problem we do not need to predefine any quantities and give exact dimensions. The algorithm assumes that all symbols in the answer (that are not numbers or predefined constants such as \\(\\pi\\) ) are quantities and that there are no other quantities that should appear in the answer. Note: This means that the algorithm does not in any way check that the stated answer is dimensionless, ensuring that that is left to the problem author. For this example a TEXT response area is used with comparison set to buckinghamPi and answer set to ['U*L/nu'] . It is not necessary to use this specific answer, any example of a correct dimensionless group should work. With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since nu is a multicharacter symbol it needs to be added as an input symbol. b) \u00b6 In this example the task is: Given \\(U\\) , \\(L\\) , \\(\\nu\\) and \\(f\\) , determine the necessary number of dimensionless groups and give one example of possible expressions for them. This task is similar to example a) with two significant differences. First, adding \\(f\\) means that there are now two groups required, and second the problem will constructed by defining the quantities and let the function compute the rest on its own instead of supplying a reference example. For this example a TEXT response area is used with comparison set to buckinghamPi , quantities set to ('U','(length/time)') ('L','(length)') ('nu','(length**2/time)') ('f','(1/time)') and answer set to - . With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since nu is a multicharacter symbol it needs to be added as an input symbol. c) \u00b6 In this example the task is: Suppose we are studying water waves that move under the influence of gravity. We suppose that the variables of interest are the acceleration in free fall \\(g\\) , the velocity of the wave \\(v\\) , the height of the wave \\(h\\) and the wave length \\(\\ell\\) . We also suppose that they are related by a dimensionally consistent equation \\(f(g,v,h,l) = 0\\) . Determine the minimum number of dimensionless \\(\\pi\\) -variables needed to describe this problem according to the Buckingham pi-theorem and give one example of possible expressions for the dimensionless quantities. For this problem two dimensionless groups are needed, see the worked solution for a terse solution that gives the general form of the dimensionless quantities. For this example a TEXT response area is used with comparison set to buckinghamPi and then give a list of correct group expressions formatted as the code for a python list. For this example the answer ['g**(-2)*v**4*h*l**3', 'g**(-2)*v**4*h**2*l**4'] was used (this corresponds to \\(p_1 = 1\\) , \\(p_2 = 2\\) , \\(q_1 = 3\\) , \\(q_2 = 4\\) in the worked solution). The feedback was costumized by setting the custom_feedback parameter too: \"custom_feedback\": { \"VALID_CANDIDATE_SET\": \"Your list of power products satisfies the Buckingham Pi theorem.\", \"NOT_DIMENSIONLESS\": \"At least one power product is not dimensionless.\", \"MORE_GROUPS_THAN_REFERENCE_SET\": \"Response has more power products than necessary.\", \"CANDIDATE_GROUPS_NOT_INDEPENDENT\": \"Power products in response are not independent.\", \"TOO_FEW_INDEPENDENT_GROUPS\": \"Candidate set contains too few independent groups.\", \"UNKNOWN_SYMBOL\": \"One of the prower products contains an unkown symbol.\", \"SUM_WITH_INDEPENDENT_TERMS\": \"The candidate set contains an expression which contains more independent terms that there are groups in total. The candidate set should ideally only contain expressions written as power products.\" } With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since nu is a multicharacter symbol it needs to be added as an input symbol. 4 Defining costum sets of units \u00b6 In this problem it is demonstrated how to use substitutions to define costum units. a) \u00b6 In this problem currencies will be us as units, and thus the quantities will no longer be physical. Here the substitutions parameter will be set so that the evaluation function can be used to compare. Note that using substitutions this way means that the default SI units can no longer be used. The following exchange rates (from Bank of England 1 August 2022) will be used: Currency Exchange rate \\(1\\) EUR \\(1.1957\\) GBP \\(1\\) USD \\(1.2283\\) GBP \\(1\\) CNY \\(8.3104\\) GBP \\(1\\) INR \\(96.943\\) GBP To compare prices written in different currencies a reference currency needs to be chosen. In this case GBP will be used. To substitute other currencies for their corresponding value in GBP the following grading parameter can be used: \"substitutions\" : \"('EUR','(1/1.1957)*GBP') ('USD','(1/1.2283)*GBP') ('CNY','(1/8.3104)*GBP') ('INR','(1/96.9430)*GBP')\" Since these conversion are not exact and for practical purposes prices are often not gives with more than two decimals of precision we also want to set the absolute tolerance, atol , to \\(0.05\\) . With default settings it is required to put * (or / ) between each part of the response and answer. By setting the grading parameter strict_syntax to false the * can be omitted and ^ can be used instead of ** . To ensure that this works correctly it is necessary to list the multicharacter symbols that are expected to appear in the answer and response as input symbols. For this example this means setting EUR , USD , CNY and INR as codes for inut symbols. In the example given in the example problem set, the answer set to 10*GBP and the following responses are tested and evaluated as correct: Strict syntax Relaxed syntax 11.96*EUR 11.96 EUR 12.28*USD 12.28 USD 83.10*CNY 83.10 CNY 969.43*INR 969.43 INR","title":"comparePhysicalQuantities"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#comparephysicalquantities","text":"Evaluation function which proveds some basic some dimensional analysis functionality. DEPRECATED Comparing physical quantities RECOMMENDED ALTERNATIVE: CompareExpressions with the physical_quantity parameter set to true Substitutions of symbols before comparison of expressions is done Checking if a comma separated list of expressions can be interpreted as a set groups that satisfies the Buckingham Pi theorem Note: When the quantities grading parameter is set, this function cannot handle short form symbols for units. Thus when defining quantities all units must be given with full names in lower-case letters. For example Nm/s or Newton*metre/SECOND will not be handled correctly, but newton*metre/second will. Note: Prefixes have lower precedence than exponentiation, e.g. 10*cm**2 will be interpreted as \\(10 \\cdot 10^{-2}~\\mathrm{metre}^2\\) rather than \\(10 \\cdot (10^{-2}~\\mathrm{metre})^2\\) . Note: This function allows omitting * and using ^ instead of ** if the grading parameter strict_syntax is set to false. In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols. Note: Only the short forms listed in the tables below are accepted. Not all units that are supported have short forms (since this leads to ambiguities). Note: When using the short forms the following convention is assumed: - Long form names takes precedence over sequences of short forms, e.g. e.g. mN will be interpreted as milli newton , Nm as newton metre , mmN as milli metre newton , mNm as milli newton metre and Nmm as newton milli metre . - Short form symbols of prefixes will take precedence over short form symbols of units from the left, e.g. mug will be interpreted as micro*gram instead metre*astronomicalunit*gram . - If there is a short form symbol for a prefix that collides with the short form for a unit (i.e. m ) then it is assumed the that unit will always be placed to the right of another unit in compound units, e.g. mN will be interpreted as milli newton , Nm as newton metre , mmN as milli metre newton , mNm as milli newton metre and Nmm as newton milli metre . - Longer short form symbols take precedence over shorter short forms, e.g. sr will be interpreted as steradian instead of second radian . Note: setting elementary_functions to true will disable using short forms symbols for units.","title":"ComparePhysicalQuantities"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#inputs","text":"All input parameters need to be supplied via the Grading parameters panel. There are seven optional parameters that can be set: elementary_functions , substitutions , quantities , strict_syntax , rtol , atol and comparison .","title":"Inputs"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#custom_feedback","text":"Custom feedback can be set on a per-task basis. Note: Custom feedback only supports fixed strings, this means that for some situations the custom feedback cannot be as detailed as the default feedback. The parameter must be set as a dictionary with keys from the feedback tags listed below. The value for each key can be any string.","title":"custom_feedback"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#feedback-tags-for-all-comparisons","text":"PARSE_ERROR_WARNING Response cannot be parsed as an expression or physical quantity. PER_FOR_DIVISION Warns about risk of ambiguity when using per instead / for division. STRICT_SYNTAX_EXPONENTIATION Warns that ^ cannot be used for exponentiation when strict_syntax is set to true . QUANTITIES_NOT_WRITTEN_CORRECTLY Text in error message that appears if list of quantities could not be parsed. SUBSTITUTIONS_NOT_WRITTEN_CORRECTLY Text in error message that appears if list of substitutions could not be parsed.","title":"Feedback tags for all comparisons"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#feedback-tags-for-buckinghampi-comparison","text":"VALID_CANDIDATE_SET Message that is displayed when a response is found to be a valid set of groups. Note: setting this will not affect the Correct/Incorrect message, it will only add further text. NOT_DIMENSIONLESS Message displayed when at least one groups is not dimensionless. MORE_GROUPS_THAN_REFERENCE_SET Message displayed when the response contains more groups than necessary. CANDIDATE_GROUPS_NOT_INDEPENDENT Message displayed when the groups in the response are not independent. TOO_FEW_INDEPENDENT_GROUPS Message displayed when the response contains fewer groups than necessary. UNKNOWN_SYMBOL Message displayed when the response contains some undefined symbol. SUM_WITH_INDEPENDENT_TERMS Message displayed when the response has too few groups but one (or more) of the groups is a sum with independent terms.","title":"Feedback tags for buckinghamPi comparison"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#elementary_functions","text":"When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting elementary_functions to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview. sin , sinc , csc (alternative cosec ), cos , sec , tan , cot (alternative cotan ), asin (alternative arcsin ), acsc (alternatives arccsc , arccosec ), acos (alternative arccos ), asec (alternative arcsec ), atan (alternative arctan ), acot (alternatives arccot , arccotan ), atan2 (alternative arctan2 ), sinh , cosh , tanh , csch (alternative cosech ), sech , asinh (alternative arcsinh ), acosh (alternative arccosh ), atanh (alternative arctanh ), acsch (alternatives arccsch , arcosech ), asech (alternative arcsech ), exp (alternative Exp ), E (equivalent to exp(1) , alternative e ), log , sqrt , sign , Abs (alternative abs ), Max (alternative max ), Min (alternative min ), arg , ceiling (alternative ceil ), floor Note: setting elementary_functions to true will disable using short forms symbols for units.","title":"elementary_functions"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#substitutions","text":"String that lists all substitutions that should be done to the answer and response inputs before processing. Each substitution should be written in the form ('original string','substitution string') and all pairs concatenated into a single string. Substitutions can be grouped by adding | between two substitutions. Then all substitutions before | will be performed before the substitutions after | . The input can contain an arbitrary number of substitutions and | symbols. Note that using substitutions will replace all default definitions of quantities and dimensions.","title":"substitutions"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#quantities","text":"String that lists all quantities that can be used in the answer and response. Each quantity should be written in the form ('quantity name','(units)') and all pairs concatenated into a single string. See tables below for available default units. Whenever units are used they must be written exactly as in the left columns of the tables given below (no short forms or single-character symbols) and units must be multiplied (or divided) by each other, as well as any accompanying quantities. NOTE: Using units and predefined quantities at the same time in an answer or response can cause problems (especially if quantities are denoted using single characters). Ideally it should be clear that either predefined quantities, or units should only be used from the question. If the comparison parameter is set to dimensions , it is not necessary to give exact units for each quantity, but the dimensions must be given instead. See tables below for available default dimensions. If the comparison parameter is set to buckinghamPi , then quantities should be set in a different way. See the detailed description of buckinghamPi further down.","title":"quantities"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-base-si-units","text":"SI base units taken from Table 1 of https://physics.nist.gov/cuu/Units/units.html Note that gram is used as a base unit instead of kilogram. SI base unit Symbol Dimension name metre m length gram g mass second s time ampere A electriccurrent kelvin k temperature mole mol amountofsubstance candela cd luminousintensity","title":"Table: Base SI units"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-si-prefixes","text":"SI base units taken from Table 5 of https://physics.nist.gov/cuu/Units/prefixes.html SI Prefix Symbol Factor SI Prefix Symbol Factor yotta Y \\(10^{24}\\) deci d \\(10^{-1}\\) zetta Z \\(10^{21}\\) centi c \\(10^{-2}\\) exa' E \\(10^{18}\\) milli m \\(10^{-3}\\) peta P \\(10^{15}\\) micro mu \\(10^{-6}\\) tera T \\(10^{12}\\) nano n \\(10^{-9}\\) giga G \\(10^{9}\\) pico p \\(10^{-12}\\) mega M \\(10^{6}\\) femto f \\(10^{-15}\\) kilo k \\(10^{3}\\) atto a \\(10^{-18}\\) hecto h \\(10^{2}\\) zepto z \\(10^{-21}\\) deka da \\(10^{1}\\) yocto y \\(10^{-24}\\)","title":"Table: SI prefixes"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-derived-si-units","text":"Derived SI units taken from Table 3 of https://physics.nist.gov/cuu/Units/units.html Note that degrees Celsius is omitted. Note that the function treats radians and steradians as dimensionless values. Unit name Symbol Expressed in base SI units radian r 1 steradian sr 1 hertz Hz \\(\\mathrm{second}^{-1}\\) newton N \\(\\mathrm{metre}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) pascal Pa \\(\\mathrm{metre}^{-1}~\\mathrm{kilogram}~\\mathrm{second}^{-2}\\) joule J \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}\\) watt W \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-3}\\) coulomb C \\(\\mathrm{second~ampere}\\) volt V \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-1}\\) farad F \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^4~\\mathrm{ampere}^2\\) ohm O \\(\\mathrm{metre}^2~\\mathrm{kilogram second}^{-3}~\\mathrm{ampere}^{-2}\\) siemens S \\(\\mathrm{metre}^{-2}~\\mathrm{kilogram}^{-1}~\\mathrm{second}^3~\\mathrm{ampere}^2\\) weber Wb \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-1}\\) tesla T \\(\\mathrm{kilogram~second}^{-2} \\mathrm{ampere}^{-1}\\) henry H \\(\\mathrm{metre}^2~\\mathrm{kilogram~second}^{-2}~\\mathrm{ampere}^{-2}\\) lumen lm \\(\\mathrm{candela}\\) lux lx \\(\\mathrm{metre}^{-2}~\\mathrm{candela}\\) becquerel Bq \\(\\mathrm{second}^{-1}\\) gray Gy \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) sievert Sv \\(\\mathrm{metre}^2~\\mathrm{second}^{-2}\\) katal kat \\(\\mathrm{mole~second}^{-1}\\)","title":"Table: Derived SI units"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-common-non-si-units","text":"Commonly used non-SI units taken from Table 6 and 7 of https://physics.nist.gov/cuu/Units/outside.html Note that the function treats angles, neper and bel as dimensionless values. Note that only the first table in this section has short form symbols defined, the second table does not. Unit name Symbol Expressed in SI units minute min \\(60~\\mathrm{second}\\) hour h \\(3600~\\mathrm{second}\\) degree deg \\(\\frac{\\pi}{180}\\) liter l \\(10^{-3}~\\mathrm{metre}^3\\) metric_ton t \\(10^3~\\mathrm{kilogram}\\) neper Np \\(1\\) bel B \\(\\frac{1}{2}~\\ln(10)\\) electronvolt eV \\(1.60218 \\cdot 10^{-19}~\\mathrm{joule}\\) atomic_mass_unit u \\(1.66054 \\cdot 10^{-27}~\\mathrm{kilogram}\\) angstrom \u00e5 \\(10^{-10}~\\mathrm{metre}\\) Unit name Expressed in SI units day \\(86400~\\mathrm{second}\\) angleminute \\(\\frac{\\pi}{10800}\\) anglesecond \\(\\frac{\\pi}{648000}\\) astronomicalunit \\(149597870700~\\mathrm{metre}\\) nauticalmile \\(1852~\\mathrm{metre}\\) knot \\(\\frac{1852}{3600}~\\mathrm{metre~second}^{-1}\\) are \\(10^2~\\mathrm{metre}^2\\) hectare \\(10^4~\\mathrm{metre}^2\\) bar \\(10^5~\\mathrm{pascal}\\) barn \\(10^{-28}~\\mathrm{metre}\\) curie $3.7 \\cdot 10^{10}~\\mathrm{becquerel} roentgen \\(2.58 \\cdot 10^{-4}~\\mathrm{kelvin~(kilogram)}^{-1}\\) rad \\(10^{-2}~\\mathrm{gray}\\) rem \\(10^{-2}~\\mathrm{sievert}\\)","title":"Table: Common non-SI units"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#table-imperial-units","text":"Commonly imperial units taken from https://en.wikipedia.org/wiki/Imperial_units Unit name Symbol Expressed in SI units inch in \\(0.0254~\\mathrm{metre}\\) foot ft \\(0.3048~\\mathrm{metre}\\) yard yd \\(0.9144~\\mathrm{metre}\\) mile mi \\(1609.344~\\mathrm{metre}\\) fluid ounce fl oz \\(28.4130625~\\mathrm{millilitre}\\) gill gi \\(142.0653125~\\mathrm{millilitre}\\) pint pt \\(568.26125~\\mathrm{millilitre}\\) quart qt \\(1.1365225~\\mathrm{litre}\\) gallon gal \\(4546.09~\\mathrm{litre}\\) ounce oz \\(28.349523125~\\mathrm{gram}\\) pound lb \\(0.45359237~\\mathrm{kilogram}\\) stone st \\(6.35029318~\\mathrm{kilogram}\\)","title":"Table: Imperial units"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#strict_syntax","text":"If strict_syntax is set to true then the answer and response must have * or / between each part of the expressions and exponentiation must be done using ** , e.g. 10*kilo*metre/second**2 is accepted but 10 kilometre/second^2 is not. If strict_syntax is set to false, then * can be omitted and ^ used instead of ** . In this case it is also recommended to list any multicharacter symbols (that are not part of the default list of SI units) expected to appear in the response as input symbols. By default strict_syntax is set to true.","title":"strict_syntax"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#rtol","text":"Maximum relative error allowed when comparing expressions.","title":"rtol"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#atol","text":"Maximum absolute error allowed when comparing expressions.","title":"atol"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#comparison","text":"Parameter that determines what kind of comparison is done. There are four possible options: expression Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close (as specified by the atol and rtol parameters). expressionExact Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is identical to as high precision as possible. dimensions Checks that the answer and response have the same dimensions, does not compare the values of the physical quantities. buckinghamPi Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem. For more details on each options see the description below and the corresponding examples. If comparison is not specified it defaults to expression .","title":"comparison"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#expression","text":"Converts the expression to base SI units and checks that the units are the same and that the value of the answer and response is sufficienty close. How big the difference is between the value of the answer and the value of the response is decided by the rtol and atol parameters. If neither atol nor rtol is specified the function will allow a relative error of \\(10^{-12}\\) . If atol is specified its value will be interpreted as the maximum allowed absolute error. If rtol is specified its value will be interpreted as the maximum allowed relative error. If both atol and rtol the function will check both the absolute and relative error.","title":"expression"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#expressionexact","text":"Converts the expression to base SI units and checks that the answer and response are identical to the highest precision possible (note that some unit conversions are not exact and that using decimal numbers in the answer or response limits this to floating point precision).","title":"expressionExact"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#dimensions","text":"Checks that the answer and response have the same dimensions, but does not compare the values of the physical quantities. With this option the quantities (specified by the quantities parameter) can be given either dimension only, or units.","title":"dimensions"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#buckinghampi","text":"Checks that the set of quantities in the response matches the set of quantities in the sense given by the Buckingham Pi theorem. There are three different ways of supplying this function with the necessary information. - In the answer, provide an example set of groups as a comma seprated list. When used this way the function assumes that the given list is correct and contains at least the minimum number of groups. - In the quantities parameter, supply a list of what the dimensions for each quantity is and set answer to - . The function will then compute a list of sufficiently many independen dimensionless quantities and compare to the response. - In the quantities parameter, supply a list of what the dimensions for each quantity is and in the answer, supply a list of groups as in the first option. The function will then check that the supplied answer is dimensionless and has a sufficient number of independent groups before comparing it to the response. Note that in lists of groups the items should ideally be written on the form \\(q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities and \\(c_1, c_2 \\ldots c_n\\) are integers, but the function can also handle item that are sums with terms written on the form \\(a \\cdot q_1^{c_1} \\cdot q_2^{c_2} \\cdots q_n^{c_n}\\) where \\(q_1, q_2 \\ldots q_n\\) are quantities, \\(c_1, c_2 \\ldots c_n\\) rational numbers and \\(a\\) a constant. If the total number of groups is less than required the set of groups is considered invalid, even if there is a sufficient number of terms with independent power products in the response.","title":"buckinghamPi"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#examples","text":"Implemented versions of these examples can be found in the module 'Examples: Evaluation Functions'.","title":"Examples"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#1-checking-the-dimensions-of-an-expression-or-physical-quantity","text":"DEPRECATED RECOMMENDED ALTERNATIVE: CompareExpressions with the physical_quantity parameter set to true This example will check if the response has dimensions \\(\\frac{\\mathrm{length}^2}{\\mathrm{time}^2}\\) .","title":"1 Checking the dimensions of an expression or physical quantity"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#a","text":"To check an expression there needs to be some predefined quantities that can be used in the expression. Since only dimensions will be checked units are not necessary (but could be used as well). Here a response area with input type TEXT and two grading parameters, quantities and comparison , will be used. quantities is defined as follows: ('d','(length)') ('t','(time)') ('v','(length/time)') comparison is set to dimensions . The answer is set two some expression with the right dimensions, e.g. v**2 . With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. In the example given in the example problem set, the following responses are tested and evaluated as correct: Strict syntax Relaxed syntax v**2 v^2 5*v**2 5v^2 (d/t)**2+v**2 (d/t)^2+v^2 d**2/t**2 d^2/t^2 d**2*t**(-2) d^2 t^(-2) d/t*v vd/t","title":"a)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#b","text":"Checking the dimensions of a quantity directly, i.e. the dimensions of an expression of the form number*units , no predefined quantities are necessary. Here a response area with input type TEXT and one grading parameter, comparison , will be used. comparison is set to dimensions . The answer is set two some expression with the right dimensions, e.g. length**2/time**2 . With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since only default SI units are expected in the answer we do not need to set any input symbols. In the example given in the example problem set, the following responses are tested and evaluated as correct: Strict syntax Relaxed syntax Using symbols metre**2/second**2 metre^2/second^2 m^2/s^2 (centi*metre)**2/hour**2 (centimetre)^2/h^2 (cm)^2/h^2 246*ohm/(kilo*gram)*coulomb**2/second 246 ohm/(kilogram) coulomb^2/second 246 O/(kg) c^2/s","title":"b)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#2-checking-the-value-of-an-expression-or-a-physical-quantity","text":"DEPRECATED RECOMMENDED ALTERNATIVE: CompareExpressions with the physical_quantity parameter set to true This examples checks if your expression is equal to \\(2~\\frac{\\mathrm{kilometre}}{\\mathrm{hour}}\\) .","title":"2 Checking the value of an expression or a physical quantity"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#a_1","text":"Here an expression with predefined quantities is checked as exactly as possible. This is done with a TEXT response area with the following parameters: quantities is set to: ('d','(length)') ('t','(time)') ('v','(length/time)') Note that short form symbols cannot be used when defining quantities. comparison is set to expressionExact . The response area answer is set to 2*v but there are many other expressions that would work just as well. Note that we cannot write 2*kilo*metre/second as response or answer since the predefined quantity t will substitute the t in metre which results in unparseable input. With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since only default SI units and single character symbols are expected in the answer we will not set the grading parameter symbols . In the example given in the example problem set, the following responses are tested and evaluated as correct: Strict syntax Relaxed syntax 2*v 2v 2000/3600*d/t 2000/3600 d/t 1/1.8*d/t d/(1.8t) v+1/3.6*d/t v+d/(3.6t)","title":"a)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#b_1","text":"Checking if a quantity is equal to \\(2~\\frac{kilometre}{hour}\\) with a fixed absolute tolerance of \\(0.05 \\frac{metre}{second}\\) can be done with a TEXT response area with atol set to 0.05 and the answer set to 2*kilo*metre/hour . Note: atol is always assumed to be given in the base SI units version of the expression. This is likely to change in future versions of the function. The comparison parameter could also be set to expression but since this is the default it is not necessary. With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since only default SI units are expected in the answer no input symbols are necessary. In the example given in the example problem set, the following responses are tested and evaluated as correct: Strict syntax Relaxed syntax Using symbols 0.556*metre/second 0.556 metre/second 0.556 m/s 0.560*metre/second 0.560 metre/second 0.560 m/s 0.6*metre/second 0.6 metre/second 0.6 m/s 2*kilo*metre/hour 2 kilometre/hour 2 km/h 1.9*kilo*metre/hour 1.9 kilometre/hour 1.9 km/h 2.1*kilo*metre/hour 2.1 kilometre/hour 2.1 km/h In the example given in the example problem set, the following responses are tested and evaluated as incorrect: Strict syntax Relaxed syntax Using symbols 0.61*metre/second 0.61 metre/second 0.61 m/s 2.2*kilo*metre/hour 2.2 kilometre/hour 2.2 km/h","title":"b)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#c","text":"Checking if a quantity is equal to \\(2~\\frac{kilometre}{hour}\\) with a fixed relative tolerance of \\(0.05\\) can be done with a TEXT response area with rtol set to 0.05 and the answer set to 2*kilo*metre/hour . The comparison parameter could also be set to expression but since this is the default it is not necessary. In the example given in the example problem set, the following responses are tested and evaluated as correct: Strict syntax Relaxed syntax Using symbols 0.533*metre/second 0.533 metre/second 0.533 m/s 2.08*kilo*metre/hour 2.08 kilometre/hour 2.08 km/h With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since only default SI units are expected it is not necessary to set any input symbols. In the example given in the example problem set, the following responses are tested and evaluated as incorrect: Strict syntax Relaxed syntax Using symbols 0.522*metre/second 0.522 metre/second 0.522 m/s 2.11*kilo*metre/hour 2.11 kilometre/hour 2.11 km/h","title":"c)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#3-checking-if-a-set-of-quantities-match-the-buckingham-pi-theorem","text":"","title":"3 Checking if a set of quantities match the Buckingham pi theorem"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#a_2","text":"In this example the task is: Given \\(U\\) , \\(L\\) and \\(\\nu\\) , suggest a dimensionless group. For this problem we do not need to predefine any quantities and give exact dimensions. The algorithm assumes that all symbols in the answer (that are not numbers or predefined constants such as \\(\\pi\\) ) are quantities and that there are no other quantities that should appear in the answer. Note: This means that the algorithm does not in any way check that the stated answer is dimensionless, ensuring that that is left to the problem author. For this example a TEXT response area is used with comparison set to buckinghamPi and answer set to ['U*L/nu'] . It is not necessary to use this specific answer, any example of a correct dimensionless group should work. With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since nu is a multicharacter symbol it needs to be added as an input symbol.","title":"a)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#b_2","text":"In this example the task is: Given \\(U\\) , \\(L\\) , \\(\\nu\\) and \\(f\\) , determine the necessary number of dimensionless groups and give one example of possible expressions for them. This task is similar to example a) with two significant differences. First, adding \\(f\\) means that there are now two groups required, and second the problem will constructed by defining the quantities and let the function compute the rest on its own instead of supplying a reference example. For this example a TEXT response area is used with comparison set to buckinghamPi , quantities set to ('U','(length/time)') ('L','(length)') ('nu','(length**2/time)') ('f','(1/time)') and answer set to - . With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since nu is a multicharacter symbol it needs to be added as an input symbol.","title":"b)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#c_1","text":"In this example the task is: Suppose we are studying water waves that move under the influence of gravity. We suppose that the variables of interest are the acceleration in free fall \\(g\\) , the velocity of the wave \\(v\\) , the height of the wave \\(h\\) and the wave length \\(\\ell\\) . We also suppose that they are related by a dimensionally consistent equation \\(f(g,v,h,l) = 0\\) . Determine the minimum number of dimensionless \\(\\pi\\) -variables needed to describe this problem according to the Buckingham pi-theorem and give one example of possible expressions for the dimensionless quantities. For this problem two dimensionless groups are needed, see the worked solution for a terse solution that gives the general form of the dimensionless quantities. For this example a TEXT response area is used with comparison set to buckinghamPi and then give a list of correct group expressions formatted as the code for a python list. For this example the answer ['g**(-2)*v**4*h*l**3', 'g**(-2)*v**4*h**2*l**4'] was used (this corresponds to \\(p_1 = 1\\) , \\(p_2 = 2\\) , \\(q_1 = 3\\) , \\(q_2 = 4\\) in the worked solution). The feedback was costumized by setting the custom_feedback parameter too: \"custom_feedback\": { \"VALID_CANDIDATE_SET\": \"Your list of power products satisfies the Buckingham Pi theorem.\", \"NOT_DIMENSIONLESS\": \"At least one power product is not dimensionless.\", \"MORE_GROUPS_THAN_REFERENCE_SET\": \"Response has more power products than necessary.\", \"CANDIDATE_GROUPS_NOT_INDEPENDENT\": \"Power products in response are not independent.\", \"TOO_FEW_INDEPENDENT_GROUPS\": \"Candidate set contains too few independent groups.\", \"UNKNOWN_SYMBOL\": \"One of the prower products contains an unkown symbol.\", \"SUM_WITH_INDEPENDENT_TERMS\": \"The candidate set contains an expression which contains more independent terms that there are groups in total. The candidate set should ideally only contain expressions written as power products.\" } With default settings it is required to put * (or / ) between each part of the response and answer. To remove this requirement the grading parameter strict_syntax is set to false. Since nu is a multicharacter symbol it needs to be added as an input symbol.","title":"c)"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#4-defining-costum-sets-of-units","text":"In this problem it is demonstrated how to use substitutions to define costum units.","title":"4 Defining costum sets of units"},{"location":"user_eval_function_docs/comparePhysicalQuantities/#a_3","text":"In this problem currencies will be us as units, and thus the quantities will no longer be physical. Here the substitutions parameter will be set so that the evaluation function can be used to compare. Note that using substitutions this way means that the default SI units can no longer be used. The following exchange rates (from Bank of England 1 August 2022) will be used: Currency Exchange rate \\(1\\) EUR \\(1.1957\\) GBP \\(1\\) USD \\(1.2283\\) GBP \\(1\\) CNY \\(8.3104\\) GBP \\(1\\) INR \\(96.943\\) GBP To compare prices written in different currencies a reference currency needs to be chosen. In this case GBP will be used. To substitute other currencies for their corresponding value in GBP the following grading parameter can be used: \"substitutions\" : \"('EUR','(1/1.1957)*GBP') ('USD','(1/1.2283)*GBP') ('CNY','(1/8.3104)*GBP') ('INR','(1/96.9430)*GBP')\" Since these conversion are not exact and for practical purposes prices are often not gives with more than two decimals of precision we also want to set the absolute tolerance, atol , to \\(0.05\\) . With default settings it is required to put * (or / ) between each part of the response and answer. By setting the grading parameter strict_syntax to false the * can be omitted and ^ can be used instead of ** . To ensure that this works correctly it is necessary to list the multicharacter symbols that are expected to appear in the answer and response as input symbols. For this example this means setting EUR , USD , CNY and INR as codes for inut symbols. In the example given in the example problem set, the answer set to 10*GBP and the following responses are tested and evaluated as correct: Strict syntax Relaxed syntax 11.96*EUR 11.96 EUR 12.28*USD 12.28 USD 83.10*CNY 83.10 CNY 969.43*INR 969.43 INR","title":"a)"},{"location":"user_eval_function_docs/symbolicEqual/","text":"Supported Response Area Types This evaluation function is supported by the following Response Area components: TEXT EXPRESSION SymbolicEqual \u00b6 This function utilises the SymPy to provide a maths-aware comparsion of a student's response to the correct answer. This means that mathematically equivalent inputs will be marked as correct. Note that pi is a reserved constant and cannot be used as a symbol name. Note that this function is designed to handle comparisons of mathematical expressions but has some limited ability to handle comparison of equations as well. More precisely, if the answer is of the form \\(f(x_1,\\ldots,x_n) = g(x_1,\\ldots,x_n)\\) and the response is of the form \\(\\tilde{f}(x_1,\\ldots,x_n) = \\tilde{g}(x_1,\\ldots,x_n)\\) then the function checks if \\(f(x_1,\\ldots,x_n) - g(x_1,\\ldots,x_n)\\) is a multiple of \\(\\tilde{f}(x_1,\\ldots,x_n) / \\tilde{g}(x_1,\\ldots,x_n)\\) . Inputs \u00b6 Optional grading parameters \u00b6 There are eight optional parameters that can be set: complexNumbers , elementary_functions , specialFunctions , strict_syntax , symbol_assumptions , multiple_answers_criteria , plus_minus and minus_plus . complexNumbers \u00b6 If you want to use I for the imaginary constant, set the grading parameter complexNumbers to True. elementary_functions \u00b6 When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting elementary_functions to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview. sin , sinc , csc (alternative cosec ), cos , sec , tan , cot (alternative cotan ), asin (alternative arcsin ), acsc (alternatives arccsc , arccosec ), acos (alternative arccos ), asec (alternative arcsec ), atan (alternative arctan ), acot (alternatives arccot , arccotan ), atan2 (alternative arctan2 ), sinh , cosh , tanh , csch (alternative cosech ), sech , asinh (alternative arcsinh ), acosh (alternative arccosh ), atanh (alternative arctanh ), acsch (alternatives arccsch , arcosech ), asech (alternative arcsech ), exp (alternative Exp ), E (equivalent to exp(1) , alternative e ), log , sqrt , sign , Abs (alternative abs ), Max (alternative max ), Min (alternative min ), arg , ceiling (alternative ceil ), floor specialFunctions \u00b6 If you want to use the special functions beta (Euler Beta function), gamma (Gamma function) and zeta (Riemann Zeta function), set the grading parameter specialFunctions to True. strict_syntax \u00b6 If strict_syntax is set to true then the answer and response must have * or / between each part of the expressions and exponentiation must be done using ** , e.g. 10*x*y/z**2 is accepted but 10xy/z^2 is not. If strict_syntax is set to false, then * can be omitted and ^ used instead of ** . In this case it is also recommended to list any multicharacter symbols expected to appear in the response as input symbols. By default strict_syntax is set to true. symbol_assumptions \u00b6 This input parameter allows the author to set an extra assumption each symbol. Each assumption should be written on the form ('symbol','assumption name') and all pairs concatenated into a single string. The possible assumption names can be found in this list: SymPy Assumption Predicates multiple_answers_criteria \u00b6 The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. Answers or responses that contain \\(\\pm\\) or \\(\\mp\\) has two possible interpretations which requires further criteria for equality. The grading parameter multiple_answers_criteria controls this. The default setting, all , is that each answer must have a corresponding answer and vice versa. The setting all_responses check that all responses are valid answers and the setting all_answers checks that all answers are found among the responses. plus_minus and minus_plus \u00b6 The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters plus_minus and minus_plus to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences. Examples \u00b6 Implemented versions of these examples can be found in the module 'Examples: Evaluation Functions'. 1 Setting input symbols to be assumed positive to avoid issues with fractional powers \u00b6 In general \\(\\frac{\\sqrt{a}}{\\sqrt{b}} \\neq \\sqrt{\\frac{a}{b}}\\) but if \\(a > 0\\) and \\(b > 0\\) then \\(\\frac{\\sqrt{a}}{\\sqrt{b}} = \\sqrt{\\frac{a}{b}}\\) . The same is true for other fractional powers. So if expression like these are expected in the answer and/or response then it is a good idea to use the symbol_assumptions parameter to note that \\(a > 0\\) and \\(b > 0\\) . This can be done by setting symbol_assumptions to ('a','positive') ('b','positive') . The example given in the example problem set uses an EXPRESSION response area that uses SymbolicEqual with answer sqrt(a/b) , strict_syntax set to false and symbol_assumptions set as above. Some examples of expressions that are accepted as correct: sqrt(a)/sqrt(b) , (a/b)**(1/2) , a**(1/2)/b**(1/2) , (a/b)^(0.5) , a^(0.5)/b^(0.5)","title":"symbolicEqual"},{"location":"user_eval_function_docs/symbolicEqual/#symbolicequal","text":"This function utilises the SymPy to provide a maths-aware comparsion of a student's response to the correct answer. This means that mathematically equivalent inputs will be marked as correct. Note that pi is a reserved constant and cannot be used as a symbol name. Note that this function is designed to handle comparisons of mathematical expressions but has some limited ability to handle comparison of equations as well. More precisely, if the answer is of the form \\(f(x_1,\\ldots,x_n) = g(x_1,\\ldots,x_n)\\) and the response is of the form \\(\\tilde{f}(x_1,\\ldots,x_n) = \\tilde{g}(x_1,\\ldots,x_n)\\) then the function checks if \\(f(x_1,\\ldots,x_n) - g(x_1,\\ldots,x_n)\\) is a multiple of \\(\\tilde{f}(x_1,\\ldots,x_n) / \\tilde{g}(x_1,\\ldots,x_n)\\) .","title":"SymbolicEqual"},{"location":"user_eval_function_docs/symbolicEqual/#inputs","text":"","title":"Inputs"},{"location":"user_eval_function_docs/symbolicEqual/#optional-grading-parameters","text":"There are eight optional parameters that can be set: complexNumbers , elementary_functions , specialFunctions , strict_syntax , symbol_assumptions , multiple_answers_criteria , plus_minus and minus_plus .","title":"Optional grading parameters"},{"location":"user_eval_function_docs/symbolicEqual/#complexnumbers","text":"If you want to use I for the imaginary constant, set the grading parameter complexNumbers to True.","title":"complexNumbers"},{"location":"user_eval_function_docs/symbolicEqual/#elementary_functions","text":"When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting elementary_functions to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview. sin , sinc , csc (alternative cosec ), cos , sec , tan , cot (alternative cotan ), asin (alternative arcsin ), acsc (alternatives arccsc , arccosec ), acos (alternative arccos ), asec (alternative arcsec ), atan (alternative arctan ), acot (alternatives arccot , arccotan ), atan2 (alternative arctan2 ), sinh , cosh , tanh , csch (alternative cosech ), sech , asinh (alternative arcsinh ), acosh (alternative arccosh ), atanh (alternative arctanh ), acsch (alternatives arccsch , arcosech ), asech (alternative arcsech ), exp (alternative Exp ), E (equivalent to exp(1) , alternative e ), log , sqrt , sign , Abs (alternative abs ), Max (alternative max ), Min (alternative min ), arg , ceiling (alternative ceil ), floor","title":"elementary_functions"},{"location":"user_eval_function_docs/symbolicEqual/#specialfunctions","text":"If you want to use the special functions beta (Euler Beta function), gamma (Gamma function) and zeta (Riemann Zeta function), set the grading parameter specialFunctions to True.","title":"specialFunctions"},{"location":"user_eval_function_docs/symbolicEqual/#strict_syntax","text":"If strict_syntax is set to true then the answer and response must have * or / between each part of the expressions and exponentiation must be done using ** , e.g. 10*x*y/z**2 is accepted but 10xy/z^2 is not. If strict_syntax is set to false, then * can be omitted and ^ used instead of ** . In this case it is also recommended to list any multicharacter symbols expected to appear in the response as input symbols. By default strict_syntax is set to true.","title":"strict_syntax"},{"location":"user_eval_function_docs/symbolicEqual/#symbol_assumptions","text":"This input parameter allows the author to set an extra assumption each symbol. Each assumption should be written on the form ('symbol','assumption name') and all pairs concatenated into a single string. The possible assumption names can be found in this list: SymPy Assumption Predicates","title":"symbol_assumptions"},{"location":"user_eval_function_docs/symbolicEqual/#multiple_answers_criteria","text":"The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. Answers or responses that contain \\(\\pm\\) or \\(\\mp\\) has two possible interpretations which requires further criteria for equality. The grading parameter multiple_answers_criteria controls this. The default setting, all , is that each answer must have a corresponding answer and vice versa. The setting all_responses check that all responses are valid answers and the setting all_answers checks that all answers are found among the responses.","title":"multiple_answers_criteria"},{"location":"user_eval_function_docs/symbolicEqual/#plus_minus-and-minus_plus","text":"The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters plus_minus and minus_plus to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences.","title":"plus_minus and minus_plus"},{"location":"user_eval_function_docs/symbolicEqual/#examples","text":"Implemented versions of these examples can be found in the module 'Examples: Evaluation Functions'.","title":"Examples"},{"location":"user_eval_function_docs/symbolicEqual/#1-setting-input-symbols-to-be-assumed-positive-to-avoid-issues-with-fractional-powers","text":"In general \\(\\frac{\\sqrt{a}}{\\sqrt{b}} \\neq \\sqrt{\\frac{a}{b}}\\) but if \\(a > 0\\) and \\(b > 0\\) then \\(\\frac{\\sqrt{a}}{\\sqrt{b}} = \\sqrt{\\frac{a}{b}}\\) . The same is true for other fractional powers. So if expression like these are expected in the answer and/or response then it is a good idea to use the symbol_assumptions parameter to note that \\(a > 0\\) and \\(b > 0\\) . This can be done by setting symbol_assumptions to ('a','positive') ('b','positive') . The example given in the example problem set uses an EXPRESSION response area that uses SymbolicEqual with answer sqrt(a/b) , strict_syntax set to false and symbol_assumptions set as above. Some examples of expressions that are accepted as correct: sqrt(a)/sqrt(b) , (a/b)**(1/2) , a**(1/2)/b**(1/2) , (a/b)^(0.5) , a^(0.5)/b^(0.5)","title":"1 Setting input symbols to be assumed positive to avoid issues with fractional powers"},{"location":"user_eval_function_docs/isSimilar/","text":"Supported Response Area Types This evaluation function is supported by the following Response Area components: NUMBER TEXT EXPRESSION IsSimilar \u00b6 Use this evaluation function to check if the student's reponse is within a tolerance range defined in params . Works exactly like the numpy.isclose function. Valid params include atol and rtol (absolute and relative tolerances) which can be used in combination, or alone.","title":"isSimilar"},{"location":"user_eval_function_docs/isSimilar/#issimilar","text":"Use this evaluation function to check if the student's reponse is within a tolerance range defined in params . Works exactly like the numpy.isclose function. Valid params include atol and rtol (absolute and relative tolerances) which can be used in combination, or alone.","title":"IsSimilar"},{"location":"user_eval_function_docs/arrayEqual/","text":"Supported Response Area Types This evaluation function is supported by the following Response Area components: MATRIX TABLE MULTIPLE_CHOICE ArrayEqual \u00b6 This function is used to compare two number arrays/vectors/matrices, provided absolute and/or relative tolerance parameters rtol and atol . This is carried out using the numpy.allclose function.","title":"arrayEqual"},{"location":"user_eval_function_docs/arrayEqual/#arrayequal","text":"This function is used to compare two number arrays/vectors/matrices, provided absolute and/or relative tolerance parameters rtol and atol . This is carried out using the numpy.allclose function.","title":"ArrayEqual"},{"location":"user_eval_function_docs/arraySymbolicEqual/","text":"Supported Response Area Types This evaluation function is supported by the following Response Area components: MATRIX TABLE ArraySymbolicEqual \u00b6 This function compares two symbolic expression arrays, with any level of nesting (2D, 3D, irregular shape, ...). Each cell is compared using the SymbolicEqual evaluation function. Inputs \u00b6 This function support all input parameters that SymbolicEqual uses.","title":"arraySymbolicEqual"},{"location":"user_eval_function_docs/arraySymbolicEqual/#arraysymbolicequal","text":"This function compares two symbolic expression arrays, with any level of nesting (2D, 3D, irregular shape, ...). Each cell is compared using the SymbolicEqual evaluation function.","title":"ArraySymbolicEqual"},{"location":"user_eval_function_docs/arraySymbolicEqual/#inputs","text":"This function support all input parameters that SymbolicEqual uses.","title":"Inputs"},{"location":"user_eval_function_docs/shortTextAnswer/","text":"Supported Response Area Types This evaluation function is supported by the following Response Area components: TEXT ShortTextAnswer \u00b6 This function evaluates the similarity value between two short texts, as well as identifying certain key strings in a student's answer. Inputs \u00b6 keystrings - Optional parameter. Represents a list of keystring objects which the function will search for in the answer. keystring object \u00b6 The keystring object contains several fields which affect how it will be interpreted: string - Required. The actual keystring being searched for. exact_match - Optional. A boolean value indicating whether to search for the exact string or for a semantically similar one. Defaults to false should_contain - Optional. A boolean value indicating whether it is expected for the keystring to be found in the answer or not. Defaults to true . Setting this flag to false indicates that a correct response will not contain the specified keystring. custom_feedback - Optional. A feedback string to be returned if the string was not found (or if it was, in case should_contain was set to false ). Defaults to None , in which case a generic response will be generated containing the string searched for. Outputs \u00b6 The function will return an object with 3 fields of interest. the is_correct and feedback fields are required by LambdaFeedback to present feedback to the user. The result field is only used for development. { \"is_correct\" : \"<bool>\" , \"result\" : { \"response\" : \"<string>\" , \"processing_time\" : \"<double>\" , }, \"feedback\" : \"string\" } response - The student answer. USed for debugging purposes. processing_time - The time it took for the function to evaluate If the function identified a problematic keystring, the result object will have an additional field: * keystring-scores - list(string, double). List of the provided keystrings and their best similarity scores that were found in the answer. Otherwise, it will have the additional fields: * method - string. Either \"w2v\" or \"BOW vector similarity\". * similarity_value - double. The similarity value between the response and the answer. If the method is w2v, it means the two texts were found to be similar. Otherwise, a BOW vector similarity check is performed in order to identify the most likely word that caused the texts to be found dissimilar. Examples \u00b6 List of example inputs and outputs for this function, each under a different sub-heading Example simple input, no keystring \u00b6 Input { \"response\" : \"Density, velocity, viscosity, length\" , \"answer\" : \"Density, speed, Viscosity, Length\" , } Output { 'is_correct' : True , 'result' : { 'response' : 'Density, speed, Viscosity, Length' , 'processing_time' : 0.022912219000000178 , 'method' : 'w2v' , 'similarity_value' : 0.9326027035713196 }, 'feedback' : 'Confidence: 0.933%' } Example keystring input \u00b6 Input { \"response\" : \"Molecules are made out of atoms\" , \"answer\" : \"Many atoms form a molecule\" , 'keystrings' : [ { 'string' : 'molecule' }, { 'string' : 'proton' , 'exact_match' : True } ] } Output { 'is_correct' : False , 'result' : { 'response' : 'Molecules are made out of atoms' , 'processing_time' : 0.30640586500000033 , 'keystring-scores' : [ ( 'molecule' , 0.990715997949492 ), ( 'proton' , 0.9186190596675989 ) # Searched for with exact match, therefore not a match. ] }, 'feedback' : \"Cannot determine if the answer is correct. Please provide more information about 'proton'\" }","title":"shortTextAnswer"},{"location":"user_eval_function_docs/shortTextAnswer/#shorttextanswer","text":"This function evaluates the similarity value between two short texts, as well as identifying certain key strings in a student's answer.","title":"ShortTextAnswer"},{"location":"user_eval_function_docs/shortTextAnswer/#inputs","text":"keystrings - Optional parameter. Represents a list of keystring objects which the function will search for in the answer.","title":"Inputs"},{"location":"user_eval_function_docs/shortTextAnswer/#keystring-object","text":"The keystring object contains several fields which affect how it will be interpreted: string - Required. The actual keystring being searched for. exact_match - Optional. A boolean value indicating whether to search for the exact string or for a semantically similar one. Defaults to false should_contain - Optional. A boolean value indicating whether it is expected for the keystring to be found in the answer or not. Defaults to true . Setting this flag to false indicates that a correct response will not contain the specified keystring. custom_feedback - Optional. A feedback string to be returned if the string was not found (or if it was, in case should_contain was set to false ). Defaults to None , in which case a generic response will be generated containing the string searched for.","title":"keystring object"},{"location":"user_eval_function_docs/shortTextAnswer/#outputs","text":"The function will return an object with 3 fields of interest. the is_correct and feedback fields are required by LambdaFeedback to present feedback to the user. The result field is only used for development. { \"is_correct\" : \"<bool>\" , \"result\" : { \"response\" : \"<string>\" , \"processing_time\" : \"<double>\" , }, \"feedback\" : \"string\" } response - The student answer. USed for debugging purposes. processing_time - The time it took for the function to evaluate If the function identified a problematic keystring, the result object will have an additional field: * keystring-scores - list(string, double). List of the provided keystrings and their best similarity scores that were found in the answer. Otherwise, it will have the additional fields: * method - string. Either \"w2v\" or \"BOW vector similarity\". * similarity_value - double. The similarity value between the response and the answer. If the method is w2v, it means the two texts were found to be similar. Otherwise, a BOW vector similarity check is performed in order to identify the most likely word that caused the texts to be found dissimilar.","title":"Outputs"},{"location":"user_eval_function_docs/shortTextAnswer/#examples","text":"List of example inputs and outputs for this function, each under a different sub-heading","title":"Examples"},{"location":"user_eval_function_docs/shortTextAnswer/#example-simple-input-no-keystring","text":"Input { \"response\" : \"Density, velocity, viscosity, length\" , \"answer\" : \"Density, speed, Viscosity, Length\" , } Output { 'is_correct' : True , 'result' : { 'response' : 'Density, speed, Viscosity, Length' , 'processing_time' : 0.022912219000000178 , 'method' : 'w2v' , 'similarity_value' : 0.9326027035713196 }, 'feedback' : 'Confidence: 0.933%' }","title":"Example simple input, no keystring"},{"location":"user_eval_function_docs/shortTextAnswer/#example-keystring-input","text":"Input { \"response\" : \"Molecules are made out of atoms\" , \"answer\" : \"Many atoms form a molecule\" , 'keystrings' : [ { 'string' : 'molecule' }, { 'string' : 'proton' , 'exact_match' : True } ] } Output { 'is_correct' : False , 'result' : { 'response' : 'Molecules are made out of atoms' , 'processing_time' : 0.30640586500000033 , 'keystring-scores' : [ ( 'molecule' , 0.990715997949492 ), ( 'proton' , 0.9186190596675989 ) # Searched for with exact match, therefore not a match. ] }, 'feedback' : \"Cannot determine if the answer is correct. Please provide more information about 'proton'\" }","title":"Example keystring input"},{"location":"user_eval_function_docs/chatGPT%20%28experimental%29/","text":"Supported Response Area Types This evaluation function is supported by the following Response Area components: TEXT chatGPT \u00b6 What does it do? \u00b6 This chatGPT evaluation function is designed to automatically evaluate student responses to questions. It currently uses the openAI API to determine the correctness (true/false) of the student's answer and can also provide them with feedback. What does the teacher need to input? \u00b6 Model As of now, gpt-3.5-turbo is the only model available. In the future, more openAI and other models can be implemented. Main_prompt In this prompt you should explain the question and answer to gpt. Default_prompt As of now, this prompt should not be changed. It tells gpt to output a Boolean, which marks the student's answer as correct or incorrect. In the future, this could be changed so that 'partially incorrect' answers, etc, are possible. Feedback_prompt Leave this prompt blank if you do not want any textual feedback to be given to the student, but just correct/incorrect. Fill in this prompt to tell gpt how to give feedback to the student. Examples of things you may want to include in your feedback_prompt : Give the student objective and constructive feedback on their answer in first person. If the student is incorrect, provide feedback/hints to help them, but do not reveal the answer. Please be aware that gpt-3.5-turbo often gets overwhelmed by a long prompts, and can sometimes ignore your instructions. It is recommended to do testing with your question, and use gpt-4 if neccesary. Be aware that the cost of gpt-4 is ~20x more than gpt-3.5-turbo (at the time of writing - 6th Sep 2023). Usage examples \u00b6 Each example below demonstrates the potential usage of main_prompt and feedback_prompt for different questions. Simple descriptive observation \u00b6 Main Prompt : In this question, the student is asked to make a comment about the behaviour of a partial sum. The correct answer is 'fast convergence'. Accept any paraphrasing/equivalent answers. To be correct, they must mention both aspects (fast and convergence). Feedback Prompt : (Empty)","title":"chatGPT (experimental)"},{"location":"user_eval_function_docs/chatGPT%20%28experimental%29/#chatgpt","text":"","title":"chatGPT"},{"location":"user_eval_function_docs/chatGPT%20%28experimental%29/#what-does-it-do","text":"This chatGPT evaluation function is designed to automatically evaluate student responses to questions. It currently uses the openAI API to determine the correctness (true/false) of the student's answer and can also provide them with feedback.","title":"What does it do?"},{"location":"user_eval_function_docs/chatGPT%20%28experimental%29/#what-does-the-teacher-need-to-input","text":"Model As of now, gpt-3.5-turbo is the only model available. In the future, more openAI and other models can be implemented. Main_prompt In this prompt you should explain the question and answer to gpt. Default_prompt As of now, this prompt should not be changed. It tells gpt to output a Boolean, which marks the student's answer as correct or incorrect. In the future, this could be changed so that 'partially incorrect' answers, etc, are possible. Feedback_prompt Leave this prompt blank if you do not want any textual feedback to be given to the student, but just correct/incorrect. Fill in this prompt to tell gpt how to give feedback to the student. Examples of things you may want to include in your feedback_prompt : Give the student objective and constructive feedback on their answer in first person. If the student is incorrect, provide feedback/hints to help them, but do not reveal the answer. Please be aware that gpt-3.5-turbo often gets overwhelmed by a long prompts, and can sometimes ignore your instructions. It is recommended to do testing with your question, and use gpt-4 if neccesary. Be aware that the cost of gpt-4 is ~20x more than gpt-3.5-turbo (at the time of writing - 6th Sep 2023).","title":"What does the teacher need to input?"},{"location":"user_eval_function_docs/chatGPT%20%28experimental%29/#usage-examples","text":"Each example below demonstrates the potential usage of main_prompt and feedback_prompt for different questions.","title":"Usage examples"},{"location":"user_eval_function_docs/chatGPT%20%28experimental%29/#simple-descriptive-observation","text":"Main Prompt : In this question, the student is asked to make a comment about the behaviour of a partial sum. The correct answer is 'fast convergence'. Accept any paraphrasing/equivalent answers. To be correct, they must mention both aspects (fast and convergence). Feedback Prompt : (Empty)","title":"Simple descriptive observation"},{"location":"user_eval_function_docs/riskAssessment/","text":"Supported Response Area Types This evaluation function is supported by the following Response Area components: TABLE riskAssessment \u00b6 Lambda Feedback Setup \u00b6 In teacher mode, create a new response area. Within the response area, click \"Configure\" Click the \"Input\" tab and change the input type to Table and make the Table have 12 rows and 1 column. Remove the column name and add the following names to the rows: Click the \"Evaluate\" tab and select \"riskAssessment\" from the Evaluation Function dropdown. As done in the above screenshot, in the \"Evaluate\" tab, set is_risk_matrix = False, set is_feedback_text = False, set is_risk_assessment = True and LLM to one of the following options: \"Claude Opus\" \"Claude Sonnet\" \"Claude Haiku\" \"GPT-4 Turbo\" \"GPT-3.5 Turbo 1106\" \"Mistral Large\" \"Mixtral 8x22B\" \"Mixtral 8x7B\" The recommended LLM is Claude Sonnet. Outputs \u00b6 Example risk assessment inputs Coresponding risk assessment outputs","title":"riskAssessment"},{"location":"user_eval_function_docs/riskAssessment/#riskassessment","text":"","title":"riskAssessment"},{"location":"user_eval_function_docs/riskAssessment/#lambda-feedback-setup","text":"In teacher mode, create a new response area. Within the response area, click \"Configure\" Click the \"Input\" tab and change the input type to Table and make the Table have 12 rows and 1 column. Remove the column name and add the following names to the rows: Click the \"Evaluate\" tab and select \"riskAssessment\" from the Evaluation Function dropdown. As done in the above screenshot, in the \"Evaluate\" tab, set is_risk_matrix = False, set is_feedback_text = False, set is_risk_assessment = True and LLM to one of the following options: \"Claude Opus\" \"Claude Sonnet\" \"Claude Haiku\" \"GPT-4 Turbo\" \"GPT-3.5 Turbo 1106\" \"Mistral Large\" \"Mixtral 8x22B\" \"Mixtral 8x7B\" The recommended LLM is Claude Sonnet.","title":"Lambda Feedback Setup"},{"location":"user_eval_function_docs/riskAssessment/#outputs","text":"Example risk assessment inputs Coresponding risk assessment outputs","title":"Outputs"},{"location":"user_eval_function_docs/compareExpressions/","text":"Supported Response Area Types This evaluation function is supported by the following Response Area components: EXPRESSION CODE ESSAY CompareExpressions \u00b6 This function utilises the SymPy to provide a maths-aware comparsion of a student's response to the correct answer. This means that mathematically equivalent inputs will be marked as correct. Note that pi is a reserved constant and cannot be used as a symbol name. Note that this function is designed to handle comparisons of mathematical expressions but has some limited ability to handle comparison of equalities as well. More precisely, if the answer is of the form \\(f(x_1,\\ldots,x_n) = g(x_1,\\ldots,x_n)\\) and the response is of the form \\(\\tilde{f}(x_1,\\ldots,x_n) = \\tilde{g}(x_1,\\ldots,x_n)\\) then the function checks if \\(f(x_1,\\ldots,x_n) - g(x_1,\\ldots,x_n)\\) is a multiple of \\(\\tilde{f}(x_1,\\ldots,x_n) / \\tilde{g}(x_1,\\ldots,x_n)\\) . Inputs \u00b6 Optional parameters \u00b6 There are eight optional parameters that can be set: complexNumbers , convention , criteria , multiple_answers_criteria , elementary_functions , physical_quantity , plus_minus / minus_plus specialFunctions , strict_syntax , symbol_assumptions . complexNumbers \u00b6 If you want to use I for the imaginary constant, set the grading parameter complexNumbers to True. convention \u00b6 Changes the implicit multiplication convention. If unset it will default to equal_precedence . If set to implicit_higher_precedence then implicit multiplication will have higher precedence than explicit multiplication, i.e. 1/ab will be equal to 1/(ab) and 1/a*b will be equal to (1/a)*b . If set to equal_precedence then implicit multiplication will have the same precedence than explicit multiplication, i.e. both 1/ab and 1/a*b will be equal to (1/a)*b . criteria \u00b6 The criteria parameter can be used to customize the comparison performed by the evaluation function. If unset the evaluation function will will default to checking if the answer and response are symbolically equal. The criteria parameter takes a string that defines a set of (comma separated) mathematical statements. If all statements in the list are true the response is considered correct. The criteria parameter reserves response and answer as keywords that will be replaced y the response and answer respectively when the criteria is checked. Setting criteria to answer=response is gives the same behaviour as leaving criteria unset. Note: Currently the criteria parameter is ignored if physical_quantity is set to true. Note: The criteria parameters functionality is currently under development and will rarely produce appropriate feedback and can be quite difficult to debug. elementary_functions \u00b6 When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting elementary_functions to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview. sin , sinc , csc (alternative cosec ), cos , sec , tan , cot (alternative cotan ), asin (alternative arcsin ), acsc (alternatives arccsc , arccosec ), acos (alternative arccos ), asec (alternative arcsec ), atan (alternative arctan ), acot (alternatives arccot , arccotan ), atan2 (alternative arctan2 ), sinh , cosh , tanh , csch (alternative cosech ), sech , asinh (alternative arcsinh ), acosh (alternative arccosh ), atanh (alternative arctanh ), acsch (alternatives arccsch , arcosech ), asech (alternative arcsech ), exp (alternative Exp ), E (equivalent to exp(1) , alternative e ), log , sqrt , sign , Abs (alternative abs ), Max (alternative max ), Min (alternative min ), arg , ceiling (alternative ceil ), floor multiple_answers_criteria \u00b6 The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. Answers or responses that contain \\(\\pm\\) or \\(\\mp\\) has two possible interpretations which requires further criteria for equality. The grading parameter multiple_answers_criteria controls this. The default setting, all , is that each answer must have a corresponding answer and vice versa. The setting all_responses check that all responses are valid answers and the setting all_answers checks that all answers are found among the responses. physical_quantity \u00b6 If unset, physical_quantity will default to false . If physical_quantity is set to true the answer and response will interpreted as a physical quantity using units and conventions decided by the strictness and units_string parameters. Remark: Setting physical_quantity to true will also mean that comparisons will be done numerically. If neither the atol nor rtol parameters are set, the evaluation function will choose a relative error based on the number of sigificant digits given in the answer. When physical_quantity the evaluation function will generate feedback based on the flowchart below. Hovering over a criterion node will show a short natural language description of the criterion. Hovering over a result node will show the feedback produced so far. Remark: In some browser it is necessary to right-click and open the image in a separate tab in order for the tooltips to show up on hover. plus_minus and minus_plus \u00b6 The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters plus_minus and minus_plus to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences. specialFunctions \u00b6 If you want to use the special functions beta (Euler Beta function), gamma (Gamma function) and zeta (Riemann Zeta function), set the grading parameter specialFunctions to True. strict_syntax \u00b6 If strict_syntax is set to true then the answer and response must have * or / between each part of the expressions and exponentiation must be done using ** , e.g. 10*x*y/z**2 is accepted but 10xy/z^2 is not. If strict_syntax is set to false, then * can be omitted and ^ used instead of ** . In this case it is also recommended to list any multicharacter symbols expected to appear in the response as input symbols. By default strict_syntax is set to true. symbol_assumptions \u00b6 This input parameter allows the author to set an extra assumption each symbol. Each assumption should be written on the form ('symbol','assumption name') and all pairs concatenated into a single string. The possible assumption names can be found in this list: SymPy Assumption Predicates Examples \u00b6 Implemented versions of these examples can be found in the module 'Examples: Evaluation Functions'. 1 Setting input symbols to be assumed positive to avoid issues with fractional powers \u00b6 In general \\(\\frac{\\sqrt{a}}{\\sqrt{b}} \\neq \\sqrt{\\frac{a}{b}}\\) but if \\(a > 0\\) and \\(b > 0\\) then \\(\\frac{\\sqrt{a}}{\\sqrt{b}} = \\sqrt{\\frac{a}{b}}\\) . The same is true for other fractional powers. So if expressions like these are expected in the answer and/or response then it is a good idea to use the symbol_assumptions parameter to note that \\(a > 0\\) and \\(b > 0\\) . This can be done by setting symbol_assumptions to ('a','positive') ('b','positive') . The example given in the example problem set uses two EXPRESSION response areas. Both response areas uses compareExpression with answer sqrt(a/b) , strict_syntax set to false, elementary_functions set to true. One response area leaves symbol_assumptions unset and the other sets the parameter as described in the previous paragraph. Some examples of expressions that are accepted as correct when positivity is assumed: sqrt(a)/sqrt(b) , (a/b)**(1/2) , a**(1/2)/b**(1/2) , (a/b)^(0.5) , a^(0.5)/b^(0.5) 2 Using plus/minus symbols \u00b6 The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters plus_minus and minus_plus to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences. It is considered good practice to make sure that the appropriate notation for \\(\\pm\\) and \\(\\mp\\) are added and displayed as input symbols in order to minimize confusion. The example given in the example problem set uses an EXPRESSION response area that uses compareExpression with answer plus_minus x**2 + minus_plus y**2 , strict_syntax set to false and elementary_function set to true. Some examples of expressions that are accepted as correct: plus_minus x**2 + minus_plus y**2 , - minus_plus x**2 + minus_plus y**2 , - minus_plus x^2 minus_plus y^2 , - minus_plus x^2 - plus_minus y^2 3 Equalities in the answer and response \u00b6 There is (limited) support for using equalities in the response and answer. The example given in the example problem set uses an EXPRESSION response area that uses compareExpression with answer x**2-5*y**2-7=0 . Some examples of expressions that are accepted as correct: x**2-5*y**2-7=0 , x^2 = 5y^2+7 , 2x^2 = 10y^2+14 4 Checking the value of an expression or a physical quantity \u00b6 If the parameter physical_quantity is set to true, the evaluation function can handle expressions that describe physical quantities. Which units are permitted and how they should be written depends on the units_string and strictness parameters respectively. There are three examples in the example problem set. Each examples uses an EXPRESSION response area that uses compareExpression with answer strict_syntax set to false and physical_quantity set to true. Example (a) \u00b6 Here the answer is 2.00 km/h . The parameters strictness and units_string are left unset which is equivalent to setting strictness to natural , and units_string to SI common imperial . Thus this response area accepts a wide range of responses, e.g. 2.00 kilometre/hour , 2 km/h , 2000 meter/hour , 2 metre/millihour Example (b) \u00b6 Here the answer is 2.00 km/h . To restrict the answers to SI units strictness is set to strict and units_string is set to SI . Some examples of accepted responses are: 0.556 metre/second , 5.56 dm/s , 55.6 centimetre second^(-1) Example (c) \u00b6 Here the answer is 2.00 km/h . To restrict the answers to imperial units strictness is set to strict and units_string is set to imperial common . Accepted response: 1.24 mile/hour","title":"compareExpressions"},{"location":"user_eval_function_docs/compareExpressions/#compareexpressions","text":"This function utilises the SymPy to provide a maths-aware comparsion of a student's response to the correct answer. This means that mathematically equivalent inputs will be marked as correct. Note that pi is a reserved constant and cannot be used as a symbol name. Note that this function is designed to handle comparisons of mathematical expressions but has some limited ability to handle comparison of equalities as well. More precisely, if the answer is of the form \\(f(x_1,\\ldots,x_n) = g(x_1,\\ldots,x_n)\\) and the response is of the form \\(\\tilde{f}(x_1,\\ldots,x_n) = \\tilde{g}(x_1,\\ldots,x_n)\\) then the function checks if \\(f(x_1,\\ldots,x_n) - g(x_1,\\ldots,x_n)\\) is a multiple of \\(\\tilde{f}(x_1,\\ldots,x_n) / \\tilde{g}(x_1,\\ldots,x_n)\\) .","title":"CompareExpressions"},{"location":"user_eval_function_docs/compareExpressions/#inputs","text":"","title":"Inputs"},{"location":"user_eval_function_docs/compareExpressions/#optional-parameters","text":"There are eight optional parameters that can be set: complexNumbers , convention , criteria , multiple_answers_criteria , elementary_functions , physical_quantity , plus_minus / minus_plus specialFunctions , strict_syntax , symbol_assumptions .","title":"Optional parameters"},{"location":"user_eval_function_docs/compareExpressions/#complexnumbers","text":"If you want to use I for the imaginary constant, set the grading parameter complexNumbers to True.","title":"complexNumbers"},{"location":"user_eval_function_docs/compareExpressions/#convention","text":"Changes the implicit multiplication convention. If unset it will default to equal_precedence . If set to implicit_higher_precedence then implicit multiplication will have higher precedence than explicit multiplication, i.e. 1/ab will be equal to 1/(ab) and 1/a*b will be equal to (1/a)*b . If set to equal_precedence then implicit multiplication will have the same precedence than explicit multiplication, i.e. both 1/ab and 1/a*b will be equal to (1/a)*b .","title":"convention"},{"location":"user_eval_function_docs/compareExpressions/#criteria","text":"The criteria parameter can be used to customize the comparison performed by the evaluation function. If unset the evaluation function will will default to checking if the answer and response are symbolically equal. The criteria parameter takes a string that defines a set of (comma separated) mathematical statements. If all statements in the list are true the response is considered correct. The criteria parameter reserves response and answer as keywords that will be replaced y the response and answer respectively when the criteria is checked. Setting criteria to answer=response is gives the same behaviour as leaving criteria unset. Note: Currently the criteria parameter is ignored if physical_quantity is set to true. Note: The criteria parameters functionality is currently under development and will rarely produce appropriate feedback and can be quite difficult to debug.","title":"criteria"},{"location":"user_eval_function_docs/compareExpressions/#elementary_functions","text":"When using implicit multiplication function names with mulitple characters are sometimes split and not interpreted properly. Setting elementary_functions to true will reserve the function names listed below and prevent them from being split. If a name is said to have one or more alternatives this means that it will accept the alternative names but the reserved name is what will be shown in the preview. sin , sinc , csc (alternative cosec ), cos , sec , tan , cot (alternative cotan ), asin (alternative arcsin ), acsc (alternatives arccsc , arccosec ), acos (alternative arccos ), asec (alternative arcsec ), atan (alternative arctan ), acot (alternatives arccot , arccotan ), atan2 (alternative arctan2 ), sinh , cosh , tanh , csch (alternative cosech ), sech , asinh (alternative arcsinh ), acosh (alternative arccosh ), atanh (alternative arctanh ), acsch (alternatives arccsch , arcosech ), asech (alternative arcsech ), exp (alternative Exp ), E (equivalent to exp(1) , alternative e ), log , sqrt , sign , Abs (alternative abs ), Max (alternative max ), Min (alternative min ), arg , ceiling (alternative ceil ), floor","title":"elementary_functions"},{"location":"user_eval_function_docs/compareExpressions/#multiple_answers_criteria","text":"The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. Answers or responses that contain \\(\\pm\\) or \\(\\mp\\) has two possible interpretations which requires further criteria for equality. The grading parameter multiple_answers_criteria controls this. The default setting, all , is that each answer must have a corresponding answer and vice versa. The setting all_responses check that all responses are valid answers and the setting all_answers checks that all answers are found among the responses.","title":"multiple_answers_criteria"},{"location":"user_eval_function_docs/compareExpressions/#physical_quantity","text":"If unset, physical_quantity will default to false . If physical_quantity is set to true the answer and response will interpreted as a physical quantity using units and conventions decided by the strictness and units_string parameters. Remark: Setting physical_quantity to true will also mean that comparisons will be done numerically. If neither the atol nor rtol parameters are set, the evaluation function will choose a relative error based on the number of sigificant digits given in the answer. When physical_quantity the evaluation function will generate feedback based on the flowchart below. Hovering over a criterion node will show a short natural language description of the criterion. Hovering over a result node will show the feedback produced so far. Remark: In some browser it is necessary to right-click and open the image in a separate tab in order for the tooltips to show up on hover.","title":"physical_quantity"},{"location":"user_eval_function_docs/compareExpressions/#plus_minus-and-minus_plus","text":"The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters plus_minus and minus_plus to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences.","title":"plus_minus and minus_plus"},{"location":"user_eval_function_docs/compareExpressions/#specialfunctions","text":"If you want to use the special functions beta (Euler Beta function), gamma (Gamma function) and zeta (Riemann Zeta function), set the grading parameter specialFunctions to True.","title":"specialFunctions"},{"location":"user_eval_function_docs/compareExpressions/#strict_syntax","text":"If strict_syntax is set to true then the answer and response must have * or / between each part of the expressions and exponentiation must be done using ** , e.g. 10*x*y/z**2 is accepted but 10xy/z^2 is not. If strict_syntax is set to false, then * can be omitted and ^ used instead of ** . In this case it is also recommended to list any multicharacter symbols expected to appear in the response as input symbols. By default strict_syntax is set to true.","title":"strict_syntax"},{"location":"user_eval_function_docs/compareExpressions/#symbol_assumptions","text":"This input parameter allows the author to set an extra assumption each symbol. Each assumption should be written on the form ('symbol','assumption name') and all pairs concatenated into a single string. The possible assumption names can be found in this list: SymPy Assumption Predicates","title":"symbol_assumptions"},{"location":"user_eval_function_docs/compareExpressions/#examples","text":"Implemented versions of these examples can be found in the module 'Examples: Evaluation Functions'.","title":"Examples"},{"location":"user_eval_function_docs/compareExpressions/#1-setting-input-symbols-to-be-assumed-positive-to-avoid-issues-with-fractional-powers","text":"In general \\(\\frac{\\sqrt{a}}{\\sqrt{b}} \\neq \\sqrt{\\frac{a}{b}}\\) but if \\(a > 0\\) and \\(b > 0\\) then \\(\\frac{\\sqrt{a}}{\\sqrt{b}} = \\sqrt{\\frac{a}{b}}\\) . The same is true for other fractional powers. So if expressions like these are expected in the answer and/or response then it is a good idea to use the symbol_assumptions parameter to note that \\(a > 0\\) and \\(b > 0\\) . This can be done by setting symbol_assumptions to ('a','positive') ('b','positive') . The example given in the example problem set uses two EXPRESSION response areas. Both response areas uses compareExpression with answer sqrt(a/b) , strict_syntax set to false, elementary_functions set to true. One response area leaves symbol_assumptions unset and the other sets the parameter as described in the previous paragraph. Some examples of expressions that are accepted as correct when positivity is assumed: sqrt(a)/sqrt(b) , (a/b)**(1/2) , a**(1/2)/b**(1/2) , (a/b)^(0.5) , a^(0.5)/b^(0.5)","title":"1 Setting input symbols to be assumed positive to avoid issues with fractional powers"},{"location":"user_eval_function_docs/compareExpressions/#2-using-plusminus-symbols","text":"The \\(\\pm\\) and \\(\\mp\\) symbols can be represented in the answer or response by plus_minus and minus_plus respectively. To use other symbols for \\(\\pm\\) and \\(\\mp\\) set the grading parameters plus_minus and minus_plus to the desired symbol. Remark: symbol replacement is brittle and can have unintended consequences. It is considered good practice to make sure that the appropriate notation for \\(\\pm\\) and \\(\\mp\\) are added and displayed as input symbols in order to minimize confusion. The example given in the example problem set uses an EXPRESSION response area that uses compareExpression with answer plus_minus x**2 + minus_plus y**2 , strict_syntax set to false and elementary_function set to true. Some examples of expressions that are accepted as correct: plus_minus x**2 + minus_plus y**2 , - minus_plus x**2 + minus_plus y**2 , - minus_plus x^2 minus_plus y^2 , - minus_plus x^2 - plus_minus y^2","title":"2 Using plus/minus symbols"},{"location":"user_eval_function_docs/compareExpressions/#3-equalities-in-the-answer-and-response","text":"There is (limited) support for using equalities in the response and answer. The example given in the example problem set uses an EXPRESSION response area that uses compareExpression with answer x**2-5*y**2-7=0 . Some examples of expressions that are accepted as correct: x**2-5*y**2-7=0 , x^2 = 5y^2+7 , 2x^2 = 10y^2+14","title":"3 Equalities in the answer and response"},{"location":"user_eval_function_docs/compareExpressions/#4-checking-the-value-of-an-expression-or-a-physical-quantity","text":"If the parameter physical_quantity is set to true, the evaluation function can handle expressions that describe physical quantities. Which units are permitted and how they should be written depends on the units_string and strictness parameters respectively. There are three examples in the example problem set. Each examples uses an EXPRESSION response area that uses compareExpression with answer strict_syntax set to false and physical_quantity set to true.","title":"4 Checking the value of an expression or a physical quantity"},{"location":"user_eval_function_docs/compareExpressions/#example-a","text":"Here the answer is 2.00 km/h . The parameters strictness and units_string are left unset which is equivalent to setting strictness to natural , and units_string to SI common imperial . Thus this response area accepts a wide range of responses, e.g. 2.00 kilometre/hour , 2 km/h , 2000 meter/hour , 2 metre/millihour","title":"Example (a)"},{"location":"user_eval_function_docs/compareExpressions/#example-b","text":"Here the answer is 2.00 km/h . To restrict the answers to SI units strictness is set to strict and units_string is set to SI . Some examples of accepted responses are: 0.556 metre/second , 5.56 dm/s , 55.6 centimetre second^(-1)","title":"Example (b)"},{"location":"user_eval_function_docs/compareExpressions/#example-c","text":"Here the answer is 2.00 km/h . To restrict the answers to imperial units strictness is set to strict and units_string is set to imperial common . Accepted response: 1.24 mile/hour","title":"Example (c)"}]}